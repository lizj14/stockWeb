


<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    
    
    <meta http-equiv="X-UA-Compatible" content="IE=emulateIE7" />
    <title>Coverage for /home/ec2-user/environment/venv/py3vir/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py: 33%</title>
    <link rel="stylesheet" href="style.css" type="text/css">
    
    <script type="text/javascript" src="jquery.min.js"></script>
    <script type="text/javascript" src="jquery.hotkeys.js"></script>
    <script type="text/javascript" src="jquery.isonscreen.js"></script>
    <script type="text/javascript" src="coverage_html.js"></script>
    <script type="text/javascript">
        jQuery(document).ready(coverage.pyfile_ready);
    </script>
</head>
<body class="pyfile">

<div id="header">
    <div class="content">
        <h1>Coverage for <b>/home/ec2-user/environment/venv/py3vir/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py</b> :
            <span class="pc_cov">33%</span>
        </h1>

        <img id="keyboard_icon" src="keybd_closed.png" alt="Show keyboard shortcuts" />

        <h2 class="stats">
            845 statements &nbsp;
            <span class="run hide_run shortkey_r button_toggle_run">283 run</span>
            <span class="mis shortkey_m button_toggle_mis">562 missing</span>
            <span class="exc shortkey_x button_toggle_exc">0 excluded</span>

            
        </h2>
    </div>
</div>

<div class="help_panel">
    <img id="panel_icon" src="keybd_open.png" alt="Hide keyboard shortcuts" />
    <p class="legend">Hot-keys on this page</p>
    <div>
    <p class="keyhelp">
        <span class="key">r</span>
        <span class="key">m</span>
        <span class="key">x</span>
        <span class="key">p</span> &nbsp; toggle line displays
    </p>
    <p class="keyhelp">
        <span class="key">j</span>
        <span class="key">k</span> &nbsp; next/prev highlighted chunk
    </p>
    <p class="keyhelp">
        <span class="key">0</span> &nbsp; (zero) top of page
    </p>
    <p class="keyhelp">
        <span class="key">1</span> &nbsp; (one) first highlighted chunk
    </p>
    </div>
</div>

<div id="source">
    <table>
        <tr>
            <td class="linenos">
<p id="n1" class="pln"><a href="#n1">1</a></p>
<p id="n2" class="pln"><a href="#n2">2</a></p>
<p id="n3" class="pln"><a href="#n3">3</a></p>
<p id="n4" class="pln"><a href="#n4">4</a></p>
<p id="n5" class="pln"><a href="#n5">5</a></p>
<p id="n6" class="pln"><a href="#n6">6</a></p>
<p id="n7" class="pln"><a href="#n7">7</a></p>
<p id="n8" class="pln"><a href="#n8">8</a></p>
<p id="n9" class="pln"><a href="#n9">9</a></p>
<p id="n10" class="pln"><a href="#n10">10</a></p>
<p id="n11" class="pln"><a href="#n11">11</a></p>
<p id="n12" class="pln"><a href="#n12">12</a></p>
<p id="n13" class="pln"><a href="#n13">13</a></p>
<p id="n14" class="pln"><a href="#n14">14</a></p>
<p id="n15" class="stm run hide_run"><a href="#n15">15</a></p>
<p id="n16" class="pln"><a href="#n16">16</a></p>
<p id="n17" class="pln"><a href="#n17">17</a></p>
<p id="n18" class="pln"><a href="#n18">18</a></p>
<p id="n19" class="stm run hide_run"><a href="#n19">19</a></p>
<p id="n20" class="stm run hide_run"><a href="#n20">20</a></p>
<p id="n21" class="stm run hide_run"><a href="#n21">21</a></p>
<p id="n22" class="pln"><a href="#n22">22</a></p>
<p id="n23" class="stm run hide_run"><a href="#n23">23</a></p>
<p id="n24" class="stm run hide_run"><a href="#n24">24</a></p>
<p id="n25" class="pln"><a href="#n25">25</a></p>
<p id="n26" class="stm run hide_run"><a href="#n26">26</a></p>
<p id="n27" class="stm run hide_run"><a href="#n27">27</a></p>
<p id="n28" class="stm run hide_run"><a href="#n28">28</a></p>
<p id="n29" class="stm run hide_run"><a href="#n29">29</a></p>
<p id="n30" class="stm run hide_run"><a href="#n30">30</a></p>
<p id="n31" class="stm run hide_run"><a href="#n31">31</a></p>
<p id="n32" class="stm run hide_run"><a href="#n32">32</a></p>
<p id="n33" class="stm run hide_run"><a href="#n33">33</a></p>
<p id="n34" class="stm run hide_run"><a href="#n34">34</a></p>
<p id="n35" class="stm run hide_run"><a href="#n35">35</a></p>
<p id="n36" class="stm run hide_run"><a href="#n36">36</a></p>
<p id="n37" class="stm run hide_run"><a href="#n37">37</a></p>
<p id="n38" class="stm run hide_run"><a href="#n38">38</a></p>
<p id="n39" class="stm run hide_run"><a href="#n39">39</a></p>
<p id="n40" class="pln"><a href="#n40">40</a></p>
<p id="n41" class="pln"><a href="#n41">41</a></p>
<p id="n42" class="stm run hide_run"><a href="#n42">42</a></p>
<p id="n43" class="pln"><a href="#n43">43</a></p>
<p id="n44" class="stm run hide_run"><a href="#n44">44</a></p>
<p id="n45" class="stm run hide_run"><a href="#n45">45</a></p>
<p id="n46" class="stm run hide_run"><a href="#n46">46</a></p>
<p id="n47" class="stm run hide_run"><a href="#n47">47</a></p>
<p id="n48" class="stm run hide_run"><a href="#n48">48</a></p>
<p id="n49" class="pln"><a href="#n49">49</a></p>
<p id="n50" class="pln"><a href="#n50">50</a></p>
<p id="n51" class="stm run hide_run"><a href="#n51">51</a></p>
<p id="n52" class="pln"><a href="#n52">52</a></p>
<p id="n53" class="stm run hide_run"><a href="#n53">53</a></p>
<p id="n54" class="stm run hide_run"><a href="#n54">54</a></p>
<p id="n55" class="stm run hide_run"><a href="#n55">55</a></p>
<p id="n56" class="stm run hide_run"><a href="#n56">56</a></p>
<p id="n57" class="pln"><a href="#n57">57</a></p>
<p id="n58" class="pln"><a href="#n58">58</a></p>
<p id="n59" class="pln"><a href="#n59">59</a></p>
<p id="n60" class="stm run hide_run"><a href="#n60">60</a></p>
<p id="n61" class="pln"><a href="#n61">61</a></p>
<p id="n62" class="pln"><a href="#n62">62</a></p>
<p id="n63" class="stm run hide_run"><a href="#n63">63</a></p>
<p id="n64" class="pln"><a href="#n64">64</a></p>
<p id="n65" class="stm run hide_run"><a href="#n65">65</a></p>
<p id="n66" class="stm run hide_run"><a href="#n66">66</a></p>
<p id="n67" class="stm run hide_run"><a href="#n67">67</a></p>
<p id="n68" class="pln"><a href="#n68">68</a></p>
<p id="n69" class="stm run hide_run"><a href="#n69">69</a></p>
<p id="n70" class="pln"><a href="#n70">70</a></p>
<p id="n71" class="pln"><a href="#n71">71</a></p>
<p id="n72" class="pln"><a href="#n72">72</a></p>
<p id="n73" class="stm run hide_run"><a href="#n73">73</a></p>
<p id="n74" class="stm run hide_run"><a href="#n74">74</a></p>
<p id="n75" class="pln"><a href="#n75">75</a></p>
<p id="n76" class="stm run hide_run"><a href="#n76">76</a></p>
<p id="n77" class="pln"><a href="#n77">77</a></p>
<p id="n78" class="pln"><a href="#n78">78</a></p>
<p id="n79" class="stm run hide_run"><a href="#n79">79</a></p>
<p id="n80" class="pln"><a href="#n80">80</a></p>
<p id="n81" class="pln"><a href="#n81">81</a></p>
<p id="n82" class="pln"><a href="#n82">82</a></p>
<p id="n83" class="pln"><a href="#n83">83</a></p>
<p id="n84" class="stm mis"><a href="#n84">84</a></p>
<p id="n85" class="pln"><a href="#n85">85</a></p>
<p id="n86" class="stm mis"><a href="#n86">86</a></p>
<p id="n87" class="stm mis"><a href="#n87">87</a></p>
<p id="n88" class="stm mis"><a href="#n88">88</a></p>
<p id="n89" class="pln"><a href="#n89">89</a></p>
<p id="n90" class="pln"><a href="#n90">90</a></p>
<p id="n91" class="stm run hide_run"><a href="#n91">91</a></p>
<p id="n92" class="stm run hide_run"><a href="#n92">92</a></p>
<p id="n93" class="pln"><a href="#n93">93</a></p>
<p id="n94" class="stm run hide_run"><a href="#n94">94</a></p>
<p id="n95" class="pln"><a href="#n95">95</a></p>
<p id="n96" class="pln"><a href="#n96">96</a></p>
<p id="n97" class="stm run hide_run"><a href="#n97">97</a></p>
<p id="n98" class="pln"><a href="#n98">98</a></p>
<p id="n99" class="pln"><a href="#n99">99</a></p>
<p id="n100" class="pln"><a href="#n100">100</a></p>
<p id="n101" class="pln"><a href="#n101">101</a></p>
<p id="n102" class="stm mis"><a href="#n102">102</a></p>
<p id="n103" class="pln"><a href="#n103">103</a></p>
<p id="n104" class="stm mis"><a href="#n104">104</a></p>
<p id="n105" class="stm mis"><a href="#n105">105</a></p>
<p id="n106" class="stm mis"><a href="#n106">106</a></p>
<p id="n107" class="pln"><a href="#n107">107</a></p>
<p id="n108" class="pln"><a href="#n108">108</a></p>
<p id="n109" class="pln"><a href="#n109">109</a></p>
<p id="n110" class="pln"><a href="#n110">110</a></p>
<p id="n111" class="pln"><a href="#n111">111</a></p>
<p id="n112" class="pln"><a href="#n112">112</a></p>
<p id="n113" class="pln"><a href="#n113">113</a></p>
<p id="n114" class="stm run hide_run"><a href="#n114">114</a></p>
<p id="n115" class="stm run hide_run"><a href="#n115">115</a></p>
<p id="n116" class="pln"><a href="#n116">116</a></p>
<p id="n117" class="pln"><a href="#n117">117</a></p>
<p id="n118" class="pln"><a href="#n118">118</a></p>
<p id="n119" class="pln"><a href="#n119">119</a></p>
<p id="n120" class="pln"><a href="#n120">120</a></p>
<p id="n121" class="pln"><a href="#n121">121</a></p>
<p id="n122" class="pln"><a href="#n122">122</a></p>
<p id="n123" class="pln"><a href="#n123">123</a></p>
<p id="n124" class="pln"><a href="#n124">124</a></p>
<p id="n125" class="pln"><a href="#n125">125</a></p>
<p id="n126" class="pln"><a href="#n126">126</a></p>
<p id="n127" class="pln"><a href="#n127">127</a></p>
<p id="n128" class="pln"><a href="#n128">128</a></p>
<p id="n129" class="pln"><a href="#n129">129</a></p>
<p id="n130" class="pln"><a href="#n130">130</a></p>
<p id="n131" class="pln"><a href="#n131">131</a></p>
<p id="n132" class="pln"><a href="#n132">132</a></p>
<p id="n133" class="pln"><a href="#n133">133</a></p>
<p id="n134" class="pln"><a href="#n134">134</a></p>
<p id="n135" class="pln"><a href="#n135">135</a></p>
<p id="n136" class="pln"><a href="#n136">136</a></p>
<p id="n137" class="pln"><a href="#n137">137</a></p>
<p id="n138" class="stm mis"><a href="#n138">138</a></p>
<p id="n139" class="stm mis"><a href="#n139">139</a></p>
<p id="n140" class="stm mis"><a href="#n140">140</a></p>
<p id="n141" class="stm mis"><a href="#n141">141</a></p>
<p id="n142" class="pln"><a href="#n142">142</a></p>
<p id="n143" class="stm mis"><a href="#n143">143</a></p>
<p id="n144" class="pln"><a href="#n144">144</a></p>
<p id="n145" class="stm mis"><a href="#n145">145</a></p>
<p id="n146" class="stm mis"><a href="#n146">146</a></p>
<p id="n147" class="pln"><a href="#n147">147</a></p>
<p id="n148" class="pln"><a href="#n148">148</a></p>
<p id="n149" class="stm mis"><a href="#n149">149</a></p>
<p id="n150" class="stm mis"><a href="#n150">150</a></p>
<p id="n151" class="stm mis"><a href="#n151">151</a></p>
<p id="n152" class="stm mis"><a href="#n152">152</a></p>
<p id="n153" class="pln"><a href="#n153">153</a></p>
<p id="n154" class="pln"><a href="#n154">154</a></p>
<p id="n155" class="pln"><a href="#n155">155</a></p>
<p id="n156" class="pln"><a href="#n156">156</a></p>
<p id="n157" class="pln"><a href="#n157">157</a></p>
<p id="n158" class="pln"><a href="#n158">158</a></p>
<p id="n159" class="stm run hide_run"><a href="#n159">159</a></p>
<p id="n160" class="stm mis"><a href="#n160">160</a></p>
<p id="n161" class="pln"><a href="#n161">161</a></p>
<p id="n162" class="pln"><a href="#n162">162</a></p>
<p id="n163" class="pln"><a href="#n163">163</a></p>
<p id="n164" class="pln"><a href="#n164">164</a></p>
<p id="n165" class="pln"><a href="#n165">165</a></p>
<p id="n166" class="stm run hide_run"><a href="#n166">166</a></p>
<p id="n167" class="pln"><a href="#n167">167</a></p>
<p id="n168" class="pln"><a href="#n168">168</a></p>
<p id="n169" class="stm run hide_run"><a href="#n169">169</a></p>
<p id="n170" class="pln"><a href="#n170">170</a></p>
<p id="n171" class="pln"><a href="#n171">171</a></p>
<p id="n172" class="pln"><a href="#n172">172</a></p>
<p id="n173" class="pln"><a href="#n173">173</a></p>
<p id="n174" class="pln"><a href="#n174">174</a></p>
<p id="n175" class="pln"><a href="#n175">175</a></p>
<p id="n176" class="stm mis"><a href="#n176">176</a></p>
<p id="n177" class="stm mis"><a href="#n177">177</a></p>
<p id="n178" class="pln"><a href="#n178">178</a></p>
<p id="n179" class="stm run hide_run"><a href="#n179">179</a></p>
<p id="n180" class="stm mis"><a href="#n180">180</a></p>
<p id="n181" class="pln"><a href="#n181">181</a></p>
<p id="n182" class="stm run hide_run"><a href="#n182">182</a></p>
<p id="n183" class="stm mis"><a href="#n183">183</a></p>
<p id="n184" class="pln"><a href="#n184">184</a></p>
<p id="n185" class="stm run hide_run"><a href="#n185">185</a></p>
<p id="n186" class="stm mis"><a href="#n186">186</a></p>
<p id="n187" class="pln"><a href="#n187">187</a></p>
<p id="n188" class="pln"><a href="#n188">188</a></p>
<p id="n189" class="stm run hide_run"><a href="#n189">189</a></p>
<p id="n190" class="stm run hide_run"><a href="#n190">190</a></p>
<p id="n191" class="pln"><a href="#n191">191</a></p>
<p id="n192" class="pln"><a href="#n192">192</a></p>
<p id="n193" class="stm mis"><a href="#n193">193</a></p>
<p id="n194" class="pln"><a href="#n194">194</a></p>
<p id="n195" class="pln"><a href="#n195">195</a></p>
<p id="n196" class="stm mis"><a href="#n196">196</a></p>
<p id="n197" class="pln"><a href="#n197">197</a></p>
<p id="n198" class="stm mis"><a href="#n198">198</a></p>
<p id="n199" class="pln"><a href="#n199">199</a></p>
<p id="n200" class="pln"><a href="#n200">200</a></p>
<p id="n201" class="stm run hide_run"><a href="#n201">201</a></p>
<p id="n202" class="stm run hide_run"><a href="#n202">202</a></p>
<p id="n203" class="stm mis"><a href="#n203">203</a></p>
<p id="n204" class="pln"><a href="#n204">204</a></p>
<p id="n205" class="pln"><a href="#n205">205</a></p>
<p id="n206" class="stm run hide_run"><a href="#n206">206</a></p>
<p id="n207" class="pln"><a href="#n207">207</a></p>
<p id="n208" class="pln"><a href="#n208">208</a></p>
<p id="n209" class="pln"><a href="#n209">209</a></p>
<p id="n210" class="stm run hide_run"><a href="#n210">210</a></p>
<p id="n211" class="pln"><a href="#n211">211</a></p>
<p id="n212" class="pln"><a href="#n212">212</a></p>
<p id="n213" class="stm run hide_run"><a href="#n213">213</a></p>
<p id="n214" class="stm mis"><a href="#n214">214</a></p>
<p id="n215" class="pln"><a href="#n215">215</a></p>
<p id="n216" class="pln"><a href="#n216">216</a></p>
<p id="n217" class="stm run hide_run"><a href="#n217">217</a></p>
<p id="n218" class="pln"><a href="#n218">218</a></p>
<p id="n219" class="pln"><a href="#n219">219</a></p>
<p id="n220" class="pln"><a href="#n220">220</a></p>
<p id="n221" class="stm run hide_run"><a href="#n221">221</a></p>
<p id="n222" class="stm run hide_run"><a href="#n222">222</a></p>
<p id="n223" class="stm mis"><a href="#n223">223</a></p>
<p id="n224" class="pln"><a href="#n224">224</a></p>
<p id="n225" class="pln"><a href="#n225">225</a></p>
<p id="n226" class="stm run hide_run"><a href="#n226">226</a></p>
<p id="n227" class="pln"><a href="#n227">227</a></p>
<p id="n228" class="pln"><a href="#n228">228</a></p>
<p id="n229" class="pln"><a href="#n229">229</a></p>
<p id="n230" class="stm run hide_run"><a href="#n230">230</a></p>
<p id="n231" class="pln"><a href="#n231">231</a></p>
<p id="n232" class="pln"><a href="#n232">232</a></p>
<p id="n233" class="stm run hide_run"><a href="#n233">233</a></p>
<p id="n234" class="stm mis"><a href="#n234">234</a></p>
<p id="n235" class="pln"><a href="#n235">235</a></p>
<p id="n236" class="pln"><a href="#n236">236</a></p>
<p id="n237" class="stm run hide_run"><a href="#n237">237</a></p>
<p id="n238" class="pln"><a href="#n238">238</a></p>
<p id="n239" class="pln"><a href="#n239">239</a></p>
<p id="n240" class="pln"><a href="#n240">240</a></p>
<p id="n241" class="pln"><a href="#n241">241</a></p>
<p id="n242" class="stm run hide_run"><a href="#n242">242</a></p>
<p id="n243" class="stm run hide_run"><a href="#n243">243</a></p>
<p id="n244" class="pln"><a href="#n244">244</a></p>
<p id="n245" class="pln"><a href="#n245">245</a></p>
<p id="n246" class="pln"><a href="#n246">246</a></p>
<p id="n247" class="pln"><a href="#n247">247</a></p>
<p id="n248" class="pln"><a href="#n248">248</a></p>
<p id="n249" class="pln"><a href="#n249">249</a></p>
<p id="n250" class="pln"><a href="#n250">250</a></p>
<p id="n251" class="pln"><a href="#n251">251</a></p>
<p id="n252" class="pln"><a href="#n252">252</a></p>
<p id="n253" class="pln"><a href="#n253">253</a></p>
<p id="n254" class="pln"><a href="#n254">254</a></p>
<p id="n255" class="pln"><a href="#n255">255</a></p>
<p id="n256" class="stm mis"><a href="#n256">256</a></p>
<p id="n257" class="stm mis"><a href="#n257">257</a></p>
<p id="n258" class="stm mis"><a href="#n258">258</a></p>
<p id="n259" class="stm mis"><a href="#n259">259</a></p>
<p id="n260" class="pln"><a href="#n260">260</a></p>
<p id="n261" class="pln"><a href="#n261">261</a></p>
<p id="n262" class="stm mis"><a href="#n262">262</a></p>
<p id="n263" class="pln"><a href="#n263">263</a></p>
<p id="n264" class="pln"><a href="#n264">264</a></p>
<p id="n265" class="pln"><a href="#n265">265</a></p>
<p id="n266" class="pln"><a href="#n266">266</a></p>
<p id="n267" class="pln"><a href="#n267">267</a></p>
<p id="n268" class="pln"><a href="#n268">268</a></p>
<p id="n269" class="stm run hide_run"><a href="#n269">269</a></p>
<p id="n270" class="pln"><a href="#n270">270</a></p>
<p id="n271" class="pln"><a href="#n271">271</a></p>
<p id="n272" class="stm run hide_run"><a href="#n272">272</a></p>
<p id="n273" class="pln"><a href="#n273">273</a></p>
<p id="n274" class="pln"><a href="#n274">274</a></p>
<p id="n275" class="pln"><a href="#n275">275</a></p>
<p id="n276" class="pln"><a href="#n276">276</a></p>
<p id="n277" class="pln"><a href="#n277">277</a></p>
<p id="n278" class="pln"><a href="#n278">278</a></p>
<p id="n279" class="pln"><a href="#n279">279</a></p>
<p id="n280" class="pln"><a href="#n280">280</a></p>
<p id="n281" class="pln"><a href="#n281">281</a></p>
<p id="n282" class="pln"><a href="#n282">282</a></p>
<p id="n283" class="pln"><a href="#n283">283</a></p>
<p id="n284" class="pln"><a href="#n284">284</a></p>
<p id="n285" class="stm mis"><a href="#n285">285</a></p>
<p id="n286" class="pln"><a href="#n286">286</a></p>
<p id="n287" class="pln"><a href="#n287">287</a></p>
<p id="n288" class="pln"><a href="#n288">288</a></p>
<p id="n289" class="pln"><a href="#n289">289</a></p>
<p id="n290" class="pln"><a href="#n290">290</a></p>
<p id="n291" class="stm run hide_run"><a href="#n291">291</a></p>
<p id="n292" class="stm run hide_run"><a href="#n292">292</a></p>
<p id="n293" class="pln"><a href="#n293">293</a></p>
<p id="n294" class="pln"><a href="#n294">294</a></p>
<p id="n295" class="pln"><a href="#n295">295</a></p>
<p id="n296" class="pln"><a href="#n296">296</a></p>
<p id="n297" class="pln"><a href="#n297">297</a></p>
<p id="n298" class="pln"><a href="#n298">298</a></p>
<p id="n299" class="pln"><a href="#n299">299</a></p>
<p id="n300" class="pln"><a href="#n300">300</a></p>
<p id="n301" class="pln"><a href="#n301">301</a></p>
<p id="n302" class="pln"><a href="#n302">302</a></p>
<p id="n303" class="pln"><a href="#n303">303</a></p>
<p id="n304" class="pln"><a href="#n304">304</a></p>
<p id="n305" class="pln"><a href="#n305">305</a></p>
<p id="n306" class="pln"><a href="#n306">306</a></p>
<p id="n307" class="pln"><a href="#n307">307</a></p>
<p id="n308" class="pln"><a href="#n308">308</a></p>
<p id="n309" class="pln"><a href="#n309">309</a></p>
<p id="n310" class="pln"><a href="#n310">310</a></p>
<p id="n311" class="pln"><a href="#n311">311</a></p>
<p id="n312" class="pln"><a href="#n312">312</a></p>
<p id="n313" class="stm mis"><a href="#n313">313</a></p>
<p id="n314" class="stm mis"><a href="#n314">314</a></p>
<p id="n315" class="stm mis"><a href="#n315">315</a></p>
<p id="n316" class="stm mis"><a href="#n316">316</a></p>
<p id="n317" class="pln"><a href="#n317">317</a></p>
<p id="n318" class="pln"><a href="#n318">318</a></p>
<p id="n319" class="stm mis"><a href="#n319">319</a></p>
<p id="n320" class="pln"><a href="#n320">320</a></p>
<p id="n321" class="pln"><a href="#n321">321</a></p>
<p id="n322" class="stm run hide_run"><a href="#n322">322</a></p>
<p id="n323" class="stm run hide_run"><a href="#n323">323</a></p>
<p id="n324" class="pln"><a href="#n324">324</a></p>
<p id="n325" class="pln"><a href="#n325">325</a></p>
<p id="n326" class="pln"><a href="#n326">326</a></p>
<p id="n327" class="pln"><a href="#n327">327</a></p>
<p id="n328" class="pln"><a href="#n328">328</a></p>
<p id="n329" class="pln"><a href="#n329">329</a></p>
<p id="n330" class="pln"><a href="#n330">330</a></p>
<p id="n331" class="pln"><a href="#n331">331</a></p>
<p id="n332" class="pln"><a href="#n332">332</a></p>
<p id="n333" class="pln"><a href="#n333">333</a></p>
<p id="n334" class="pln"><a href="#n334">334</a></p>
<p id="n335" class="pln"><a href="#n335">335</a></p>
<p id="n336" class="stm mis"><a href="#n336">336</a></p>
<p id="n337" class="stm mis"><a href="#n337">337</a></p>
<p id="n338" class="stm mis"><a href="#n338">338</a></p>
<p id="n339" class="stm mis"><a href="#n339">339</a></p>
<p id="n340" class="pln"><a href="#n340">340</a></p>
<p id="n341" class="pln"><a href="#n341">341</a></p>
<p id="n342" class="stm mis"><a href="#n342">342</a></p>
<p id="n343" class="pln"><a href="#n343">343</a></p>
<p id="n344" class="pln"><a href="#n344">344</a></p>
<p id="n345" class="stm run hide_run"><a href="#n345">345</a></p>
<p id="n346" class="stm run hide_run"><a href="#n346">346</a></p>
<p id="n347" class="pln"><a href="#n347">347</a></p>
<p id="n348" class="pln"><a href="#n348">348</a></p>
<p id="n349" class="pln"><a href="#n349">349</a></p>
<p id="n350" class="pln"><a href="#n350">350</a></p>
<p id="n351" class="pln"><a href="#n351">351</a></p>
<p id="n352" class="pln"><a href="#n352">352</a></p>
<p id="n353" class="pln"><a href="#n353">353</a></p>
<p id="n354" class="pln"><a href="#n354">354</a></p>
<p id="n355" class="pln"><a href="#n355">355</a></p>
<p id="n356" class="pln"><a href="#n356">356</a></p>
<p id="n357" class="pln"><a href="#n357">357</a></p>
<p id="n358" class="pln"><a href="#n358">358</a></p>
<p id="n359" class="stm mis"><a href="#n359">359</a></p>
<p id="n360" class="stm mis"><a href="#n360">360</a></p>
<p id="n361" class="stm mis"><a href="#n361">361</a></p>
<p id="n362" class="stm mis"><a href="#n362">362</a></p>
<p id="n363" class="pln"><a href="#n363">363</a></p>
<p id="n364" class="pln"><a href="#n364">364</a></p>
<p id="n365" class="stm mis"><a href="#n365">365</a></p>
<p id="n366" class="pln"><a href="#n366">366</a></p>
<p id="n367" class="pln"><a href="#n367">367</a></p>
<p id="n368" class="stm run hide_run"><a href="#n368">368</a></p>
<p id="n369" class="stm run hide_run"><a href="#n369">369</a></p>
<p id="n370" class="stm run hide_run"><a href="#n370">370</a></p>
<p id="n371" class="pln"><a href="#n371">371</a></p>
<p id="n372" class="pln"><a href="#n372">372</a></p>
<p id="n373" class="pln"><a href="#n373">373</a></p>
<p id="n374" class="pln"><a href="#n374">374</a></p>
<p id="n375" class="pln"><a href="#n375">375</a></p>
<p id="n376" class="pln"><a href="#n376">376</a></p>
<p id="n377" class="pln"><a href="#n377">377</a></p>
<p id="n378" class="pln"><a href="#n378">378</a></p>
<p id="n379" class="pln"><a href="#n379">379</a></p>
<p id="n380" class="pln"><a href="#n380">380</a></p>
<p id="n381" class="stm mis"><a href="#n381">381</a></p>
<p id="n382" class="stm mis"><a href="#n382">382</a></p>
<p id="n383" class="stm mis"><a href="#n383">383</a></p>
<p id="n384" class="stm mis"><a href="#n384">384</a></p>
<p id="n385" class="pln"><a href="#n385">385</a></p>
<p id="n386" class="pln"><a href="#n386">386</a></p>
<p id="n387" class="stm mis"><a href="#n387">387</a></p>
<p id="n388" class="pln"><a href="#n388">388</a></p>
<p id="n389" class="pln"><a href="#n389">389</a></p>
<p id="n390" class="stm run hide_run"><a href="#n390">390</a></p>
<p id="n391" class="pln"><a href="#n391">391</a></p>
<p id="n392" class="pln"><a href="#n392">392</a></p>
<p id="n393" class="pln"><a href="#n393">393</a></p>
<p id="n394" class="pln"><a href="#n394">394</a></p>
<p id="n395" class="pln"><a href="#n395">395</a></p>
<p id="n396" class="pln"><a href="#n396">396</a></p>
<p id="n397" class="pln"><a href="#n397">397</a></p>
<p id="n398" class="pln"><a href="#n398">398</a></p>
<p id="n399" class="pln"><a href="#n399">399</a></p>
<p id="n400" class="pln"><a href="#n400">400</a></p>
<p id="n401" class="pln"><a href="#n401">401</a></p>
<p id="n402" class="pln"><a href="#n402">402</a></p>
<p id="n403" class="pln"><a href="#n403">403</a></p>
<p id="n404" class="pln"><a href="#n404">404</a></p>
<p id="n405" class="pln"><a href="#n405">405</a></p>
<p id="n406" class="pln"><a href="#n406">406</a></p>
<p id="n407" class="pln"><a href="#n407">407</a></p>
<p id="n408" class="stm mis"><a href="#n408">408</a></p>
<p id="n409" class="pln"><a href="#n409">409</a></p>
<p id="n410" class="stm mis"><a href="#n410">410</a></p>
<p id="n411" class="stm mis"><a href="#n411">411</a></p>
<p id="n412" class="stm mis"><a href="#n412">412</a></p>
<p id="n413" class="stm mis"><a href="#n413">413</a></p>
<p id="n414" class="pln"><a href="#n414">414</a></p>
<p id="n415" class="stm mis"><a href="#n415">415</a></p>
<p id="n416" class="pln"><a href="#n416">416</a></p>
<p id="n417" class="stm mis"><a href="#n417">417</a></p>
<p id="n418" class="pln"><a href="#n418">418</a></p>
<p id="n419" class="pln"><a href="#n419">419</a></p>
<p id="n420" class="stm run hide_run"><a href="#n420">420</a></p>
<p id="n421" class="stm run hide_run"><a href="#n421">421</a></p>
<p id="n422" class="pln"><a href="#n422">422</a></p>
<p id="n423" class="pln"><a href="#n423">423</a></p>
<p id="n424" class="pln"><a href="#n424">424</a></p>
<p id="n425" class="pln"><a href="#n425">425</a></p>
<p id="n426" class="pln"><a href="#n426">426</a></p>
<p id="n427" class="pln"><a href="#n427">427</a></p>
<p id="n428" class="pln"><a href="#n428">428</a></p>
<p id="n429" class="pln"><a href="#n429">429</a></p>
<p id="n430" class="pln"><a href="#n430">430</a></p>
<p id="n431" class="pln"><a href="#n431">431</a></p>
<p id="n432" class="pln"><a href="#n432">432</a></p>
<p id="n433" class="pln"><a href="#n433">433</a></p>
<p id="n434" class="pln"><a href="#n434">434</a></p>
<p id="n435" class="pln"><a href="#n435">435</a></p>
<p id="n436" class="pln"><a href="#n436">436</a></p>
<p id="n437" class="pln"><a href="#n437">437</a></p>
<p id="n438" class="pln"><a href="#n438">438</a></p>
<p id="n439" class="pln"><a href="#n439">439</a></p>
<p id="n440" class="pln"><a href="#n440">440</a></p>
<p id="n441" class="pln"><a href="#n441">441</a></p>
<p id="n442" class="pln"><a href="#n442">442</a></p>
<p id="n443" class="stm mis"><a href="#n443">443</a></p>
<p id="n444" class="stm mis"><a href="#n444">444</a></p>
<p id="n445" class="pln"><a href="#n445">445</a></p>
<p id="n446" class="pln"><a href="#n446">446</a></p>
<p id="n447" class="pln"><a href="#n447">447</a></p>
<p id="n448" class="stm run hide_run"><a href="#n448">448</a></p>
<p id="n449" class="stm run hide_run"><a href="#n449">449</a></p>
<p id="n450" class="pln"><a href="#n450">450</a></p>
<p id="n451" class="pln"><a href="#n451">451</a></p>
<p id="n452" class="pln"><a href="#n452">452</a></p>
<p id="n453" class="pln"><a href="#n453">453</a></p>
<p id="n454" class="pln"><a href="#n454">454</a></p>
<p id="n455" class="pln"><a href="#n455">455</a></p>
<p id="n456" class="pln"><a href="#n456">456</a></p>
<p id="n457" class="pln"><a href="#n457">457</a></p>
<p id="n458" class="pln"><a href="#n458">458</a></p>
<p id="n459" class="pln"><a href="#n459">459</a></p>
<p id="n460" class="pln"><a href="#n460">460</a></p>
<p id="n461" class="pln"><a href="#n461">461</a></p>
<p id="n462" class="pln"><a href="#n462">462</a></p>
<p id="n463" class="pln"><a href="#n463">463</a></p>
<p id="n464" class="pln"><a href="#n464">464</a></p>
<p id="n465" class="pln"><a href="#n465">465</a></p>
<p id="n466" class="pln"><a href="#n466">466</a></p>
<p id="n467" class="pln"><a href="#n467">467</a></p>
<p id="n468" class="pln"><a href="#n468">468</a></p>
<p id="n469" class="pln"><a href="#n469">469</a></p>
<p id="n470" class="pln"><a href="#n470">470</a></p>
<p id="n471" class="pln"><a href="#n471">471</a></p>
<p id="n472" class="pln"><a href="#n472">472</a></p>
<p id="n473" class="pln"><a href="#n473">473</a></p>
<p id="n474" class="pln"><a href="#n474">474</a></p>
<p id="n475" class="pln"><a href="#n475">475</a></p>
<p id="n476" class="stm mis"><a href="#n476">476</a></p>
<p id="n477" class="stm mis"><a href="#n477">477</a></p>
<p id="n478" class="stm mis"><a href="#n478">478</a></p>
<p id="n479" class="stm mis"><a href="#n479">479</a></p>
<p id="n480" class="stm mis"><a href="#n480">480</a></p>
<p id="n481" class="stm mis"><a href="#n481">481</a></p>
<p id="n482" class="stm mis"><a href="#n482">482</a></p>
<p id="n483" class="stm mis"><a href="#n483">483</a></p>
<p id="n484" class="pln"><a href="#n484">484</a></p>
<p id="n485" class="stm mis"><a href="#n485">485</a></p>
<p id="n486" class="pln"><a href="#n486">486</a></p>
<p id="n487" class="stm mis"><a href="#n487">487</a></p>
<p id="n488" class="pln"><a href="#n488">488</a></p>
<p id="n489" class="pln"><a href="#n489">489</a></p>
<p id="n490" class="stm run hide_run"><a href="#n490">490</a></p>
<p id="n491" class="stm run hide_run"><a href="#n491">491</a></p>
<p id="n492" class="stm run hide_run"><a href="#n492">492</a></p>
<p id="n493" class="pln"><a href="#n493">493</a></p>
<p id="n494" class="pln"><a href="#n494">494</a></p>
<p id="n495" class="pln"><a href="#n495">495</a></p>
<p id="n496" class="pln"><a href="#n496">496</a></p>
<p id="n497" class="pln"><a href="#n497">497</a></p>
<p id="n498" class="pln"><a href="#n498">498</a></p>
<p id="n499" class="pln"><a href="#n499">499</a></p>
<p id="n500" class="pln"><a href="#n500">500</a></p>
<p id="n501" class="pln"><a href="#n501">501</a></p>
<p id="n502" class="pln"><a href="#n502">502</a></p>
<p id="n503" class="pln"><a href="#n503">503</a></p>
<p id="n504" class="pln"><a href="#n504">504</a></p>
<p id="n505" class="pln"><a href="#n505">505</a></p>
<p id="n506" class="pln"><a href="#n506">506</a></p>
<p id="n507" class="pln"><a href="#n507">507</a></p>
<p id="n508" class="pln"><a href="#n508">508</a></p>
<p id="n509" class="pln"><a href="#n509">509</a></p>
<p id="n510" class="pln"><a href="#n510">510</a></p>
<p id="n511" class="pln"><a href="#n511">511</a></p>
<p id="n512" class="pln"><a href="#n512">512</a></p>
<p id="n513" class="pln"><a href="#n513">513</a></p>
<p id="n514" class="stm mis"><a href="#n514">514</a></p>
<p id="n515" class="stm mis"><a href="#n515">515</a></p>
<p id="n516" class="stm mis"><a href="#n516">516</a></p>
<p id="n517" class="stm mis"><a href="#n517">517</a></p>
<p id="n518" class="pln"><a href="#n518">518</a></p>
<p id="n519" class="stm mis"><a href="#n519">519</a></p>
<p id="n520" class="pln"><a href="#n520">520</a></p>
<p id="n521" class="pln"><a href="#n521">521</a></p>
<p id="n522" class="stm run hide_run"><a href="#n522">522</a></p>
<p id="n523" class="stm run hide_run"><a href="#n523">523</a></p>
<p id="n524" class="stm run hide_run"><a href="#n524">524</a></p>
<p id="n525" class="pln"><a href="#n525">525</a></p>
<p id="n526" class="pln"><a href="#n526">526</a></p>
<p id="n527" class="pln"><a href="#n527">527</a></p>
<p id="n528" class="pln"><a href="#n528">528</a></p>
<p id="n529" class="pln"><a href="#n529">529</a></p>
<p id="n530" class="pln"><a href="#n530">530</a></p>
<p id="n531" class="pln"><a href="#n531">531</a></p>
<p id="n532" class="pln"><a href="#n532">532</a></p>
<p id="n533" class="pln"><a href="#n533">533</a></p>
<p id="n534" class="pln"><a href="#n534">534</a></p>
<p id="n535" class="pln"><a href="#n535">535</a></p>
<p id="n536" class="pln"><a href="#n536">536</a></p>
<p id="n537" class="pln"><a href="#n537">537</a></p>
<p id="n538" class="pln"><a href="#n538">538</a></p>
<p id="n539" class="pln"><a href="#n539">539</a></p>
<p id="n540" class="pln"><a href="#n540">540</a></p>
<p id="n541" class="pln"><a href="#n541">541</a></p>
<p id="n542" class="pln"><a href="#n542">542</a></p>
<p id="n543" class="pln"><a href="#n543">543</a></p>
<p id="n544" class="pln"><a href="#n544">544</a></p>
<p id="n545" class="pln"><a href="#n545">545</a></p>
<p id="n546" class="stm mis"><a href="#n546">546</a></p>
<p id="n547" class="stm mis"><a href="#n547">547</a></p>
<p id="n548" class="stm mis"><a href="#n548">548</a></p>
<p id="n549" class="pln"><a href="#n549">549</a></p>
<p id="n550" class="stm mis"><a href="#n550">550</a></p>
<p id="n551" class="pln"><a href="#n551">551</a></p>
<p id="n552" class="pln"><a href="#n552">552</a></p>
<p id="n553" class="stm run hide_run"><a href="#n553">553</a></p>
<p id="n554" class="stm run hide_run"><a href="#n554">554</a></p>
<p id="n555" class="stm run hide_run"><a href="#n555">555</a></p>
<p id="n556" class="pln"><a href="#n556">556</a></p>
<p id="n557" class="pln"><a href="#n557">557</a></p>
<p id="n558" class="pln"><a href="#n558">558</a></p>
<p id="n559" class="pln"><a href="#n559">559</a></p>
<p id="n560" class="pln"><a href="#n560">560</a></p>
<p id="n561" class="pln"><a href="#n561">561</a></p>
<p id="n562" class="pln"><a href="#n562">562</a></p>
<p id="n563" class="pln"><a href="#n563">563</a></p>
<p id="n564" class="pln"><a href="#n564">564</a></p>
<p id="n565" class="pln"><a href="#n565">565</a></p>
<p id="n566" class="pln"><a href="#n566">566</a></p>
<p id="n567" class="pln"><a href="#n567">567</a></p>
<p id="n568" class="pln"><a href="#n568">568</a></p>
<p id="n569" class="pln"><a href="#n569">569</a></p>
<p id="n570" class="pln"><a href="#n570">570</a></p>
<p id="n571" class="pln"><a href="#n571">571</a></p>
<p id="n572" class="pln"><a href="#n572">572</a></p>
<p id="n573" class="pln"><a href="#n573">573</a></p>
<p id="n574" class="pln"><a href="#n574">574</a></p>
<p id="n575" class="pln"><a href="#n575">575</a></p>
<p id="n576" class="pln"><a href="#n576">576</a></p>
<p id="n577" class="pln"><a href="#n577">577</a></p>
<p id="n578" class="pln"><a href="#n578">578</a></p>
<p id="n579" class="pln"><a href="#n579">579</a></p>
<p id="n580" class="pln"><a href="#n580">580</a></p>
<p id="n581" class="pln"><a href="#n581">581</a></p>
<p id="n582" class="pln"><a href="#n582">582</a></p>
<p id="n583" class="stm mis"><a href="#n583">583</a></p>
<p id="n584" class="stm mis"><a href="#n584">584</a></p>
<p id="n585" class="stm mis"><a href="#n585">585</a></p>
<p id="n586" class="pln"><a href="#n586">586</a></p>
<p id="n587" class="stm mis"><a href="#n587">587</a></p>
<p id="n588" class="pln"><a href="#n588">588</a></p>
<p id="n589" class="pln"><a href="#n589">589</a></p>
<p id="n590" class="pln"><a href="#n590">590</a></p>
<p id="n591" class="pln"><a href="#n591">591</a></p>
<p id="n592" class="pln"><a href="#n592">592</a></p>
<p id="n593" class="stm run hide_run"><a href="#n593">593</a></p>
<p id="n594" class="stm run hide_run"><a href="#n594">594</a></p>
<p id="n595" class="pln"><a href="#n595">595</a></p>
<p id="n596" class="pln"><a href="#n596">596</a></p>
<p id="n597" class="pln"><a href="#n597">597</a></p>
<p id="n598" class="pln"><a href="#n598">598</a></p>
<p id="n599" class="pln"><a href="#n599">599</a></p>
<p id="n600" class="pln"><a href="#n600">600</a></p>
<p id="n601" class="pln"><a href="#n601">601</a></p>
<p id="n602" class="pln"><a href="#n602">602</a></p>
<p id="n603" class="pln"><a href="#n603">603</a></p>
<p id="n604" class="pln"><a href="#n604">604</a></p>
<p id="n605" class="pln"><a href="#n605">605</a></p>
<p id="n606" class="pln"><a href="#n606">606</a></p>
<p id="n607" class="pln"><a href="#n607">607</a></p>
<p id="n608" class="pln"><a href="#n608">608</a></p>
<p id="n609" class="pln"><a href="#n609">609</a></p>
<p id="n610" class="pln"><a href="#n610">610</a></p>
<p id="n611" class="pln"><a href="#n611">611</a></p>
<p id="n612" class="pln"><a href="#n612">612</a></p>
<p id="n613" class="stm mis"><a href="#n613">613</a></p>
<p id="n614" class="stm mis"><a href="#n614">614</a></p>
<p id="n615" class="stm mis"><a href="#n615">615</a></p>
<p id="n616" class="pln"><a href="#n616">616</a></p>
<p id="n617" class="stm mis"><a href="#n617">617</a></p>
<p id="n618" class="pln"><a href="#n618">618</a></p>
<p id="n619" class="pln"><a href="#n619">619</a></p>
<p id="n620" class="stm run hide_run"><a href="#n620">620</a></p>
<p id="n621" class="stm run hide_run"><a href="#n621">621</a></p>
<p id="n622" class="pln"><a href="#n622">622</a></p>
<p id="n623" class="pln"><a href="#n623">623</a></p>
<p id="n624" class="pln"><a href="#n624">624</a></p>
<p id="n625" class="pln"><a href="#n625">625</a></p>
<p id="n626" class="pln"><a href="#n626">626</a></p>
<p id="n627" class="pln"><a href="#n627">627</a></p>
<p id="n628" class="pln"><a href="#n628">628</a></p>
<p id="n629" class="pln"><a href="#n629">629</a></p>
<p id="n630" class="pln"><a href="#n630">630</a></p>
<p id="n631" class="pln"><a href="#n631">631</a></p>
<p id="n632" class="pln"><a href="#n632">632</a></p>
<p id="n633" class="pln"><a href="#n633">633</a></p>
<p id="n634" class="pln"><a href="#n634">634</a></p>
<p id="n635" class="pln"><a href="#n635">635</a></p>
<p id="n636" class="pln"><a href="#n636">636</a></p>
<p id="n637" class="pln"><a href="#n637">637</a></p>
<p id="n638" class="pln"><a href="#n638">638</a></p>
<p id="n639" class="pln"><a href="#n639">639</a></p>
<p id="n640" class="pln"><a href="#n640">640</a></p>
<p id="n641" class="pln"><a href="#n641">641</a></p>
<p id="n642" class="pln"><a href="#n642">642</a></p>
<p id="n643" class="pln"><a href="#n643">643</a></p>
<p id="n644" class="pln"><a href="#n644">644</a></p>
<p id="n645" class="pln"><a href="#n645">645</a></p>
<p id="n646" class="pln"><a href="#n646">646</a></p>
<p id="n647" class="pln"><a href="#n647">647</a></p>
<p id="n648" class="pln"><a href="#n648">648</a></p>
<p id="n649" class="pln"><a href="#n649">649</a></p>
<p id="n650" class="pln"><a href="#n650">650</a></p>
<p id="n651" class="pln"><a href="#n651">651</a></p>
<p id="n652" class="pln"><a href="#n652">652</a></p>
<p id="n653" class="pln"><a href="#n653">653</a></p>
<p id="n654" class="pln"><a href="#n654">654</a></p>
<p id="n655" class="pln"><a href="#n655">655</a></p>
<p id="n656" class="pln"><a href="#n656">656</a></p>
<p id="n657" class="pln"><a href="#n657">657</a></p>
<p id="n658" class="pln"><a href="#n658">658</a></p>
<p id="n659" class="stm mis"><a href="#n659">659</a></p>
<p id="n660" class="stm mis"><a href="#n660">660</a></p>
<p id="n661" class="pln"><a href="#n661">661</a></p>
<p id="n662" class="stm mis"><a href="#n662">662</a></p>
<p id="n663" class="stm mis"><a href="#n663">663</a></p>
<p id="n664" class="stm mis"><a href="#n664">664</a></p>
<p id="n665" class="stm mis"><a href="#n665">665</a></p>
<p id="n666" class="stm mis"><a href="#n666">666</a></p>
<p id="n667" class="stm mis"><a href="#n667">667</a></p>
<p id="n668" class="stm mis"><a href="#n668">668</a></p>
<p id="n669" class="stm mis"><a href="#n669">669</a></p>
<p id="n670" class="pln"><a href="#n670">670</a></p>
<p id="n671" class="pln"><a href="#n671">671</a></p>
<p id="n672" class="pln"><a href="#n672">672</a></p>
<p id="n673" class="pln"><a href="#n673">673</a></p>
<p id="n674" class="pln"><a href="#n674">674</a></p>
<p id="n675" class="stm mis"><a href="#n675">675</a></p>
<p id="n676" class="stm mis"><a href="#n676">676</a></p>
<p id="n677" class="stm mis"><a href="#n677">677</a></p>
<p id="n678" class="stm mis"><a href="#n678">678</a></p>
<p id="n679" class="stm mis"><a href="#n679">679</a></p>
<p id="n680" class="stm mis"><a href="#n680">680</a></p>
<p id="n681" class="pln"><a href="#n681">681</a></p>
<p id="n682" class="pln"><a href="#n682">682</a></p>
<p id="n683" class="stm run hide_run"><a href="#n683">683</a></p>
<p id="n684" class="stm run hide_run"><a href="#n684">684</a></p>
<p id="n685" class="pln"><a href="#n685">685</a></p>
<p id="n686" class="pln"><a href="#n686">686</a></p>
<p id="n687" class="pln"><a href="#n687">687</a></p>
<p id="n688" class="pln"><a href="#n688">688</a></p>
<p id="n689" class="pln"><a href="#n689">689</a></p>
<p id="n690" class="pln"><a href="#n690">690</a></p>
<p id="n691" class="pln"><a href="#n691">691</a></p>
<p id="n692" class="pln"><a href="#n692">692</a></p>
<p id="n693" class="pln"><a href="#n693">693</a></p>
<p id="n694" class="pln"><a href="#n694">694</a></p>
<p id="n695" class="pln"><a href="#n695">695</a></p>
<p id="n696" class="pln"><a href="#n696">696</a></p>
<p id="n697" class="pln"><a href="#n697">697</a></p>
<p id="n698" class="pln"><a href="#n698">698</a></p>
<p id="n699" class="pln"><a href="#n699">699</a></p>
<p id="n700" class="pln"><a href="#n700">700</a></p>
<p id="n701" class="stm mis"><a href="#n701">701</a></p>
<p id="n702" class="stm mis"><a href="#n702">702</a></p>
<p id="n703" class="stm mis"><a href="#n703">703</a></p>
<p id="n704" class="stm mis"><a href="#n704">704</a></p>
<p id="n705" class="stm mis"><a href="#n705">705</a></p>
<p id="n706" class="pln"><a href="#n706">706</a></p>
<p id="n707" class="pln"><a href="#n707">707</a></p>
<p id="n708" class="pln"><a href="#n708">708</a></p>
<p id="n709" class="stm mis"><a href="#n709">709</a></p>
<p id="n710" class="stm mis"><a href="#n710">710</a></p>
<p id="n711" class="pln"><a href="#n711">711</a></p>
<p id="n712" class="pln"><a href="#n712">712</a></p>
<p id="n713" class="pln"><a href="#n713">713</a></p>
<p id="n714" class="stm mis"><a href="#n714">714</a></p>
<p id="n715" class="pln"><a href="#n715">715</a></p>
<p id="n716" class="pln"><a href="#n716">716</a></p>
<p id="n717" class="stm run hide_run"><a href="#n717">717</a></p>
<p id="n718" class="stm run hide_run"><a href="#n718">718</a></p>
<p id="n719" class="pln"><a href="#n719">719</a></p>
<p id="n720" class="pln"><a href="#n720">720</a></p>
<p id="n721" class="pln"><a href="#n721">721</a></p>
<p id="n722" class="pln"><a href="#n722">722</a></p>
<p id="n723" class="pln"><a href="#n723">723</a></p>
<p id="n724" class="pln"><a href="#n724">724</a></p>
<p id="n725" class="pln"><a href="#n725">725</a></p>
<p id="n726" class="pln"><a href="#n726">726</a></p>
<p id="n727" class="pln"><a href="#n727">727</a></p>
<p id="n728" class="pln"><a href="#n728">728</a></p>
<p id="n729" class="pln"><a href="#n729">729</a></p>
<p id="n730" class="pln"><a href="#n730">730</a></p>
<p id="n731" class="pln"><a href="#n731">731</a></p>
<p id="n732" class="stm mis"><a href="#n732">732</a></p>
<p id="n733" class="pln"><a href="#n733">733</a></p>
<p id="n734" class="pln"><a href="#n734">734</a></p>
<p id="n735" class="stm run hide_run"><a href="#n735">735</a></p>
<p id="n736" class="stm run hide_run"><a href="#n736">736</a></p>
<p id="n737" class="pln"><a href="#n737">737</a></p>
<p id="n738" class="pln"><a href="#n738">738</a></p>
<p id="n739" class="pln"><a href="#n739">739</a></p>
<p id="n740" class="pln"><a href="#n740">740</a></p>
<p id="n741" class="pln"><a href="#n741">741</a></p>
<p id="n742" class="pln"><a href="#n742">742</a></p>
<p id="n743" class="pln"><a href="#n743">743</a></p>
<p id="n744" class="pln"><a href="#n744">744</a></p>
<p id="n745" class="pln"><a href="#n745">745</a></p>
<p id="n746" class="pln"><a href="#n746">746</a></p>
<p id="n747" class="pln"><a href="#n747">747</a></p>
<p id="n748" class="pln"><a href="#n748">748</a></p>
<p id="n749" class="pln"><a href="#n749">749</a></p>
<p id="n750" class="stm mis"><a href="#n750">750</a></p>
<p id="n751" class="pln"><a href="#n751">751</a></p>
<p id="n752" class="pln"><a href="#n752">752</a></p>
<p id="n753" class="stm run hide_run"><a href="#n753">753</a></p>
<p id="n754" class="stm run hide_run"><a href="#n754">754</a></p>
<p id="n755" class="pln"><a href="#n755">755</a></p>
<p id="n756" class="pln"><a href="#n756">756</a></p>
<p id="n757" class="pln"><a href="#n757">757</a></p>
<p id="n758" class="pln"><a href="#n758">758</a></p>
<p id="n759" class="pln"><a href="#n759">759</a></p>
<p id="n760" class="pln"><a href="#n760">760</a></p>
<p id="n761" class="pln"><a href="#n761">761</a></p>
<p id="n762" class="pln"><a href="#n762">762</a></p>
<p id="n763" class="pln"><a href="#n763">763</a></p>
<p id="n764" class="pln"><a href="#n764">764</a></p>
<p id="n765" class="pln"><a href="#n765">765</a></p>
<p id="n766" class="pln"><a href="#n766">766</a></p>
<p id="n767" class="pln"><a href="#n767">767</a></p>
<p id="n768" class="stm mis"><a href="#n768">768</a></p>
<p id="n769" class="pln"><a href="#n769">769</a></p>
<p id="n770" class="pln"><a href="#n770">770</a></p>
<p id="n771" class="stm run hide_run"><a href="#n771">771</a></p>
<p id="n772" class="stm run hide_run"><a href="#n772">772</a></p>
<p id="n773" class="pln"><a href="#n773">773</a></p>
<p id="n774" class="pln"><a href="#n774">774</a></p>
<p id="n775" class="pln"><a href="#n775">775</a></p>
<p id="n776" class="pln"><a href="#n776">776</a></p>
<p id="n777" class="pln"><a href="#n777">777</a></p>
<p id="n778" class="pln"><a href="#n778">778</a></p>
<p id="n779" class="pln"><a href="#n779">779</a></p>
<p id="n780" class="pln"><a href="#n780">780</a></p>
<p id="n781" class="pln"><a href="#n781">781</a></p>
<p id="n782" class="pln"><a href="#n782">782</a></p>
<p id="n783" class="pln"><a href="#n783">783</a></p>
<p id="n784" class="pln"><a href="#n784">784</a></p>
<p id="n785" class="pln"><a href="#n785">785</a></p>
<p id="n786" class="stm mis"><a href="#n786">786</a></p>
<p id="n787" class="pln"><a href="#n787">787</a></p>
<p id="n788" class="pln"><a href="#n788">788</a></p>
<p id="n789" class="stm run hide_run"><a href="#n789">789</a></p>
<p id="n790" class="stm run hide_run"><a href="#n790">790</a></p>
<p id="n791" class="pln"><a href="#n791">791</a></p>
<p id="n792" class="pln"><a href="#n792">792</a></p>
<p id="n793" class="pln"><a href="#n793">793</a></p>
<p id="n794" class="pln"><a href="#n794">794</a></p>
<p id="n795" class="pln"><a href="#n795">795</a></p>
<p id="n796" class="pln"><a href="#n796">796</a></p>
<p id="n797" class="pln"><a href="#n797">797</a></p>
<p id="n798" class="pln"><a href="#n798">798</a></p>
<p id="n799" class="pln"><a href="#n799">799</a></p>
<p id="n800" class="pln"><a href="#n800">800</a></p>
<p id="n801" class="pln"><a href="#n801">801</a></p>
<p id="n802" class="pln"><a href="#n802">802</a></p>
<p id="n803" class="pln"><a href="#n803">803</a></p>
<p id="n804" class="stm mis"><a href="#n804">804</a></p>
<p id="n805" class="pln"><a href="#n805">805</a></p>
<p id="n806" class="pln"><a href="#n806">806</a></p>
<p id="n807" class="stm run hide_run"><a href="#n807">807</a></p>
<p id="n808" class="stm run hide_run"><a href="#n808">808</a></p>
<p id="n809" class="pln"><a href="#n809">809</a></p>
<p id="n810" class="pln"><a href="#n810">810</a></p>
<p id="n811" class="pln"><a href="#n811">811</a></p>
<p id="n812" class="pln"><a href="#n812">812</a></p>
<p id="n813" class="pln"><a href="#n813">813</a></p>
<p id="n814" class="pln"><a href="#n814">814</a></p>
<p id="n815" class="pln"><a href="#n815">815</a></p>
<p id="n816" class="pln"><a href="#n816">816</a></p>
<p id="n817" class="pln"><a href="#n817">817</a></p>
<p id="n818" class="pln"><a href="#n818">818</a></p>
<p id="n819" class="pln"><a href="#n819">819</a></p>
<p id="n820" class="pln"><a href="#n820">820</a></p>
<p id="n821" class="pln"><a href="#n821">821</a></p>
<p id="n822" class="stm mis"><a href="#n822">822</a></p>
<p id="n823" class="pln"><a href="#n823">823</a></p>
<p id="n824" class="pln"><a href="#n824">824</a></p>
<p id="n825" class="stm run hide_run"><a href="#n825">825</a></p>
<p id="n826" class="stm run hide_run"><a href="#n826">826</a></p>
<p id="n827" class="pln"><a href="#n827">827</a></p>
<p id="n828" class="pln"><a href="#n828">828</a></p>
<p id="n829" class="pln"><a href="#n829">829</a></p>
<p id="n830" class="pln"><a href="#n830">830</a></p>
<p id="n831" class="pln"><a href="#n831">831</a></p>
<p id="n832" class="pln"><a href="#n832">832</a></p>
<p id="n833" class="pln"><a href="#n833">833</a></p>
<p id="n834" class="pln"><a href="#n834">834</a></p>
<p id="n835" class="pln"><a href="#n835">835</a></p>
<p id="n836" class="pln"><a href="#n836">836</a></p>
<p id="n837" class="pln"><a href="#n837">837</a></p>
<p id="n838" class="pln"><a href="#n838">838</a></p>
<p id="n839" class="pln"><a href="#n839">839</a></p>
<p id="n840" class="stm mis"><a href="#n840">840</a></p>
<p id="n841" class="pln"><a href="#n841">841</a></p>
<p id="n842" class="pln"><a href="#n842">842</a></p>
<p id="n843" class="stm run hide_run"><a href="#n843">843</a></p>
<p id="n844" class="stm run hide_run"><a href="#n844">844</a></p>
<p id="n845" class="pln"><a href="#n845">845</a></p>
<p id="n846" class="pln"><a href="#n846">846</a></p>
<p id="n847" class="pln"><a href="#n847">847</a></p>
<p id="n848" class="stm run hide_run"><a href="#n848">848</a></p>
<p id="n849" class="pln"><a href="#n849">849</a></p>
<p id="n850" class="pln"><a href="#n850">850</a></p>
<p id="n851" class="stm run hide_run"><a href="#n851">851</a></p>
<p id="n852" class="pln"><a href="#n852">852</a></p>
<p id="n853" class="pln"><a href="#n853">853</a></p>
<p id="n854" class="pln"><a href="#n854">854</a></p>
<p id="n855" class="pln"><a href="#n855">855</a></p>
<p id="n856" class="pln"><a href="#n856">856</a></p>
<p id="n857" class="pln"><a href="#n857">857</a></p>
<p id="n858" class="pln"><a href="#n858">858</a></p>
<p id="n859" class="pln"><a href="#n859">859</a></p>
<p id="n860" class="pln"><a href="#n860">860</a></p>
<p id="n861" class="pln"><a href="#n861">861</a></p>
<p id="n862" class="pln"><a href="#n862">862</a></p>
<p id="n863" class="stm run hide_run"><a href="#n863">863</a></p>
<p id="n864" class="stm run hide_run"><a href="#n864">864</a></p>
<p id="n865" class="stm run hide_run"><a href="#n865">865</a></p>
<p id="n866" class="stm run hide_run"><a href="#n866">866</a></p>
<p id="n867" class="stm mis"><a href="#n867">867</a></p>
<p id="n868" class="stm mis"><a href="#n868">868</a></p>
<p id="n869" class="stm mis"><a href="#n869">869</a></p>
<p id="n870" class="stm mis"><a href="#n870">870</a></p>
<p id="n871" class="pln"><a href="#n871">871</a></p>
<p id="n872" class="pln"><a href="#n872">872</a></p>
<p id="n873" class="pln"><a href="#n873">873</a></p>
<p id="n874" class="stm mis"><a href="#n874">874</a></p>
<p id="n875" class="stm mis"><a href="#n875">875</a></p>
<p id="n876" class="pln"><a href="#n876">876</a></p>
<p id="n877" class="stm mis"><a href="#n877">877</a></p>
<p id="n878" class="stm mis"><a href="#n878">878</a></p>
<p id="n879" class="pln"><a href="#n879">879</a></p>
<p id="n880" class="stm run hide_run"><a href="#n880">880</a></p>
<p id="n881" class="stm mis"><a href="#n881">881</a></p>
<p id="n882" class="stm mis"><a href="#n882">882</a></p>
<p id="n883" class="stm mis"><a href="#n883">883</a></p>
<p id="n884" class="pln"><a href="#n884">884</a></p>
<p id="n885" class="pln"><a href="#n885">885</a></p>
<p id="n886" class="pln"><a href="#n886">886</a></p>
<p id="n887" class="pln"><a href="#n887">887</a></p>
<p id="n888" class="pln"><a href="#n888">888</a></p>
<p id="n889" class="pln"><a href="#n889">889</a></p>
<p id="n890" class="pln"><a href="#n890">890</a></p>
<p id="n891" class="stm run hide_run"><a href="#n891">891</a></p>
<p id="n892" class="stm mis"><a href="#n892">892</a></p>
<p id="n893" class="stm mis"><a href="#n893">893</a></p>
<p id="n894" class="stm mis"><a href="#n894">894</a></p>
<p id="n895" class="pln"><a href="#n895">895</a></p>
<p id="n896" class="pln"><a href="#n896">896</a></p>
<p id="n897" class="stm run hide_run"><a href="#n897">897</a></p>
<p id="n898" class="stm run hide_run"><a href="#n898">898</a></p>
<p id="n899" class="stm mis"><a href="#n899">899</a></p>
<p id="n900" class="stm mis"><a href="#n900">900</a></p>
<p id="n901" class="stm run hide_run"><a href="#n901">901</a></p>
<p id="n902" class="stm run hide_run"><a href="#n902">902</a></p>
<p id="n903" class="stm run hide_run"><a href="#n903">903</a></p>
<p id="n904" class="pln"><a href="#n904">904</a></p>
<p id="n905" class="stm run hide_run"><a href="#n905">905</a></p>
<p id="n906" class="stm run hide_run"><a href="#n906">906</a></p>
<p id="n907" class="stm run hide_run"><a href="#n907">907</a></p>
<p id="n908" class="stm run hide_run"><a href="#n908">908</a></p>
<p id="n909" class="stm run hide_run"><a href="#n909">909</a></p>
<p id="n910" class="pln"><a href="#n910">910</a></p>
<p id="n911" class="stm run hide_run"><a href="#n911">911</a></p>
<p id="n912" class="pln"><a href="#n912">912</a></p>
<p id="n913" class="stm run hide_run"><a href="#n913">913</a></p>
<p id="n914" class="pln"><a href="#n914">914</a></p>
<p id="n915" class="pln"><a href="#n915">915</a></p>
<p id="n916" class="pln"><a href="#n916">916</a></p>
<p id="n917" class="stm run hide_run"><a href="#n917">917</a></p>
<p id="n918" class="pln"><a href="#n918">918</a></p>
<p id="n919" class="pln"><a href="#n919">919</a></p>
<p id="n920" class="pln"><a href="#n920">920</a></p>
<p id="n921" class="pln"><a href="#n921">921</a></p>
<p id="n922" class="pln"><a href="#n922">922</a></p>
<p id="n923" class="pln"><a href="#n923">923</a></p>
<p id="n924" class="pln"><a href="#n924">924</a></p>
<p id="n925" class="pln"><a href="#n925">925</a></p>
<p id="n926" class="pln"><a href="#n926">926</a></p>
<p id="n927" class="pln"><a href="#n927">927</a></p>
<p id="n928" class="pln"><a href="#n928">928</a></p>
<p id="n929" class="pln"><a href="#n929">929</a></p>
<p id="n930" class="pln"><a href="#n930">930</a></p>
<p id="n931" class="pln"><a href="#n931">931</a></p>
<p id="n932" class="pln"><a href="#n932">932</a></p>
<p id="n933" class="pln"><a href="#n933">933</a></p>
<p id="n934" class="pln"><a href="#n934">934</a></p>
<p id="n935" class="pln"><a href="#n935">935</a></p>
<p id="n936" class="stm run hide_run"><a href="#n936">936</a></p>
<p id="n937" class="pln"><a href="#n937">937</a></p>
<p id="n938" class="stm mis"><a href="#n938">938</a></p>
<p id="n939" class="pln"><a href="#n939">939</a></p>
<p id="n940" class="stm mis"><a href="#n940">940</a></p>
<p id="n941" class="stm mis"><a href="#n941">941</a></p>
<p id="n942" class="stm mis"><a href="#n942">942</a></p>
<p id="n943" class="stm mis"><a href="#n943">943</a></p>
<p id="n944" class="stm mis"><a href="#n944">944</a></p>
<p id="n945" class="stm mis"><a href="#n945">945</a></p>
<p id="n946" class="pln"><a href="#n946">946</a></p>
<p id="n947" class="stm mis"><a href="#n947">947</a></p>
<p id="n948" class="stm mis"><a href="#n948">948</a></p>
<p id="n949" class="stm mis"><a href="#n949">949</a></p>
<p id="n950" class="stm mis"><a href="#n950">950</a></p>
<p id="n951" class="stm mis"><a href="#n951">951</a></p>
<p id="n952" class="stm mis"><a href="#n952">952</a></p>
<p id="n953" class="stm mis"><a href="#n953">953</a></p>
<p id="n954" class="stm mis"><a href="#n954">954</a></p>
<p id="n955" class="pln"><a href="#n955">955</a></p>
<p id="n956" class="pln"><a href="#n956">956</a></p>
<p id="n957" class="pln"><a href="#n957">957</a></p>
<p id="n958" class="stm run hide_run"><a href="#n958">958</a></p>
<p id="n959" class="stm mis"><a href="#n959">959</a></p>
<p id="n960" class="stm mis"><a href="#n960">960</a></p>
<p id="n961" class="stm mis"><a href="#n961">961</a></p>
<p id="n962" class="stm mis"><a href="#n962">962</a></p>
<p id="n963" class="stm mis"><a href="#n963">963</a></p>
<p id="n964" class="stm mis"><a href="#n964">964</a></p>
<p id="n965" class="stm mis"><a href="#n965">965</a></p>
<p id="n966" class="pln"><a href="#n966">966</a></p>
<p id="n967" class="stm mis"><a href="#n967">967</a></p>
<p id="n968" class="stm mis"><a href="#n968">968</a></p>
<p id="n969" class="stm mis"><a href="#n969">969</a></p>
<p id="n970" class="stm mis"><a href="#n970">970</a></p>
<p id="n971" class="stm mis"><a href="#n971">971</a></p>
<p id="n972" class="stm mis"><a href="#n972">972</a></p>
<p id="n973" class="stm mis"><a href="#n973">973</a></p>
<p id="n974" class="stm mis"><a href="#n974">974</a></p>
<p id="n975" class="pln"><a href="#n975">975</a></p>
<p id="n976" class="pln"><a href="#n976">976</a></p>
<p id="n977" class="stm run hide_run"><a href="#n977">977</a></p>
<p id="n978" class="pln"><a href="#n978">978</a></p>
<p id="n979" class="pln"><a href="#n979">979</a></p>
<p id="n980" class="pln"><a href="#n980">980</a></p>
<p id="n981" class="pln"><a href="#n981">981</a></p>
<p id="n982" class="pln"><a href="#n982">982</a></p>
<p id="n983" class="pln"><a href="#n983">983</a></p>
<p id="n984" class="pln"><a href="#n984">984</a></p>
<p id="n985" class="pln"><a href="#n985">985</a></p>
<p id="n986" class="pln"><a href="#n986">986</a></p>
<p id="n987" class="pln"><a href="#n987">987</a></p>
<p id="n988" class="stm mis"><a href="#n988">988</a></p>
<p id="n989" class="stm mis"><a href="#n989">989</a></p>
<p id="n990" class="stm mis"><a href="#n990">990</a></p>
<p id="n991" class="stm mis"><a href="#n991">991</a></p>
<p id="n992" class="stm mis"><a href="#n992">992</a></p>
<p id="n993" class="stm mis"><a href="#n993">993</a></p>
<p id="n994" class="stm mis"><a href="#n994">994</a></p>
<p id="n995" class="pln"><a href="#n995">995</a></p>
<p id="n996" class="stm mis"><a href="#n996">996</a></p>
<p id="n997" class="stm mis"><a href="#n997">997</a></p>
<p id="n998" class="pln"><a href="#n998">998</a></p>
<p id="n999" class="stm mis"><a href="#n999">999</a></p>
<p id="n1000" class="pln"><a href="#n1000">1000</a></p>
<p id="n1001" class="pln"><a href="#n1001">1001</a></p>
<p id="n1002" class="stm run hide_run"><a href="#n1002">1002</a></p>
<p id="n1003" class="stm run hide_run"><a href="#n1003">1003</a></p>
<p id="n1004" class="pln"><a href="#n1004">1004</a></p>
<p id="n1005" class="pln"><a href="#n1005">1005</a></p>
<p id="n1006" class="pln"><a href="#n1006">1006</a></p>
<p id="n1007" class="pln"><a href="#n1007">1007</a></p>
<p id="n1008" class="pln"><a href="#n1008">1008</a></p>
<p id="n1009" class="pln"><a href="#n1009">1009</a></p>
<p id="n1010" class="pln"><a href="#n1010">1010</a></p>
<p id="n1011" class="pln"><a href="#n1011">1011</a></p>
<p id="n1012" class="pln"><a href="#n1012">1012</a></p>
<p id="n1013" class="pln"><a href="#n1013">1013</a></p>
<p id="n1014" class="pln"><a href="#n1014">1014</a></p>
<p id="n1015" class="pln"><a href="#n1015">1015</a></p>
<p id="n1016" class="pln"><a href="#n1016">1016</a></p>
<p id="n1017" class="pln"><a href="#n1017">1017</a></p>
<p id="n1018" class="pln"><a href="#n1018">1018</a></p>
<p id="n1019" class="pln"><a href="#n1019">1019</a></p>
<p id="n1020" class="pln"><a href="#n1020">1020</a></p>
<p id="n1021" class="pln"><a href="#n1021">1021</a></p>
<p id="n1022" class="pln"><a href="#n1022">1022</a></p>
<p id="n1023" class="pln"><a href="#n1023">1023</a></p>
<p id="n1024" class="pln"><a href="#n1024">1024</a></p>
<p id="n1025" class="pln"><a href="#n1025">1025</a></p>
<p id="n1026" class="pln"><a href="#n1026">1026</a></p>
<p id="n1027" class="pln"><a href="#n1027">1027</a></p>
<p id="n1028" class="pln"><a href="#n1028">1028</a></p>
<p id="n1029" class="pln"><a href="#n1029">1029</a></p>
<p id="n1030" class="pln"><a href="#n1030">1030</a></p>
<p id="n1031" class="stm mis"><a href="#n1031">1031</a></p>
<p id="n1032" class="pln"><a href="#n1032">1032</a></p>
<p id="n1033" class="pln"><a href="#n1033">1033</a></p>
<p id="n1034" class="stm run hide_run"><a href="#n1034">1034</a></p>
<p id="n1035" class="stm run hide_run"><a href="#n1035">1035</a></p>
<p id="n1036" class="pln"><a href="#n1036">1036</a></p>
<p id="n1037" class="pln"><a href="#n1037">1037</a></p>
<p id="n1038" class="pln"><a href="#n1038">1038</a></p>
<p id="n1039" class="pln"><a href="#n1039">1039</a></p>
<p id="n1040" class="pln"><a href="#n1040">1040</a></p>
<p id="n1041" class="pln"><a href="#n1041">1041</a></p>
<p id="n1042" class="pln"><a href="#n1042">1042</a></p>
<p id="n1043" class="pln"><a href="#n1043">1043</a></p>
<p id="n1044" class="pln"><a href="#n1044">1044</a></p>
<p id="n1045" class="pln"><a href="#n1045">1045</a></p>
<p id="n1046" class="pln"><a href="#n1046">1046</a></p>
<p id="n1047" class="pln"><a href="#n1047">1047</a></p>
<p id="n1048" class="pln"><a href="#n1048">1048</a></p>
<p id="n1049" class="pln"><a href="#n1049">1049</a></p>
<p id="n1050" class="pln"><a href="#n1050">1050</a></p>
<p id="n1051" class="pln"><a href="#n1051">1051</a></p>
<p id="n1052" class="pln"><a href="#n1052">1052</a></p>
<p id="n1053" class="stm mis"><a href="#n1053">1053</a></p>
<p id="n1054" class="pln"><a href="#n1054">1054</a></p>
<p id="n1055" class="pln"><a href="#n1055">1055</a></p>
<p id="n1056" class="stm run hide_run"><a href="#n1056">1056</a></p>
<p id="n1057" class="stm run hide_run"><a href="#n1057">1057</a></p>
<p id="n1058" class="pln"><a href="#n1058">1058</a></p>
<p id="n1059" class="pln"><a href="#n1059">1059</a></p>
<p id="n1060" class="pln"><a href="#n1060">1060</a></p>
<p id="n1061" class="pln"><a href="#n1061">1061</a></p>
<p id="n1062" class="pln"><a href="#n1062">1062</a></p>
<p id="n1063" class="pln"><a href="#n1063">1063</a></p>
<p id="n1064" class="pln"><a href="#n1064">1064</a></p>
<p id="n1065" class="pln"><a href="#n1065">1065</a></p>
<p id="n1066" class="pln"><a href="#n1066">1066</a></p>
<p id="n1067" class="pln"><a href="#n1067">1067</a></p>
<p id="n1068" class="stm mis"><a href="#n1068">1068</a></p>
<p id="n1069" class="stm mis"><a href="#n1069">1069</a></p>
<p id="n1070" class="stm mis"><a href="#n1070">1070</a></p>
<p id="n1071" class="stm mis"><a href="#n1071">1071</a></p>
<p id="n1072" class="stm mis"><a href="#n1072">1072</a></p>
<p id="n1073" class="stm mis"><a href="#n1073">1073</a></p>
<p id="n1074" class="stm mis"><a href="#n1074">1074</a></p>
<p id="n1075" class="pln"><a href="#n1075">1075</a></p>
<p id="n1076" class="stm mis"><a href="#n1076">1076</a></p>
<p id="n1077" class="pln"><a href="#n1077">1077</a></p>
<p id="n1078" class="pln"><a href="#n1078">1078</a></p>
<p id="n1079" class="pln"><a href="#n1079">1079</a></p>
<p id="n1080" class="stm run hide_run"><a href="#n1080">1080</a></p>
<p id="n1081" class="pln"><a href="#n1081">1081</a></p>
<p id="n1082" class="pln"><a href="#n1082">1082</a></p>
<p id="n1083" class="pln"><a href="#n1083">1083</a></p>
<p id="n1084" class="pln"><a href="#n1084">1084</a></p>
<p id="n1085" class="stm run hide_run"><a href="#n1085">1085</a></p>
<p id="n1086" class="stm run hide_run"><a href="#n1086">1086</a></p>
<p id="n1087" class="stm run hide_run"><a href="#n1087">1087</a></p>
<p id="n1088" class="pln"><a href="#n1088">1088</a></p>
<p id="n1089" class="pln"><a href="#n1089">1089</a></p>
<p id="n1090" class="pln"><a href="#n1090">1090</a></p>
<p id="n1091" class="pln"><a href="#n1091">1091</a></p>
<p id="n1092" class="pln"><a href="#n1092">1092</a></p>
<p id="n1093" class="pln"><a href="#n1093">1093</a></p>
<p id="n1094" class="pln"><a href="#n1094">1094</a></p>
<p id="n1095" class="pln"><a href="#n1095">1095</a></p>
<p id="n1096" class="pln"><a href="#n1096">1096</a></p>
<p id="n1097" class="pln"><a href="#n1097">1097</a></p>
<p id="n1098" class="pln"><a href="#n1098">1098</a></p>
<p id="n1099" class="pln"><a href="#n1099">1099</a></p>
<p id="n1100" class="pln"><a href="#n1100">1100</a></p>
<p id="n1101" class="pln"><a href="#n1101">1101</a></p>
<p id="n1102" class="pln"><a href="#n1102">1102</a></p>
<p id="n1103" class="pln"><a href="#n1103">1103</a></p>
<p id="n1104" class="pln"><a href="#n1104">1104</a></p>
<p id="n1105" class="pln"><a href="#n1105">1105</a></p>
<p id="n1106" class="pln"><a href="#n1106">1106</a></p>
<p id="n1107" class="pln"><a href="#n1107">1107</a></p>
<p id="n1108" class="pln"><a href="#n1108">1108</a></p>
<p id="n1109" class="pln"><a href="#n1109">1109</a></p>
<p id="n1110" class="stm mis"><a href="#n1110">1110</a></p>
<p id="n1111" class="stm mis"><a href="#n1111">1111</a></p>
<p id="n1112" class="pln"><a href="#n1112">1112</a></p>
<p id="n1113" class="pln"><a href="#n1113">1113</a></p>
<p id="n1114" class="stm run hide_run"><a href="#n1114">1114</a></p>
<p id="n1115" class="stm run hide_run"><a href="#n1115">1115</a></p>
<p id="n1116" class="stm run hide_run"><a href="#n1116">1116</a></p>
<p id="n1117" class="stm run hide_run"><a href="#n1117">1117</a></p>
<p id="n1118" class="pln"><a href="#n1118">1118</a></p>
<p id="n1119" class="stm run hide_run"><a href="#n1119">1119</a></p>
<p id="n1120" class="stm run hide_run"><a href="#n1120">1120</a></p>
<p id="n1121" class="stm run hide_run"><a href="#n1121">1121</a></p>
<p id="n1122" class="stm run hide_run"><a href="#n1122">1122</a></p>
<p id="n1123" class="stm run hide_run"><a href="#n1123">1123</a></p>
<p id="n1124" class="stm run hide_run"><a href="#n1124">1124</a></p>
<p id="n1125" class="pln"><a href="#n1125">1125</a></p>
<p id="n1126" class="pln"><a href="#n1126">1126</a></p>
<p id="n1127" class="stm run hide_run"><a href="#n1127">1127</a></p>
<p id="n1128" class="pln"><a href="#n1128">1128</a></p>
<p id="n1129" class="stm run hide_run"><a href="#n1129">1129</a></p>
<p id="n1130" class="stm run hide_run"><a href="#n1130">1130</a></p>
<p id="n1131" class="stm run hide_run"><a href="#n1131">1131</a></p>
<p id="n1132" class="pln"><a href="#n1132">1132</a></p>
<p id="n1133" class="stm mis"><a href="#n1133">1133</a></p>
<p id="n1134" class="stm mis"><a href="#n1134">1134</a></p>
<p id="n1135" class="pln"><a href="#n1135">1135</a></p>
<p id="n1136" class="stm mis"><a href="#n1136">1136</a></p>
<p id="n1137" class="pln"><a href="#n1137">1137</a></p>
<p id="n1138" class="pln"><a href="#n1138">1138</a></p>
<p id="n1139" class="pln"><a href="#n1139">1139</a></p>
<p id="n1140" class="pln"><a href="#n1140">1140</a></p>
<p id="n1141" class="pln"><a href="#n1141">1141</a></p>
<p id="n1142" class="stm run hide_run"><a href="#n1142">1142</a></p>
<p id="n1143" class="pln"><a href="#n1143">1143</a></p>
<p id="n1144" class="stm run hide_run"><a href="#n1144">1144</a></p>
<p id="n1145" class="pln"><a href="#n1145">1145</a></p>
<p id="n1146" class="stm run hide_run"><a href="#n1146">1146</a></p>
<p id="n1147" class="pln"><a href="#n1147">1147</a></p>
<p id="n1148" class="pln"><a href="#n1148">1148</a></p>
<p id="n1149" class="stm run hide_run"><a href="#n1149">1149</a></p>
<p id="n1150" class="stm run hide_run"><a href="#n1150">1150</a></p>
<p id="n1151" class="stm run hide_run"><a href="#n1151">1151</a></p>
<p id="n1152" class="stm run hide_run"><a href="#n1152">1152</a></p>
<p id="n1153" class="stm run hide_run"><a href="#n1153">1153</a></p>
<p id="n1154" class="stm run hide_run"><a href="#n1154">1154</a></p>
<p id="n1155" class="stm run hide_run"><a href="#n1155">1155</a></p>
<p id="n1156" class="stm run hide_run"><a href="#n1156">1156</a></p>
<p id="n1157" class="pln"><a href="#n1157">1157</a></p>
<p id="n1158" class="pln"><a href="#n1158">1158</a></p>
<p id="n1159" class="stm run hide_run"><a href="#n1159">1159</a></p>
<p id="n1160" class="stm run hide_run"><a href="#n1160">1160</a></p>
<p id="n1161" class="stm run hide_run"><a href="#n1161">1161</a></p>
<p id="n1162" class="pln"><a href="#n1162">1162</a></p>
<p id="n1163" class="pln"><a href="#n1163">1163</a></p>
<p id="n1164" class="stm mis"><a href="#n1164">1164</a></p>
<p id="n1165" class="pln"><a href="#n1165">1165</a></p>
<p id="n1166" class="pln"><a href="#n1166">1166</a></p>
<p id="n1167" class="pln"><a href="#n1167">1167</a></p>
<p id="n1168" class="pln"><a href="#n1168">1168</a></p>
<p id="n1169" class="pln"><a href="#n1169">1169</a></p>
<p id="n1170" class="stm run hide_run"><a href="#n1170">1170</a></p>
<p id="n1171" class="stm run hide_run"><a href="#n1171">1171</a></p>
<p id="n1172" class="stm run hide_run"><a href="#n1172">1172</a></p>
<p id="n1173" class="pln"><a href="#n1173">1173</a></p>
<p id="n1174" class="stm run hide_run"><a href="#n1174">1174</a></p>
<p id="n1175" class="stm run hide_run"><a href="#n1175">1175</a></p>
<p id="n1176" class="stm run hide_run"><a href="#n1176">1176</a></p>
<p id="n1177" class="stm run hide_run"><a href="#n1177">1177</a></p>
<p id="n1178" class="pln"><a href="#n1178">1178</a></p>
<p id="n1179" class="pln"><a href="#n1179">1179</a></p>
<p id="n1180" class="stm run hide_run"><a href="#n1180">1180</a></p>
<p id="n1181" class="stm run hide_run"><a href="#n1181">1181</a></p>
<p id="n1182" class="pln"><a href="#n1182">1182</a></p>
<p id="n1183" class="pln"><a href="#n1183">1183</a></p>
<p id="n1184" class="pln"><a href="#n1184">1184</a></p>
<p id="n1185" class="pln"><a href="#n1185">1185</a></p>
<p id="n1186" class="pln"><a href="#n1186">1186</a></p>
<p id="n1187" class="pln"><a href="#n1187">1187</a></p>
<p id="n1188" class="pln"><a href="#n1188">1188</a></p>
<p id="n1189" class="pln"><a href="#n1189">1189</a></p>
<p id="n1190" class="pln"><a href="#n1190">1190</a></p>
<p id="n1191" class="pln"><a href="#n1191">1191</a></p>
<p id="n1192" class="pln"><a href="#n1192">1192</a></p>
<p id="n1193" class="pln"><a href="#n1193">1193</a></p>
<p id="n1194" class="pln"><a href="#n1194">1194</a></p>
<p id="n1195" class="pln"><a href="#n1195">1195</a></p>
<p id="n1196" class="pln"><a href="#n1196">1196</a></p>
<p id="n1197" class="pln"><a href="#n1197">1197</a></p>
<p id="n1198" class="pln"><a href="#n1198">1198</a></p>
<p id="n1199" class="pln"><a href="#n1199">1199</a></p>
<p id="n1200" class="pln"><a href="#n1200">1200</a></p>
<p id="n1201" class="pln"><a href="#n1201">1201</a></p>
<p id="n1202" class="pln"><a href="#n1202">1202</a></p>
<p id="n1203" class="pln"><a href="#n1203">1203</a></p>
<p id="n1204" class="pln"><a href="#n1204">1204</a></p>
<p id="n1205" class="pln"><a href="#n1205">1205</a></p>
<p id="n1206" class="pln"><a href="#n1206">1206</a></p>
<p id="n1207" class="pln"><a href="#n1207">1207</a></p>
<p id="n1208" class="pln"><a href="#n1208">1208</a></p>
<p id="n1209" class="pln"><a href="#n1209">1209</a></p>
<p id="n1210" class="pln"><a href="#n1210">1210</a></p>
<p id="n1211" class="pln"><a href="#n1211">1211</a></p>
<p id="n1212" class="pln"><a href="#n1212">1212</a></p>
<p id="n1213" class="pln"><a href="#n1213">1213</a></p>
<p id="n1214" class="pln"><a href="#n1214">1214</a></p>
<p id="n1215" class="pln"><a href="#n1215">1215</a></p>
<p id="n1216" class="pln"><a href="#n1216">1216</a></p>
<p id="n1217" class="pln"><a href="#n1217">1217</a></p>
<p id="n1218" class="pln"><a href="#n1218">1218</a></p>
<p id="n1219" class="pln"><a href="#n1219">1219</a></p>
<p id="n1220" class="pln"><a href="#n1220">1220</a></p>
<p id="n1221" class="pln"><a href="#n1221">1221</a></p>
<p id="n1222" class="pln"><a href="#n1222">1222</a></p>
<p id="n1223" class="pln"><a href="#n1223">1223</a></p>
<p id="n1224" class="pln"><a href="#n1224">1224</a></p>
<p id="n1225" class="pln"><a href="#n1225">1225</a></p>
<p id="n1226" class="pln"><a href="#n1226">1226</a></p>
<p id="n1227" class="pln"><a href="#n1227">1227</a></p>
<p id="n1228" class="pln"><a href="#n1228">1228</a></p>
<p id="n1229" class="stm mis"><a href="#n1229">1229</a></p>
<p id="n1230" class="stm mis"><a href="#n1230">1230</a></p>
<p id="n1231" class="pln"><a href="#n1231">1231</a></p>
<p id="n1232" class="stm mis"><a href="#n1232">1232</a></p>
<p id="n1233" class="stm mis"><a href="#n1233">1233</a></p>
<p id="n1234" class="stm mis"><a href="#n1234">1234</a></p>
<p id="n1235" class="stm mis"><a href="#n1235">1235</a></p>
<p id="n1236" class="pln"><a href="#n1236">1236</a></p>
<p id="n1237" class="pln"><a href="#n1237">1237</a></p>
<p id="n1238" class="stm mis"><a href="#n1238">1238</a></p>
<p id="n1239" class="stm mis"><a href="#n1239">1239</a></p>
<p id="n1240" class="pln"><a href="#n1240">1240</a></p>
<p id="n1241" class="pln"><a href="#n1241">1241</a></p>
<p id="n1242" class="stm mis"><a href="#n1242">1242</a></p>
<p id="n1243" class="stm mis"><a href="#n1243">1243</a></p>
<p id="n1244" class="pln"><a href="#n1244">1244</a></p>
<p id="n1245" class="pln"><a href="#n1245">1245</a></p>
<p id="n1246" class="pln"><a href="#n1246">1246</a></p>
<p id="n1247" class="stm mis"><a href="#n1247">1247</a></p>
<p id="n1248" class="stm mis"><a href="#n1248">1248</a></p>
<p id="n1249" class="stm mis"><a href="#n1249">1249</a></p>
<p id="n1250" class="pln"><a href="#n1250">1250</a></p>
<p id="n1251" class="stm mis"><a href="#n1251">1251</a></p>
<p id="n1252" class="pln"><a href="#n1252">1252</a></p>
<p id="n1253" class="pln"><a href="#n1253">1253</a></p>
<p id="n1254" class="pln"><a href="#n1254">1254</a></p>
<p id="n1255" class="stm run hide_run"><a href="#n1255">1255</a></p>
<p id="n1256" class="pln"><a href="#n1256">1256</a></p>
<p id="n1257" class="pln"><a href="#n1257">1257</a></p>
<p id="n1258" class="stm mis"><a href="#n1258">1258</a></p>
<p id="n1259" class="stm mis"><a href="#n1259">1259</a></p>
<p id="n1260" class="stm mis"><a href="#n1260">1260</a></p>
<p id="n1261" class="stm mis"><a href="#n1261">1261</a></p>
<p id="n1262" class="stm mis"><a href="#n1262">1262</a></p>
<p id="n1263" class="stm mis"><a href="#n1263">1263</a></p>
<p id="n1264" class="pln"><a href="#n1264">1264</a></p>
<p id="n1265" class="pln"><a href="#n1265">1265</a></p>
<p id="n1266" class="stm mis"><a href="#n1266">1266</a></p>
<p id="n1267" class="stm mis"><a href="#n1267">1267</a></p>
<p id="n1268" class="stm mis"><a href="#n1268">1268</a></p>
<p id="n1269" class="stm mis"><a href="#n1269">1269</a></p>
<p id="n1270" class="pln"><a href="#n1270">1270</a></p>
<p id="n1271" class="stm mis"><a href="#n1271">1271</a></p>
<p id="n1272" class="stm mis"><a href="#n1272">1272</a></p>
<p id="n1273" class="pln"><a href="#n1273">1273</a></p>
<p id="n1274" class="pln"><a href="#n1274">1274</a></p>
<p id="n1275" class="stm mis"><a href="#n1275">1275</a></p>
<p id="n1276" class="pln"><a href="#n1276">1276</a></p>
<p id="n1277" class="pln"><a href="#n1277">1277</a></p>
<p id="n1278" class="stm run hide_run"><a href="#n1278">1278</a></p>
<p id="n1279" class="pln"><a href="#n1279">1279</a></p>
<p id="n1280" class="stm mis"><a href="#n1280">1280</a></p>
<p id="n1281" class="pln"><a href="#n1281">1281</a></p>
<p id="n1282" class="stm mis"><a href="#n1282">1282</a></p>
<p id="n1283" class="stm mis"><a href="#n1283">1283</a></p>
<p id="n1284" class="pln"><a href="#n1284">1284</a></p>
<p id="n1285" class="pln"><a href="#n1285">1285</a></p>
<p id="n1286" class="stm run hide_run"><a href="#n1286">1286</a></p>
<p id="n1287" class="stm run hide_run"><a href="#n1287">1287</a></p>
<p id="n1288" class="pln"><a href="#n1288">1288</a></p>
<p id="n1289" class="stm run hide_run"><a href="#n1289">1289</a></p>
<p id="n1290" class="pln"><a href="#n1290">1290</a></p>
<p id="n1291" class="pln"><a href="#n1291">1291</a></p>
<p id="n1292" class="pln"><a href="#n1292">1292</a></p>
<p id="n1293" class="pln"><a href="#n1293">1293</a></p>
<p id="n1294" class="pln"><a href="#n1294">1294</a></p>
<p id="n1295" class="pln"><a href="#n1295">1295</a></p>
<p id="n1296" class="pln"><a href="#n1296">1296</a></p>
<p id="n1297" class="pln"><a href="#n1297">1297</a></p>
<p id="n1298" class="pln"><a href="#n1298">1298</a></p>
<p id="n1299" class="pln"><a href="#n1299">1299</a></p>
<p id="n1300" class="pln"><a href="#n1300">1300</a></p>
<p id="n1301" class="pln"><a href="#n1301">1301</a></p>
<p id="n1302" class="pln"><a href="#n1302">1302</a></p>
<p id="n1303" class="pln"><a href="#n1303">1303</a></p>
<p id="n1304" class="pln"><a href="#n1304">1304</a></p>
<p id="n1305" class="pln"><a href="#n1305">1305</a></p>
<p id="n1306" class="pln"><a href="#n1306">1306</a></p>
<p id="n1307" class="pln"><a href="#n1307">1307</a></p>
<p id="n1308" class="pln"><a href="#n1308">1308</a></p>
<p id="n1309" class="pln"><a href="#n1309">1309</a></p>
<p id="n1310" class="pln"><a href="#n1310">1310</a></p>
<p id="n1311" class="pln"><a href="#n1311">1311</a></p>
<p id="n1312" class="pln"><a href="#n1312">1312</a></p>
<p id="n1313" class="pln"><a href="#n1313">1313</a></p>
<p id="n1314" class="pln"><a href="#n1314">1314</a></p>
<p id="n1315" class="pln"><a href="#n1315">1315</a></p>
<p id="n1316" class="pln"><a href="#n1316">1316</a></p>
<p id="n1317" class="pln"><a href="#n1317">1317</a></p>
<p id="n1318" class="pln"><a href="#n1318">1318</a></p>
<p id="n1319" class="pln"><a href="#n1319">1319</a></p>
<p id="n1320" class="pln"><a href="#n1320">1320</a></p>
<p id="n1321" class="pln"><a href="#n1321">1321</a></p>
<p id="n1322" class="pln"><a href="#n1322">1322</a></p>
<p id="n1323" class="pln"><a href="#n1323">1323</a></p>
<p id="n1324" class="pln"><a href="#n1324">1324</a></p>
<p id="n1325" class="pln"><a href="#n1325">1325</a></p>
<p id="n1326" class="pln"><a href="#n1326">1326</a></p>
<p id="n1327" class="pln"><a href="#n1327">1327</a></p>
<p id="n1328" class="pln"><a href="#n1328">1328</a></p>
<p id="n1329" class="pln"><a href="#n1329">1329</a></p>
<p id="n1330" class="pln"><a href="#n1330">1330</a></p>
<p id="n1331" class="pln"><a href="#n1331">1331</a></p>
<p id="n1332" class="pln"><a href="#n1332">1332</a></p>
<p id="n1333" class="pln"><a href="#n1333">1333</a></p>
<p id="n1334" class="stm mis"><a href="#n1334">1334</a></p>
<p id="n1335" class="pln"><a href="#n1335">1335</a></p>
<p id="n1336" class="stm mis"><a href="#n1336">1336</a></p>
<p id="n1337" class="stm mis"><a href="#n1337">1337</a></p>
<p id="n1338" class="pln"><a href="#n1338">1338</a></p>
<p id="n1339" class="stm mis"><a href="#n1339">1339</a></p>
<p id="n1340" class="pln"><a href="#n1340">1340</a></p>
<p id="n1341" class="pln"><a href="#n1341">1341</a></p>
<p id="n1342" class="pln"><a href="#n1342">1342</a></p>
<p id="n1343" class="pln"><a href="#n1343">1343</a></p>
<p id="n1344" class="pln"><a href="#n1344">1344</a></p>
<p id="n1345" class="pln"><a href="#n1345">1345</a></p>
<p id="n1346" class="pln"><a href="#n1346">1346</a></p>
<p id="n1347" class="pln"><a href="#n1347">1347</a></p>
<p id="n1348" class="stm run hide_run"><a href="#n1348">1348</a></p>
<p id="n1349" class="stm run hide_run"><a href="#n1349">1349</a></p>
<p id="n1350" class="pln"><a href="#n1350">1350</a></p>
<p id="n1351" class="stm run hide_run"><a href="#n1351">1351</a></p>
<p id="n1352" class="pln"><a href="#n1352">1352</a></p>
<p id="n1353" class="pln"><a href="#n1353">1353</a></p>
<p id="n1354" class="pln"><a href="#n1354">1354</a></p>
<p id="n1355" class="pln"><a href="#n1355">1355</a></p>
<p id="n1356" class="pln"><a href="#n1356">1356</a></p>
<p id="n1357" class="pln"><a href="#n1357">1357</a></p>
<p id="n1358" class="pln"><a href="#n1358">1358</a></p>
<p id="n1359" class="pln"><a href="#n1359">1359</a></p>
<p id="n1360" class="pln"><a href="#n1360">1360</a></p>
<p id="n1361" class="pln"><a href="#n1361">1361</a></p>
<p id="n1362" class="pln"><a href="#n1362">1362</a></p>
<p id="n1363" class="pln"><a href="#n1363">1363</a></p>
<p id="n1364" class="pln"><a href="#n1364">1364</a></p>
<p id="n1365" class="pln"><a href="#n1365">1365</a></p>
<p id="n1366" class="pln"><a href="#n1366">1366</a></p>
<p id="n1367" class="pln"><a href="#n1367">1367</a></p>
<p id="n1368" class="pln"><a href="#n1368">1368</a></p>
<p id="n1369" class="pln"><a href="#n1369">1369</a></p>
<p id="n1370" class="pln"><a href="#n1370">1370</a></p>
<p id="n1371" class="pln"><a href="#n1371">1371</a></p>
<p id="n1372" class="pln"><a href="#n1372">1372</a></p>
<p id="n1373" class="pln"><a href="#n1373">1373</a></p>
<p id="n1374" class="pln"><a href="#n1374">1374</a></p>
<p id="n1375" class="pln"><a href="#n1375">1375</a></p>
<p id="n1376" class="pln"><a href="#n1376">1376</a></p>
<p id="n1377" class="pln"><a href="#n1377">1377</a></p>
<p id="n1378" class="pln"><a href="#n1378">1378</a></p>
<p id="n1379" class="pln"><a href="#n1379">1379</a></p>
<p id="n1380" class="pln"><a href="#n1380">1380</a></p>
<p id="n1381" class="pln"><a href="#n1381">1381</a></p>
<p id="n1382" class="pln"><a href="#n1382">1382</a></p>
<p id="n1383" class="pln"><a href="#n1383">1383</a></p>
<p id="n1384" class="pln"><a href="#n1384">1384</a></p>
<p id="n1385" class="pln"><a href="#n1385">1385</a></p>
<p id="n1386" class="pln"><a href="#n1386">1386</a></p>
<p id="n1387" class="pln"><a href="#n1387">1387</a></p>
<p id="n1388" class="pln"><a href="#n1388">1388</a></p>
<p id="n1389" class="pln"><a href="#n1389">1389</a></p>
<p id="n1390" class="pln"><a href="#n1390">1390</a></p>
<p id="n1391" class="pln"><a href="#n1391">1391</a></p>
<p id="n1392" class="pln"><a href="#n1392">1392</a></p>
<p id="n1393" class="pln"><a href="#n1393">1393</a></p>
<p id="n1394" class="pln"><a href="#n1394">1394</a></p>
<p id="n1395" class="pln"><a href="#n1395">1395</a></p>
<p id="n1396" class="pln"><a href="#n1396">1396</a></p>
<p id="n1397" class="pln"><a href="#n1397">1397</a></p>
<p id="n1398" class="pln"><a href="#n1398">1398</a></p>
<p id="n1399" class="pln"><a href="#n1399">1399</a></p>
<p id="n1400" class="pln"><a href="#n1400">1400</a></p>
<p id="n1401" class="pln"><a href="#n1401">1401</a></p>
<p id="n1402" class="pln"><a href="#n1402">1402</a></p>
<p id="n1403" class="pln"><a href="#n1403">1403</a></p>
<p id="n1404" class="pln"><a href="#n1404">1404</a></p>
<p id="n1405" class="pln"><a href="#n1405">1405</a></p>
<p id="n1406" class="pln"><a href="#n1406">1406</a></p>
<p id="n1407" class="stm mis"><a href="#n1407">1407</a></p>
<p id="n1408" class="pln"><a href="#n1408">1408</a></p>
<p id="n1409" class="stm mis"><a href="#n1409">1409</a></p>
<p id="n1410" class="stm mis"><a href="#n1410">1410</a></p>
<p id="n1411" class="pln"><a href="#n1411">1411</a></p>
<p id="n1412" class="stm mis"><a href="#n1412">1412</a></p>
<p id="n1413" class="stm mis"><a href="#n1413">1413</a></p>
<p id="n1414" class="pln"><a href="#n1414">1414</a></p>
<p id="n1415" class="stm mis"><a href="#n1415">1415</a></p>
<p id="n1416" class="stm mis"><a href="#n1416">1416</a></p>
<p id="n1417" class="pln"><a href="#n1417">1417</a></p>
<p id="n1418" class="pln"><a href="#n1418">1418</a></p>
<p id="n1419" class="pln"><a href="#n1419">1419</a></p>
<p id="n1420" class="pln"><a href="#n1420">1420</a></p>
<p id="n1421" class="pln"><a href="#n1421">1421</a></p>
<p id="n1422" class="pln"><a href="#n1422">1422</a></p>
<p id="n1423" class="pln"><a href="#n1423">1423</a></p>
<p id="n1424" class="pln"><a href="#n1424">1424</a></p>
<p id="n1425" class="pln"><a href="#n1425">1425</a></p>
<p id="n1426" class="stm run hide_run"><a href="#n1426">1426</a></p>
<p id="n1427" class="stm run hide_run"><a href="#n1427">1427</a></p>
<p id="n1428" class="pln"><a href="#n1428">1428</a></p>
<p id="n1429" class="stm run hide_run"><a href="#n1429">1429</a></p>
<p id="n1430" class="pln"><a href="#n1430">1430</a></p>
<p id="n1431" class="pln"><a href="#n1431">1431</a></p>
<p id="n1432" class="pln"><a href="#n1432">1432</a></p>
<p id="n1433" class="pln"><a href="#n1433">1433</a></p>
<p id="n1434" class="pln"><a href="#n1434">1434</a></p>
<p id="n1435" class="pln"><a href="#n1435">1435</a></p>
<p id="n1436" class="pln"><a href="#n1436">1436</a></p>
<p id="n1437" class="pln"><a href="#n1437">1437</a></p>
<p id="n1438" class="pln"><a href="#n1438">1438</a></p>
<p id="n1439" class="pln"><a href="#n1439">1439</a></p>
<p id="n1440" class="pln"><a href="#n1440">1440</a></p>
<p id="n1441" class="pln"><a href="#n1441">1441</a></p>
<p id="n1442" class="pln"><a href="#n1442">1442</a></p>
<p id="n1443" class="pln"><a href="#n1443">1443</a></p>
<p id="n1444" class="pln"><a href="#n1444">1444</a></p>
<p id="n1445" class="pln"><a href="#n1445">1445</a></p>
<p id="n1446" class="pln"><a href="#n1446">1446</a></p>
<p id="n1447" class="pln"><a href="#n1447">1447</a></p>
<p id="n1448" class="pln"><a href="#n1448">1448</a></p>
<p id="n1449" class="pln"><a href="#n1449">1449</a></p>
<p id="n1450" class="pln"><a href="#n1450">1450</a></p>
<p id="n1451" class="pln"><a href="#n1451">1451</a></p>
<p id="n1452" class="pln"><a href="#n1452">1452</a></p>
<p id="n1453" class="pln"><a href="#n1453">1453</a></p>
<p id="n1454" class="pln"><a href="#n1454">1454</a></p>
<p id="n1455" class="pln"><a href="#n1455">1455</a></p>
<p id="n1456" class="pln"><a href="#n1456">1456</a></p>
<p id="n1457" class="pln"><a href="#n1457">1457</a></p>
<p id="n1458" class="pln"><a href="#n1458">1458</a></p>
<p id="n1459" class="pln"><a href="#n1459">1459</a></p>
<p id="n1460" class="pln"><a href="#n1460">1460</a></p>
<p id="n1461" class="pln"><a href="#n1461">1461</a></p>
<p id="n1462" class="pln"><a href="#n1462">1462</a></p>
<p id="n1463" class="pln"><a href="#n1463">1463</a></p>
<p id="n1464" class="pln"><a href="#n1464">1464</a></p>
<p id="n1465" class="pln"><a href="#n1465">1465</a></p>
<p id="n1466" class="pln"><a href="#n1466">1466</a></p>
<p id="n1467" class="pln"><a href="#n1467">1467</a></p>
<p id="n1468" class="pln"><a href="#n1468">1468</a></p>
<p id="n1469" class="pln"><a href="#n1469">1469</a></p>
<p id="n1470" class="pln"><a href="#n1470">1470</a></p>
<p id="n1471" class="pln"><a href="#n1471">1471</a></p>
<p id="n1472" class="pln"><a href="#n1472">1472</a></p>
<p id="n1473" class="pln"><a href="#n1473">1473</a></p>
<p id="n1474" class="pln"><a href="#n1474">1474</a></p>
<p id="n1475" class="pln"><a href="#n1475">1475</a></p>
<p id="n1476" class="pln"><a href="#n1476">1476</a></p>
<p id="n1477" class="pln"><a href="#n1477">1477</a></p>
<p id="n1478" class="pln"><a href="#n1478">1478</a></p>
<p id="n1479" class="pln"><a href="#n1479">1479</a></p>
<p id="n1480" class="pln"><a href="#n1480">1480</a></p>
<p id="n1481" class="pln"><a href="#n1481">1481</a></p>
<p id="n1482" class="pln"><a href="#n1482">1482</a></p>
<p id="n1483" class="pln"><a href="#n1483">1483</a></p>
<p id="n1484" class="stm mis"><a href="#n1484">1484</a></p>
<p id="n1485" class="pln"><a href="#n1485">1485</a></p>
<p id="n1486" class="pln"><a href="#n1486">1486</a></p>
<p id="n1487" class="stm mis"><a href="#n1487">1487</a></p>
<p id="n1488" class="stm mis"><a href="#n1488">1488</a></p>
<p id="n1489" class="stm mis"><a href="#n1489">1489</a></p>
<p id="n1490" class="pln"><a href="#n1490">1490</a></p>
<p id="n1491" class="pln"><a href="#n1491">1491</a></p>
<p id="n1492" class="pln"><a href="#n1492">1492</a></p>
<p id="n1493" class="pln"><a href="#n1493">1493</a></p>
<p id="n1494" class="pln"><a href="#n1494">1494</a></p>
<p id="n1495" class="pln"><a href="#n1495">1495</a></p>
<p id="n1496" class="pln"><a href="#n1496">1496</a></p>
<p id="n1497" class="pln"><a href="#n1497">1497</a></p>
<p id="n1498" class="stm run hide_run"><a href="#n1498">1498</a></p>
<p id="n1499" class="stm run hide_run"><a href="#n1499">1499</a></p>
<p id="n1500" class="pln"><a href="#n1500">1500</a></p>
<p id="n1501" class="stm run hide_run"><a href="#n1501">1501</a></p>
<p id="n1502" class="pln"><a href="#n1502">1502</a></p>
<p id="n1503" class="pln"><a href="#n1503">1503</a></p>
<p id="n1504" class="pln"><a href="#n1504">1504</a></p>
<p id="n1505" class="pln"><a href="#n1505">1505</a></p>
<p id="n1506" class="pln"><a href="#n1506">1506</a></p>
<p id="n1507" class="pln"><a href="#n1507">1507</a></p>
<p id="n1508" class="pln"><a href="#n1508">1508</a></p>
<p id="n1509" class="pln"><a href="#n1509">1509</a></p>
<p id="n1510" class="pln"><a href="#n1510">1510</a></p>
<p id="n1511" class="pln"><a href="#n1511">1511</a></p>
<p id="n1512" class="pln"><a href="#n1512">1512</a></p>
<p id="n1513" class="pln"><a href="#n1513">1513</a></p>
<p id="n1514" class="pln"><a href="#n1514">1514</a></p>
<p id="n1515" class="pln"><a href="#n1515">1515</a></p>
<p id="n1516" class="pln"><a href="#n1516">1516</a></p>
<p id="n1517" class="pln"><a href="#n1517">1517</a></p>
<p id="n1518" class="pln"><a href="#n1518">1518</a></p>
<p id="n1519" class="pln"><a href="#n1519">1519</a></p>
<p id="n1520" class="pln"><a href="#n1520">1520</a></p>
<p id="n1521" class="pln"><a href="#n1521">1521</a></p>
<p id="n1522" class="pln"><a href="#n1522">1522</a></p>
<p id="n1523" class="pln"><a href="#n1523">1523</a></p>
<p id="n1524" class="pln"><a href="#n1524">1524</a></p>
<p id="n1525" class="pln"><a href="#n1525">1525</a></p>
<p id="n1526" class="pln"><a href="#n1526">1526</a></p>
<p id="n1527" class="pln"><a href="#n1527">1527</a></p>
<p id="n1528" class="pln"><a href="#n1528">1528</a></p>
<p id="n1529" class="pln"><a href="#n1529">1529</a></p>
<p id="n1530" class="pln"><a href="#n1530">1530</a></p>
<p id="n1531" class="pln"><a href="#n1531">1531</a></p>
<p id="n1532" class="pln"><a href="#n1532">1532</a></p>
<p id="n1533" class="pln"><a href="#n1533">1533</a></p>
<p id="n1534" class="stm mis"><a href="#n1534">1534</a></p>
<p id="n1535" class="pln"><a href="#n1535">1535</a></p>
<p id="n1536" class="pln"><a href="#n1536">1536</a></p>
<p id="n1537" class="stm mis"><a href="#n1537">1537</a></p>
<p id="n1538" class="stm mis"><a href="#n1538">1538</a></p>
<p id="n1539" class="stm mis"><a href="#n1539">1539</a></p>
<p id="n1540" class="pln"><a href="#n1540">1540</a></p>
<p id="n1541" class="pln"><a href="#n1541">1541</a></p>
<p id="n1542" class="pln"><a href="#n1542">1542</a></p>
<p id="n1543" class="pln"><a href="#n1543">1543</a></p>
<p id="n1544" class="pln"><a href="#n1544">1544</a></p>
<p id="n1545" class="pln"><a href="#n1545">1545</a></p>
<p id="n1546" class="pln"><a href="#n1546">1546</a></p>
<p id="n1547" class="pln"><a href="#n1547">1547</a></p>
<p id="n1548" class="stm run hide_run"><a href="#n1548">1548</a></p>
<p id="n1549" class="stm run hide_run"><a href="#n1549">1549</a></p>
<p id="n1550" class="pln"><a href="#n1550">1550</a></p>
<p id="n1551" class="stm run hide_run"><a href="#n1551">1551</a></p>
<p id="n1552" class="pln"><a href="#n1552">1552</a></p>
<p id="n1553" class="pln"><a href="#n1553">1553</a></p>
<p id="n1554" class="pln"><a href="#n1554">1554</a></p>
<p id="n1555" class="pln"><a href="#n1555">1555</a></p>
<p id="n1556" class="pln"><a href="#n1556">1556</a></p>
<p id="n1557" class="pln"><a href="#n1557">1557</a></p>
<p id="n1558" class="pln"><a href="#n1558">1558</a></p>
<p id="n1559" class="pln"><a href="#n1559">1559</a></p>
<p id="n1560" class="pln"><a href="#n1560">1560</a></p>
<p id="n1561" class="pln"><a href="#n1561">1561</a></p>
<p id="n1562" class="pln"><a href="#n1562">1562</a></p>
<p id="n1563" class="pln"><a href="#n1563">1563</a></p>
<p id="n1564" class="pln"><a href="#n1564">1564</a></p>
<p id="n1565" class="pln"><a href="#n1565">1565</a></p>
<p id="n1566" class="pln"><a href="#n1566">1566</a></p>
<p id="n1567" class="pln"><a href="#n1567">1567</a></p>
<p id="n1568" class="pln"><a href="#n1568">1568</a></p>
<p id="n1569" class="pln"><a href="#n1569">1569</a></p>
<p id="n1570" class="pln"><a href="#n1570">1570</a></p>
<p id="n1571" class="pln"><a href="#n1571">1571</a></p>
<p id="n1572" class="pln"><a href="#n1572">1572</a></p>
<p id="n1573" class="pln"><a href="#n1573">1573</a></p>
<p id="n1574" class="pln"><a href="#n1574">1574</a></p>
<p id="n1575" class="pln"><a href="#n1575">1575</a></p>
<p id="n1576" class="pln"><a href="#n1576">1576</a></p>
<p id="n1577" class="pln"><a href="#n1577">1577</a></p>
<p id="n1578" class="pln"><a href="#n1578">1578</a></p>
<p id="n1579" class="pln"><a href="#n1579">1579</a></p>
<p id="n1580" class="pln"><a href="#n1580">1580</a></p>
<p id="n1581" class="pln"><a href="#n1581">1581</a></p>
<p id="n1582" class="pln"><a href="#n1582">1582</a></p>
<p id="n1583" class="pln"><a href="#n1583">1583</a></p>
<p id="n1584" class="stm mis"><a href="#n1584">1584</a></p>
<p id="n1585" class="pln"><a href="#n1585">1585</a></p>
<p id="n1586" class="stm mis"><a href="#n1586">1586</a></p>
<p id="n1587" class="stm mis"><a href="#n1587">1587</a></p>
<p id="n1588" class="stm mis"><a href="#n1588">1588</a></p>
<p id="n1589" class="pln"><a href="#n1589">1589</a></p>
<p id="n1590" class="pln"><a href="#n1590">1590</a></p>
<p id="n1591" class="pln"><a href="#n1591">1591</a></p>
<p id="n1592" class="pln"><a href="#n1592">1592</a></p>
<p id="n1593" class="pln"><a href="#n1593">1593</a></p>
<p id="n1594" class="pln"><a href="#n1594">1594</a></p>
<p id="n1595" class="pln"><a href="#n1595">1595</a></p>
<p id="n1596" class="pln"><a href="#n1596">1596</a></p>
<p id="n1597" class="stm run hide_run"><a href="#n1597">1597</a></p>
<p id="n1598" class="stm run hide_run"><a href="#n1598">1598</a></p>
<p id="n1599" class="pln"><a href="#n1599">1599</a></p>
<p id="n1600" class="stm run hide_run"><a href="#n1600">1600</a></p>
<p id="n1601" class="pln"><a href="#n1601">1601</a></p>
<p id="n1602" class="pln"><a href="#n1602">1602</a></p>
<p id="n1603" class="pln"><a href="#n1603">1603</a></p>
<p id="n1604" class="pln"><a href="#n1604">1604</a></p>
<p id="n1605" class="pln"><a href="#n1605">1605</a></p>
<p id="n1606" class="pln"><a href="#n1606">1606</a></p>
<p id="n1607" class="pln"><a href="#n1607">1607</a></p>
<p id="n1608" class="pln"><a href="#n1608">1608</a></p>
<p id="n1609" class="pln"><a href="#n1609">1609</a></p>
<p id="n1610" class="pln"><a href="#n1610">1610</a></p>
<p id="n1611" class="pln"><a href="#n1611">1611</a></p>
<p id="n1612" class="pln"><a href="#n1612">1612</a></p>
<p id="n1613" class="pln"><a href="#n1613">1613</a></p>
<p id="n1614" class="pln"><a href="#n1614">1614</a></p>
<p id="n1615" class="pln"><a href="#n1615">1615</a></p>
<p id="n1616" class="pln"><a href="#n1616">1616</a></p>
<p id="n1617" class="pln"><a href="#n1617">1617</a></p>
<p id="n1618" class="pln"><a href="#n1618">1618</a></p>
<p id="n1619" class="pln"><a href="#n1619">1619</a></p>
<p id="n1620" class="pln"><a href="#n1620">1620</a></p>
<p id="n1621" class="pln"><a href="#n1621">1621</a></p>
<p id="n1622" class="pln"><a href="#n1622">1622</a></p>
<p id="n1623" class="pln"><a href="#n1623">1623</a></p>
<p id="n1624" class="pln"><a href="#n1624">1624</a></p>
<p id="n1625" class="pln"><a href="#n1625">1625</a></p>
<p id="n1626" class="pln"><a href="#n1626">1626</a></p>
<p id="n1627" class="pln"><a href="#n1627">1627</a></p>
<p id="n1628" class="pln"><a href="#n1628">1628</a></p>
<p id="n1629" class="pln"><a href="#n1629">1629</a></p>
<p id="n1630" class="pln"><a href="#n1630">1630</a></p>
<p id="n1631" class="pln"><a href="#n1631">1631</a></p>
<p id="n1632" class="pln"><a href="#n1632">1632</a></p>
<p id="n1633" class="stm mis"><a href="#n1633">1633</a></p>
<p id="n1634" class="pln"><a href="#n1634">1634</a></p>
<p id="n1635" class="stm mis"><a href="#n1635">1635</a></p>
<p id="n1636" class="stm mis"><a href="#n1636">1636</a></p>
<p id="n1637" class="stm mis"><a href="#n1637">1637</a></p>
<p id="n1638" class="pln"><a href="#n1638">1638</a></p>
<p id="n1639" class="pln"><a href="#n1639">1639</a></p>
<p id="n1640" class="pln"><a href="#n1640">1640</a></p>
<p id="n1641" class="pln"><a href="#n1641">1641</a></p>
<p id="n1642" class="pln"><a href="#n1642">1642</a></p>
<p id="n1643" class="pln"><a href="#n1643">1643</a></p>
<p id="n1644" class="pln"><a href="#n1644">1644</a></p>
<p id="n1645" class="pln"><a href="#n1645">1645</a></p>
<p id="n1646" class="stm run hide_run"><a href="#n1646">1646</a></p>
<p id="n1647" class="stm run hide_run"><a href="#n1647">1647</a></p>
<p id="n1648" class="pln"><a href="#n1648">1648</a></p>
<p id="n1649" class="stm run hide_run"><a href="#n1649">1649</a></p>
<p id="n1650" class="pln"><a href="#n1650">1650</a></p>
<p id="n1651" class="pln"><a href="#n1651">1651</a></p>
<p id="n1652" class="pln"><a href="#n1652">1652</a></p>
<p id="n1653" class="pln"><a href="#n1653">1653</a></p>
<p id="n1654" class="pln"><a href="#n1654">1654</a></p>
<p id="n1655" class="pln"><a href="#n1655">1655</a></p>
<p id="n1656" class="pln"><a href="#n1656">1656</a></p>
<p id="n1657" class="pln"><a href="#n1657">1657</a></p>
<p id="n1658" class="pln"><a href="#n1658">1658</a></p>
<p id="n1659" class="pln"><a href="#n1659">1659</a></p>
<p id="n1660" class="pln"><a href="#n1660">1660</a></p>
<p id="n1661" class="pln"><a href="#n1661">1661</a></p>
<p id="n1662" class="pln"><a href="#n1662">1662</a></p>
<p id="n1663" class="pln"><a href="#n1663">1663</a></p>
<p id="n1664" class="pln"><a href="#n1664">1664</a></p>
<p id="n1665" class="pln"><a href="#n1665">1665</a></p>
<p id="n1666" class="pln"><a href="#n1666">1666</a></p>
<p id="n1667" class="pln"><a href="#n1667">1667</a></p>
<p id="n1668" class="pln"><a href="#n1668">1668</a></p>
<p id="n1669" class="pln"><a href="#n1669">1669</a></p>
<p id="n1670" class="pln"><a href="#n1670">1670</a></p>
<p id="n1671" class="pln"><a href="#n1671">1671</a></p>
<p id="n1672" class="pln"><a href="#n1672">1672</a></p>
<p id="n1673" class="pln"><a href="#n1673">1673</a></p>
<p id="n1674" class="pln"><a href="#n1674">1674</a></p>
<p id="n1675" class="pln"><a href="#n1675">1675</a></p>
<p id="n1676" class="pln"><a href="#n1676">1676</a></p>
<p id="n1677" class="pln"><a href="#n1677">1677</a></p>
<p id="n1678" class="pln"><a href="#n1678">1678</a></p>
<p id="n1679" class="pln"><a href="#n1679">1679</a></p>
<p id="n1680" class="pln"><a href="#n1680">1680</a></p>
<p id="n1681" class="pln"><a href="#n1681">1681</a></p>
<p id="n1682" class="pln"><a href="#n1682">1682</a></p>
<p id="n1683" class="pln"><a href="#n1683">1683</a></p>
<p id="n1684" class="pln"><a href="#n1684">1684</a></p>
<p id="n1685" class="pln"><a href="#n1685">1685</a></p>
<p id="n1686" class="pln"><a href="#n1686">1686</a></p>
<p id="n1687" class="pln"><a href="#n1687">1687</a></p>
<p id="n1688" class="pln"><a href="#n1688">1688</a></p>
<p id="n1689" class="pln"><a href="#n1689">1689</a></p>
<p id="n1690" class="pln"><a href="#n1690">1690</a></p>
<p id="n1691" class="stm mis"><a href="#n1691">1691</a></p>
<p id="n1692" class="pln"><a href="#n1692">1692</a></p>
<p id="n1693" class="stm mis"><a href="#n1693">1693</a></p>
<p id="n1694" class="stm mis"><a href="#n1694">1694</a></p>
<p id="n1695" class="stm mis"><a href="#n1695">1695</a></p>
<p id="n1696" class="pln"><a href="#n1696">1696</a></p>
<p id="n1697" class="pln"><a href="#n1697">1697</a></p>
<p id="n1698" class="pln"><a href="#n1698">1698</a></p>
<p id="n1699" class="pln"><a href="#n1699">1699</a></p>
<p id="n1700" class="pln"><a href="#n1700">1700</a></p>
<p id="n1701" class="pln"><a href="#n1701">1701</a></p>
<p id="n1702" class="pln"><a href="#n1702">1702</a></p>
<p id="n1703" class="pln"><a href="#n1703">1703</a></p>
<p id="n1704" class="stm run hide_run"><a href="#n1704">1704</a></p>
<p id="n1705" class="stm run hide_run"><a href="#n1705">1705</a></p>
<p id="n1706" class="pln"><a href="#n1706">1706</a></p>
<p id="n1707" class="stm run hide_run"><a href="#n1707">1707</a></p>
<p id="n1708" class="pln"><a href="#n1708">1708</a></p>
<p id="n1709" class="pln"><a href="#n1709">1709</a></p>
<p id="n1710" class="pln"><a href="#n1710">1710</a></p>
<p id="n1711" class="pln"><a href="#n1711">1711</a></p>
<p id="n1712" class="pln"><a href="#n1712">1712</a></p>
<p id="n1713" class="pln"><a href="#n1713">1713</a></p>
<p id="n1714" class="pln"><a href="#n1714">1714</a></p>
<p id="n1715" class="pln"><a href="#n1715">1715</a></p>
<p id="n1716" class="pln"><a href="#n1716">1716</a></p>
<p id="n1717" class="pln"><a href="#n1717">1717</a></p>
<p id="n1718" class="pln"><a href="#n1718">1718</a></p>
<p id="n1719" class="pln"><a href="#n1719">1719</a></p>
<p id="n1720" class="pln"><a href="#n1720">1720</a></p>
<p id="n1721" class="pln"><a href="#n1721">1721</a></p>
<p id="n1722" class="pln"><a href="#n1722">1722</a></p>
<p id="n1723" class="pln"><a href="#n1723">1723</a></p>
<p id="n1724" class="pln"><a href="#n1724">1724</a></p>
<p id="n1725" class="pln"><a href="#n1725">1725</a></p>
<p id="n1726" class="pln"><a href="#n1726">1726</a></p>
<p id="n1727" class="pln"><a href="#n1727">1727</a></p>
<p id="n1728" class="pln"><a href="#n1728">1728</a></p>
<p id="n1729" class="pln"><a href="#n1729">1729</a></p>
<p id="n1730" class="pln"><a href="#n1730">1730</a></p>
<p id="n1731" class="pln"><a href="#n1731">1731</a></p>
<p id="n1732" class="pln"><a href="#n1732">1732</a></p>
<p id="n1733" class="pln"><a href="#n1733">1733</a></p>
<p id="n1734" class="pln"><a href="#n1734">1734</a></p>
<p id="n1735" class="pln"><a href="#n1735">1735</a></p>
<p id="n1736" class="pln"><a href="#n1736">1736</a></p>
<p id="n1737" class="pln"><a href="#n1737">1737</a></p>
<p id="n1738" class="pln"><a href="#n1738">1738</a></p>
<p id="n1739" class="pln"><a href="#n1739">1739</a></p>
<p id="n1740" class="pln"><a href="#n1740">1740</a></p>
<p id="n1741" class="pln"><a href="#n1741">1741</a></p>
<p id="n1742" class="pln"><a href="#n1742">1742</a></p>
<p id="n1743" class="pln"><a href="#n1743">1743</a></p>
<p id="n1744" class="pln"><a href="#n1744">1744</a></p>
<p id="n1745" class="pln"><a href="#n1745">1745</a></p>
<p id="n1746" class="pln"><a href="#n1746">1746</a></p>
<p id="n1747" class="pln"><a href="#n1747">1747</a></p>
<p id="n1748" class="pln"><a href="#n1748">1748</a></p>
<p id="n1749" class="stm mis"><a href="#n1749">1749</a></p>
<p id="n1750" class="pln"><a href="#n1750">1750</a></p>
<p id="n1751" class="stm mis"><a href="#n1751">1751</a></p>
<p id="n1752" class="stm mis"><a href="#n1752">1752</a></p>
<p id="n1753" class="stm mis"><a href="#n1753">1753</a></p>
<p id="n1754" class="pln"><a href="#n1754">1754</a></p>
<p id="n1755" class="pln"><a href="#n1755">1755</a></p>
<p id="n1756" class="pln"><a href="#n1756">1756</a></p>
<p id="n1757" class="pln"><a href="#n1757">1757</a></p>
<p id="n1758" class="pln"><a href="#n1758">1758</a></p>
<p id="n1759" class="pln"><a href="#n1759">1759</a></p>
<p id="n1760" class="pln"><a href="#n1760">1760</a></p>
<p id="n1761" class="pln"><a href="#n1761">1761</a></p>
<p id="n1762" class="stm run hide_run"><a href="#n1762">1762</a></p>
<p id="n1763" class="stm run hide_run"><a href="#n1763">1763</a></p>
<p id="n1764" class="pln"><a href="#n1764">1764</a></p>
<p id="n1765" class="stm run hide_run"><a href="#n1765">1765</a></p>
<p id="n1766" class="pln"><a href="#n1766">1766</a></p>
<p id="n1767" class="pln"><a href="#n1767">1767</a></p>
<p id="n1768" class="pln"><a href="#n1768">1768</a></p>
<p id="n1769" class="pln"><a href="#n1769">1769</a></p>
<p id="n1770" class="pln"><a href="#n1770">1770</a></p>
<p id="n1771" class="pln"><a href="#n1771">1771</a></p>
<p id="n1772" class="pln"><a href="#n1772">1772</a></p>
<p id="n1773" class="pln"><a href="#n1773">1773</a></p>
<p id="n1774" class="pln"><a href="#n1774">1774</a></p>
<p id="n1775" class="pln"><a href="#n1775">1775</a></p>
<p id="n1776" class="pln"><a href="#n1776">1776</a></p>
<p id="n1777" class="pln"><a href="#n1777">1777</a></p>
<p id="n1778" class="pln"><a href="#n1778">1778</a></p>
<p id="n1779" class="pln"><a href="#n1779">1779</a></p>
<p id="n1780" class="pln"><a href="#n1780">1780</a></p>
<p id="n1781" class="pln"><a href="#n1781">1781</a></p>
<p id="n1782" class="pln"><a href="#n1782">1782</a></p>
<p id="n1783" class="pln"><a href="#n1783">1783</a></p>
<p id="n1784" class="pln"><a href="#n1784">1784</a></p>
<p id="n1785" class="pln"><a href="#n1785">1785</a></p>
<p id="n1786" class="pln"><a href="#n1786">1786</a></p>
<p id="n1787" class="pln"><a href="#n1787">1787</a></p>
<p id="n1788" class="pln"><a href="#n1788">1788</a></p>
<p id="n1789" class="pln"><a href="#n1789">1789</a></p>
<p id="n1790" class="pln"><a href="#n1790">1790</a></p>
<p id="n1791" class="pln"><a href="#n1791">1791</a></p>
<p id="n1792" class="pln"><a href="#n1792">1792</a></p>
<p id="n1793" class="pln"><a href="#n1793">1793</a></p>
<p id="n1794" class="pln"><a href="#n1794">1794</a></p>
<p id="n1795" class="pln"><a href="#n1795">1795</a></p>
<p id="n1796" class="pln"><a href="#n1796">1796</a></p>
<p id="n1797" class="pln"><a href="#n1797">1797</a></p>
<p id="n1798" class="pln"><a href="#n1798">1798</a></p>
<p id="n1799" class="pln"><a href="#n1799">1799</a></p>
<p id="n1800" class="pln"><a href="#n1800">1800</a></p>
<p id="n1801" class="pln"><a href="#n1801">1801</a></p>
<p id="n1802" class="pln"><a href="#n1802">1802</a></p>
<p id="n1803" class="pln"><a href="#n1803">1803</a></p>
<p id="n1804" class="pln"><a href="#n1804">1804</a></p>
<p id="n1805" class="pln"><a href="#n1805">1805</a></p>
<p id="n1806" class="pln"><a href="#n1806">1806</a></p>
<p id="n1807" class="pln"><a href="#n1807">1807</a></p>
<p id="n1808" class="pln"><a href="#n1808">1808</a></p>
<p id="n1809" class="stm mis"><a href="#n1809">1809</a></p>
<p id="n1810" class="pln"><a href="#n1810">1810</a></p>
<p id="n1811" class="stm mis"><a href="#n1811">1811</a></p>
<p id="n1812" class="stm mis"><a href="#n1812">1812</a></p>
<p id="n1813" class="stm mis"><a href="#n1813">1813</a></p>
<p id="n1814" class="stm mis"><a href="#n1814">1814</a></p>
<p id="n1815" class="stm mis"><a href="#n1815">1815</a></p>
<p id="n1816" class="pln"><a href="#n1816">1816</a></p>
<p id="n1817" class="pln"><a href="#n1817">1817</a></p>
<p id="n1818" class="pln"><a href="#n1818">1818</a></p>
<p id="n1819" class="pln"><a href="#n1819">1819</a></p>
<p id="n1820" class="stm mis"><a href="#n1820">1820</a></p>
<p id="n1821" class="pln"><a href="#n1821">1821</a></p>
<p id="n1822" class="pln"><a href="#n1822">1822</a></p>
<p id="n1823" class="pln"><a href="#n1823">1823</a></p>
<p id="n1824" class="stm mis"><a href="#n1824">1824</a></p>
<p id="n1825" class="pln"><a href="#n1825">1825</a></p>
<p id="n1826" class="pln"><a href="#n1826">1826</a></p>
<p id="n1827" class="pln"><a href="#n1827">1827</a></p>
<p id="n1828" class="pln"><a href="#n1828">1828</a></p>
<p id="n1829" class="pln"><a href="#n1829">1829</a></p>
<p id="n1830" class="stm mis"><a href="#n1830">1830</a></p>
<p id="n1831" class="stm mis"><a href="#n1831">1831</a></p>
<p id="n1832" class="stm mis"><a href="#n1832">1832</a></p>
<p id="n1833" class="stm mis"><a href="#n1833">1833</a></p>
<p id="n1834" class="pln"><a href="#n1834">1834</a></p>
<p id="n1835" class="pln"><a href="#n1835">1835</a></p>
<p id="n1836" class="stm run hide_run"><a href="#n1836">1836</a></p>
<p id="n1837" class="stm run hide_run"><a href="#n1837">1837</a></p>
<p id="n1838" class="stm run hide_run"><a href="#n1838">1838</a></p>
<p id="n1839" class="pln"><a href="#n1839">1839</a></p>
<p id="n1840" class="pln"><a href="#n1840">1840</a></p>
<p id="n1841" class="pln"><a href="#n1841">1841</a></p>
<p id="n1842" class="pln"><a href="#n1842">1842</a></p>
<p id="n1843" class="pln"><a href="#n1843">1843</a></p>
<p id="n1844" class="pln"><a href="#n1844">1844</a></p>
<p id="n1845" class="pln"><a href="#n1845">1845</a></p>
<p id="n1846" class="pln"><a href="#n1846">1846</a></p>
<p id="n1847" class="pln"><a href="#n1847">1847</a></p>
<p id="n1848" class="pln"><a href="#n1848">1848</a></p>
<p id="n1849" class="pln"><a href="#n1849">1849</a></p>
<p id="n1850" class="pln"><a href="#n1850">1850</a></p>
<p id="n1851" class="pln"><a href="#n1851">1851</a></p>
<p id="n1852" class="pln"><a href="#n1852">1852</a></p>
<p id="n1853" class="pln"><a href="#n1853">1853</a></p>
<p id="n1854" class="pln"><a href="#n1854">1854</a></p>
<p id="n1855" class="pln"><a href="#n1855">1855</a></p>
<p id="n1856" class="pln"><a href="#n1856">1856</a></p>
<p id="n1857" class="pln"><a href="#n1857">1857</a></p>
<p id="n1858" class="pln"><a href="#n1858">1858</a></p>
<p id="n1859" class="pln"><a href="#n1859">1859</a></p>
<p id="n1860" class="pln"><a href="#n1860">1860</a></p>
<p id="n1861" class="pln"><a href="#n1861">1861</a></p>
<p id="n1862" class="pln"><a href="#n1862">1862</a></p>
<p id="n1863" class="pln"><a href="#n1863">1863</a></p>
<p id="n1864" class="pln"><a href="#n1864">1864</a></p>
<p id="n1865" class="pln"><a href="#n1865">1865</a></p>
<p id="n1866" class="pln"><a href="#n1866">1866</a></p>
<p id="n1867" class="pln"><a href="#n1867">1867</a></p>
<p id="n1868" class="pln"><a href="#n1868">1868</a></p>
<p id="n1869" class="pln"><a href="#n1869">1869</a></p>
<p id="n1870" class="pln"><a href="#n1870">1870</a></p>
<p id="n1871" class="pln"><a href="#n1871">1871</a></p>
<p id="n1872" class="pln"><a href="#n1872">1872</a></p>
<p id="n1873" class="pln"><a href="#n1873">1873</a></p>
<p id="n1874" class="stm mis"><a href="#n1874">1874</a></p>
<p id="n1875" class="stm mis"><a href="#n1875">1875</a></p>
<p id="n1876" class="stm mis"><a href="#n1876">1876</a></p>
<p id="n1877" class="pln"><a href="#n1877">1877</a></p>
<p id="n1878" class="pln"><a href="#n1878">1878</a></p>
<p id="n1879" class="stm run hide_run"><a href="#n1879">1879</a></p>
<p id="n1880" class="stm run hide_run"><a href="#n1880">1880</a></p>
<p id="n1881" class="pln"><a href="#n1881">1881</a></p>
<p id="n1882" class="pln"><a href="#n1882">1882</a></p>
<p id="n1883" class="pln"><a href="#n1883">1883</a></p>
<p id="n1884" class="pln"><a href="#n1884">1884</a></p>
<p id="n1885" class="pln"><a href="#n1885">1885</a></p>
<p id="n1886" class="pln"><a href="#n1886">1886</a></p>
<p id="n1887" class="pln"><a href="#n1887">1887</a></p>
<p id="n1888" class="pln"><a href="#n1888">1888</a></p>
<p id="n1889" class="pln"><a href="#n1889">1889</a></p>
<p id="n1890" class="pln"><a href="#n1890">1890</a></p>
<p id="n1891" class="pln"><a href="#n1891">1891</a></p>
<p id="n1892" class="pln"><a href="#n1892">1892</a></p>
<p id="n1893" class="pln"><a href="#n1893">1893</a></p>
<p id="n1894" class="pln"><a href="#n1894">1894</a></p>
<p id="n1895" class="pln"><a href="#n1895">1895</a></p>
<p id="n1896" class="pln"><a href="#n1896">1896</a></p>
<p id="n1897" class="pln"><a href="#n1897">1897</a></p>
<p id="n1898" class="pln"><a href="#n1898">1898</a></p>
<p id="n1899" class="pln"><a href="#n1899">1899</a></p>
<p id="n1900" class="pln"><a href="#n1900">1900</a></p>
<p id="n1901" class="pln"><a href="#n1901">1901</a></p>
<p id="n1902" class="pln"><a href="#n1902">1902</a></p>
<p id="n1903" class="pln"><a href="#n1903">1903</a></p>
<p id="n1904" class="pln"><a href="#n1904">1904</a></p>
<p id="n1905" class="pln"><a href="#n1905">1905</a></p>
<p id="n1906" class="pln"><a href="#n1906">1906</a></p>
<p id="n1907" class="pln"><a href="#n1907">1907</a></p>
<p id="n1908" class="pln"><a href="#n1908">1908</a></p>
<p id="n1909" class="pln"><a href="#n1909">1909</a></p>
<p id="n1910" class="pln"><a href="#n1910">1910</a></p>
<p id="n1911" class="pln"><a href="#n1911">1911</a></p>
<p id="n1912" class="pln"><a href="#n1912">1912</a></p>
<p id="n1913" class="pln"><a href="#n1913">1913</a></p>
<p id="n1914" class="pln"><a href="#n1914">1914</a></p>
<p id="n1915" class="pln"><a href="#n1915">1915</a></p>
<p id="n1916" class="pln"><a href="#n1916">1916</a></p>
<p id="n1917" class="pln"><a href="#n1917">1917</a></p>
<p id="n1918" class="pln"><a href="#n1918">1918</a></p>
<p id="n1919" class="pln"><a href="#n1919">1919</a></p>
<p id="n1920" class="pln"><a href="#n1920">1920</a></p>
<p id="n1921" class="pln"><a href="#n1921">1921</a></p>
<p id="n1922" class="pln"><a href="#n1922">1922</a></p>
<p id="n1923" class="pln"><a href="#n1923">1923</a></p>
<p id="n1924" class="pln"><a href="#n1924">1924</a></p>
<p id="n1925" class="pln"><a href="#n1925">1925</a></p>
<p id="n1926" class="pln"><a href="#n1926">1926</a></p>
<p id="n1927" class="pln"><a href="#n1927">1927</a></p>
<p id="n1928" class="pln"><a href="#n1928">1928</a></p>
<p id="n1929" class="pln"><a href="#n1929">1929</a></p>
<p id="n1930" class="pln"><a href="#n1930">1930</a></p>
<p id="n1931" class="pln"><a href="#n1931">1931</a></p>
<p id="n1932" class="pln"><a href="#n1932">1932</a></p>
<p id="n1933" class="pln"><a href="#n1933">1933</a></p>
<p id="n1934" class="pln"><a href="#n1934">1934</a></p>
<p id="n1935" class="pln"><a href="#n1935">1935</a></p>
<p id="n1936" class="pln"><a href="#n1936">1936</a></p>
<p id="n1937" class="pln"><a href="#n1937">1937</a></p>
<p id="n1938" class="pln"><a href="#n1938">1938</a></p>
<p id="n1939" class="pln"><a href="#n1939">1939</a></p>
<p id="n1940" class="pln"><a href="#n1940">1940</a></p>
<p id="n1941" class="pln"><a href="#n1941">1941</a></p>
<p id="n1942" class="pln"><a href="#n1942">1942</a></p>
<p id="n1943" class="pln"><a href="#n1943">1943</a></p>
<p id="n1944" class="pln"><a href="#n1944">1944</a></p>
<p id="n1945" class="pln"><a href="#n1945">1945</a></p>
<p id="n1946" class="pln"><a href="#n1946">1946</a></p>
<p id="n1947" class="pln"><a href="#n1947">1947</a></p>
<p id="n1948" class="pln"><a href="#n1948">1948</a></p>
<p id="n1949" class="pln"><a href="#n1949">1949</a></p>
<p id="n1950" class="pln"><a href="#n1950">1950</a></p>
<p id="n1951" class="pln"><a href="#n1951">1951</a></p>
<p id="n1952" class="pln"><a href="#n1952">1952</a></p>
<p id="n1953" class="pln"><a href="#n1953">1953</a></p>
<p id="n1954" class="pln"><a href="#n1954">1954</a></p>
<p id="n1955" class="pln"><a href="#n1955">1955</a></p>
<p id="n1956" class="pln"><a href="#n1956">1956</a></p>
<p id="n1957" class="pln"><a href="#n1957">1957</a></p>
<p id="n1958" class="pln"><a href="#n1958">1958</a></p>
<p id="n1959" class="pln"><a href="#n1959">1959</a></p>
<p id="n1960" class="pln"><a href="#n1960">1960</a></p>
<p id="n1961" class="pln"><a href="#n1961">1961</a></p>
<p id="n1962" class="pln"><a href="#n1962">1962</a></p>
<p id="n1963" class="pln"><a href="#n1963">1963</a></p>
<p id="n1964" class="pln"><a href="#n1964">1964</a></p>
<p id="n1965" class="pln"><a href="#n1965">1965</a></p>
<p id="n1966" class="pln"><a href="#n1966">1966</a></p>
<p id="n1967" class="pln"><a href="#n1967">1967</a></p>
<p id="n1968" class="pln"><a href="#n1968">1968</a></p>
<p id="n1969" class="pln"><a href="#n1969">1969</a></p>
<p id="n1970" class="pln"><a href="#n1970">1970</a></p>
<p id="n1971" class="pln"><a href="#n1971">1971</a></p>
<p id="n1972" class="pln"><a href="#n1972">1972</a></p>
<p id="n1973" class="pln"><a href="#n1973">1973</a></p>
<p id="n1974" class="pln"><a href="#n1974">1974</a></p>
<p id="n1975" class="pln"><a href="#n1975">1975</a></p>
<p id="n1976" class="pln"><a href="#n1976">1976</a></p>
<p id="n1977" class="pln"><a href="#n1977">1977</a></p>
<p id="n1978" class="pln"><a href="#n1978">1978</a></p>
<p id="n1979" class="pln"><a href="#n1979">1979</a></p>
<p id="n1980" class="pln"><a href="#n1980">1980</a></p>
<p id="n1981" class="pln"><a href="#n1981">1981</a></p>
<p id="n1982" class="pln"><a href="#n1982">1982</a></p>
<p id="n1983" class="pln"><a href="#n1983">1983</a></p>
<p id="n1984" class="pln"><a href="#n1984">1984</a></p>
<p id="n1985" class="pln"><a href="#n1985">1985</a></p>
<p id="n1986" class="pln"><a href="#n1986">1986</a></p>
<p id="n1987" class="pln"><a href="#n1987">1987</a></p>
<p id="n1988" class="pln"><a href="#n1988">1988</a></p>
<p id="n1989" class="stm mis"><a href="#n1989">1989</a></p>
<p id="n1990" class="stm mis"><a href="#n1990">1990</a></p>
<p id="n1991" class="stm mis"><a href="#n1991">1991</a></p>
<p id="n1992" class="stm mis"><a href="#n1992">1992</a></p>
<p id="n1993" class="stm mis"><a href="#n1993">1993</a></p>
<p id="n1994" class="pln"><a href="#n1994">1994</a></p>
<p id="n1995" class="stm mis"><a href="#n1995">1995</a></p>
<p id="n1996" class="stm mis"><a href="#n1996">1996</a></p>
<p id="n1997" class="stm mis"><a href="#n1997">1997</a></p>
<p id="n1998" class="stm mis"><a href="#n1998">1998</a></p>
<p id="n1999" class="stm mis"><a href="#n1999">1999</a></p>
<p id="n2000" class="pln"><a href="#n2000">2000</a></p>
<p id="n2001" class="stm mis"><a href="#n2001">2001</a></p>
<p id="n2002" class="stm mis"><a href="#n2002">2002</a></p>
<p id="n2003" class="pln"><a href="#n2003">2003</a></p>
<p id="n2004" class="pln"><a href="#n2004">2004</a></p>
<p id="n2005" class="stm mis"><a href="#n2005">2005</a></p>
<p id="n2006" class="stm mis"><a href="#n2006">2006</a></p>
<p id="n2007" class="stm mis"><a href="#n2007">2007</a></p>
<p id="n2008" class="pln"><a href="#n2008">2008</a></p>
<p id="n2009" class="pln"><a href="#n2009">2009</a></p>
<p id="n2010" class="pln"><a href="#n2010">2010</a></p>
<p id="n2011" class="pln"><a href="#n2011">2011</a></p>
<p id="n2012" class="stm mis"><a href="#n2012">2012</a></p>
<p id="n2013" class="stm mis"><a href="#n2013">2013</a></p>
<p id="n2014" class="stm mis"><a href="#n2014">2014</a></p>
<p id="n2015" class="stm mis"><a href="#n2015">2015</a></p>
<p id="n2016" class="stm mis"><a href="#n2016">2016</a></p>
<p id="n2017" class="stm mis"><a href="#n2017">2017</a></p>
<p id="n2018" class="stm mis"><a href="#n2018">2018</a></p>
<p id="n2019" class="pln"><a href="#n2019">2019</a></p>
<p id="n2020" class="pln"><a href="#n2020">2020</a></p>
<p id="n2021" class="pln"><a href="#n2021">2021</a></p>
<p id="n2022" class="pln"><a href="#n2022">2022</a></p>
<p id="n2023" class="pln"><a href="#n2023">2023</a></p>
<p id="n2024" class="stm mis"><a href="#n2024">2024</a></p>
<p id="n2025" class="stm mis"><a href="#n2025">2025</a></p>
<p id="n2026" class="stm mis"><a href="#n2026">2026</a></p>
<p id="n2027" class="stm mis"><a href="#n2027">2027</a></p>
<p id="n2028" class="stm mis"><a href="#n2028">2028</a></p>
<p id="n2029" class="stm mis"><a href="#n2029">2029</a></p>
<p id="n2030" class="pln"><a href="#n2030">2030</a></p>
<p id="n2031" class="stm mis"><a href="#n2031">2031</a></p>
<p id="n2032" class="stm mis"><a href="#n2032">2032</a></p>
<p id="n2033" class="stm mis"><a href="#n2033">2033</a></p>
<p id="n2034" class="stm mis"><a href="#n2034">2034</a></p>
<p id="n2035" class="pln"><a href="#n2035">2035</a></p>
<p id="n2036" class="stm mis"><a href="#n2036">2036</a></p>
<p id="n2037" class="pln"><a href="#n2037">2037</a></p>
<p id="n2038" class="pln"><a href="#n2038">2038</a></p>
<p id="n2039" class="stm mis"><a href="#n2039">2039</a></p>
<p id="n2040" class="stm mis"><a href="#n2040">2040</a></p>
<p id="n2041" class="stm mis"><a href="#n2041">2041</a></p>
<p id="n2042" class="pln"><a href="#n2042">2042</a></p>
<p id="n2043" class="pln"><a href="#n2043">2043</a></p>
<p id="n2044" class="pln"><a href="#n2044">2044</a></p>
<p id="n2045" class="pln"><a href="#n2045">2045</a></p>
<p id="n2046" class="pln"><a href="#n2046">2046</a></p>
<p id="n2047" class="pln"><a href="#n2047">2047</a></p>
<p id="n2048" class="pln"><a href="#n2048">2048</a></p>
<p id="n2049" class="pln"><a href="#n2049">2049</a></p>
<p id="n2050" class="pln"><a href="#n2050">2050</a></p>
<p id="n2051" class="pln"><a href="#n2051">2051</a></p>
<p id="n2052" class="stm mis"><a href="#n2052">2052</a></p>
<p id="n2053" class="stm mis"><a href="#n2053">2053</a></p>
<p id="n2054" class="stm mis"><a href="#n2054">2054</a></p>
<p id="n2055" class="pln"><a href="#n2055">2055</a></p>
<p id="n2056" class="stm mis"><a href="#n2056">2056</a></p>
<p id="n2057" class="pln"><a href="#n2057">2057</a></p>
<p id="n2058" class="pln"><a href="#n2058">2058</a></p>
<p id="n2059" class="pln"><a href="#n2059">2059</a></p>
<p id="n2060" class="stm run hide_run"><a href="#n2060">2060</a></p>
<p id="n2061" class="pln"><a href="#n2061">2061</a></p>
<p id="n2062" class="stm run hide_run"><a href="#n2062">2062</a></p>
<p id="n2063" class="stm run hide_run"><a href="#n2063">2063</a></p>
<p id="n2064" class="pln"><a href="#n2064">2064</a></p>
<p id="n2065" class="pln"><a href="#n2065">2065</a></p>
<p id="n2066" class="stm run hide_run"><a href="#n2066">2066</a></p>
<p id="n2067" class="pln"><a href="#n2067">2067</a></p>
<p id="n2068" class="pln"><a href="#n2068">2068</a></p>
<p id="n2069" class="stm mis"><a href="#n2069">2069</a></p>
<p id="n2070" class="stm mis"><a href="#n2070">2070</a></p>
<p id="n2071" class="stm mis"><a href="#n2071">2071</a></p>
<p id="n2072" class="stm mis"><a href="#n2072">2072</a></p>
<p id="n2073" class="stm mis"><a href="#n2073">2073</a></p>
<p id="n2074" class="pln"><a href="#n2074">2074</a></p>
<p id="n2075" class="stm mis"><a href="#n2075">2075</a></p>
<p id="n2076" class="stm mis"><a href="#n2076">2076</a></p>
<p id="n2077" class="stm mis"><a href="#n2077">2077</a></p>
<p id="n2078" class="stm mis"><a href="#n2078">2078</a></p>
<p id="n2079" class="stm mis"><a href="#n2079">2079</a></p>
<p id="n2080" class="pln"><a href="#n2080">2080</a></p>
<p id="n2081" class="pln"><a href="#n2081">2081</a></p>
<p id="n2082" class="stm run hide_run"><a href="#n2082">2082</a></p>
<p id="n2083" class="pln"><a href="#n2083">2083</a></p>
<p id="n2084" class="pln"><a href="#n2084">2084</a></p>
<p id="n2085" class="pln"><a href="#n2085">2085</a></p>
<p id="n2086" class="pln"><a href="#n2086">2086</a></p>
<p id="n2087" class="pln"><a href="#n2087">2087</a></p>
<p id="n2088" class="pln"><a href="#n2088">2088</a></p>
<p id="n2089" class="pln"><a href="#n2089">2089</a></p>
<p id="n2090" class="pln"><a href="#n2090">2090</a></p>
<p id="n2091" class="pln"><a href="#n2091">2091</a></p>
<p id="n2092" class="pln"><a href="#n2092">2092</a></p>
<p id="n2093" class="pln"><a href="#n2093">2093</a></p>
<p id="n2094" class="pln"><a href="#n2094">2094</a></p>
<p id="n2095" class="pln"><a href="#n2095">2095</a></p>
<p id="n2096" class="pln"><a href="#n2096">2096</a></p>
<p id="n2097" class="pln"><a href="#n2097">2097</a></p>
<p id="n2098" class="stm mis"><a href="#n2098">2098</a></p>
<p id="n2099" class="stm mis"><a href="#n2099">2099</a></p>
<p id="n2100" class="stm mis"><a href="#n2100">2100</a></p>
<p id="n2101" class="stm mis"><a href="#n2101">2101</a></p>
<p id="n2102" class="stm mis"><a href="#n2102">2102</a></p>
<p id="n2103" class="stm mis"><a href="#n2103">2103</a></p>
<p id="n2104" class="pln"><a href="#n2104">2104</a></p>
<p id="n2105" class="pln"><a href="#n2105">2105</a></p>
<p id="n2106" class="stm run hide_run"><a href="#n2106">2106</a></p>
<p id="n2107" class="pln"><a href="#n2107">2107</a></p>
<p id="n2108" class="pln"><a href="#n2108">2108</a></p>
<p id="n2109" class="pln"><a href="#n2109">2109</a></p>
<p id="n2110" class="pln"><a href="#n2110">2110</a></p>
<p id="n2111" class="pln"><a href="#n2111">2111</a></p>
<p id="n2112" class="pln"><a href="#n2112">2112</a></p>
<p id="n2113" class="pln"><a href="#n2113">2113</a></p>
<p id="n2114" class="pln"><a href="#n2114">2114</a></p>
<p id="n2115" class="pln"><a href="#n2115">2115</a></p>
<p id="n2116" class="pln"><a href="#n2116">2116</a></p>
<p id="n2117" class="pln"><a href="#n2117">2117</a></p>
<p id="n2118" class="pln"><a href="#n2118">2118</a></p>
<p id="n2119" class="pln"><a href="#n2119">2119</a></p>
<p id="n2120" class="pln"><a href="#n2120">2120</a></p>
<p id="n2121" class="pln"><a href="#n2121">2121</a></p>
<p id="n2122" class="stm mis"><a href="#n2122">2122</a></p>
<p id="n2123" class="stm mis"><a href="#n2123">2123</a></p>
<p id="n2124" class="stm mis"><a href="#n2124">2124</a></p>
<p id="n2125" class="stm mis"><a href="#n2125">2125</a></p>
<p id="n2126" class="pln"><a href="#n2126">2126</a></p>
<p id="n2127" class="pln"><a href="#n2127">2127</a></p>
<p id="n2128" class="stm mis"><a href="#n2128">2128</a></p>
<p id="n2129" class="stm mis"><a href="#n2129">2129</a></p>
<p id="n2130" class="stm mis"><a href="#n2130">2130</a></p>
<p id="n2131" class="stm mis"><a href="#n2131">2131</a></p>
<p id="n2132" class="stm mis"><a href="#n2132">2132</a></p>
<p id="n2133" class="stm mis"><a href="#n2133">2133</a></p>
<p id="n2134" class="pln"><a href="#n2134">2134</a></p>
<p id="n2135" class="pln"><a href="#n2135">2135</a></p>
<p id="n2136" class="pln"><a href="#n2136">2136</a></p>
<p id="n2137" class="stm mis"><a href="#n2137">2137</a></p>
<p id="n2138" class="stm mis"><a href="#n2138">2138</a></p>
<p id="n2139" class="pln"><a href="#n2139">2139</a></p>
<p id="n2140" class="pln"><a href="#n2140">2140</a></p>
<p id="n2141" class="stm run hide_run"><a href="#n2141">2141</a></p>
<p id="n2142" class="stm run hide_run"><a href="#n2142">2142</a></p>
<p id="n2143" class="pln"><a href="#n2143">2143</a></p>
<p id="n2144" class="pln"><a href="#n2144">2144</a></p>
<p id="n2145" class="pln"><a href="#n2145">2145</a></p>
<p id="n2146" class="pln"><a href="#n2146">2146</a></p>
<p id="n2147" class="pln"><a href="#n2147">2147</a></p>
<p id="n2148" class="pln"><a href="#n2148">2148</a></p>
<p id="n2149" class="pln"><a href="#n2149">2149</a></p>
<p id="n2150" class="pln"><a href="#n2150">2150</a></p>
<p id="n2151" class="pln"><a href="#n2151">2151</a></p>
<p id="n2152" class="pln"><a href="#n2152">2152</a></p>
<p id="n2153" class="pln"><a href="#n2153">2153</a></p>
<p id="n2154" class="pln"><a href="#n2154">2154</a></p>
<p id="n2155" class="pln"><a href="#n2155">2155</a></p>
<p id="n2156" class="pln"><a href="#n2156">2156</a></p>
<p id="n2157" class="stm mis"><a href="#n2157">2157</a></p>
<p id="n2158" class="stm mis"><a href="#n2158">2158</a></p>
<p id="n2159" class="pln"><a href="#n2159">2159</a></p>
<p id="n2160" class="stm mis"><a href="#n2160">2160</a></p>
<p id="n2161" class="stm mis"><a href="#n2161">2161</a></p>
<p id="n2162" class="stm mis"><a href="#n2162">2162</a></p>
<p id="n2163" class="pln"><a href="#n2163">2163</a></p>
<p id="n2164" class="pln"><a href="#n2164">2164</a></p>
<p id="n2165" class="stm mis"><a href="#n2165">2165</a></p>
<p id="n2166" class="stm mis"><a href="#n2166">2166</a></p>
<p id="n2167" class="stm mis"><a href="#n2167">2167</a></p>
<p id="n2168" class="pln"><a href="#n2168">2168</a></p>
<p id="n2169" class="stm mis"><a href="#n2169">2169</a></p>
<p id="n2170" class="stm mis"><a href="#n2170">2170</a></p>
<p id="n2171" class="stm mis"><a href="#n2171">2171</a></p>
<p id="n2172" class="stm mis"><a href="#n2172">2172</a></p>
<p id="n2173" class="stm mis"><a href="#n2173">2173</a></p>
<p id="n2174" class="pln"><a href="#n2174">2174</a></p>
<p id="n2175" class="pln"><a href="#n2175">2175</a></p>
<p id="n2176" class="stm run hide_run"><a href="#n2176">2176</a></p>
<p id="n2177" class="stm run hide_run"><a href="#n2177">2177</a></p>
<p id="n2178" class="stm run hide_run"><a href="#n2178">2178</a></p>
<p id="n2179" class="pln"><a href="#n2179">2179</a></p>
<p id="n2180" class="pln"><a href="#n2180">2180</a></p>
<p id="n2181" class="pln"><a href="#n2181">2181</a></p>
<p id="n2182" class="pln"><a href="#n2182">2182</a></p>
<p id="n2183" class="pln"><a href="#n2183">2183</a></p>
<p id="n2184" class="pln"><a href="#n2184">2184</a></p>
<p id="n2185" class="pln"><a href="#n2185">2185</a></p>
<p id="n2186" class="pln"><a href="#n2186">2186</a></p>
<p id="n2187" class="pln"><a href="#n2187">2187</a></p>
<p id="n2188" class="pln"><a href="#n2188">2188</a></p>
<p id="n2189" class="pln"><a href="#n2189">2189</a></p>
<p id="n2190" class="pln"><a href="#n2190">2190</a></p>
<p id="n2191" class="pln"><a href="#n2191">2191</a></p>
<p id="n2192" class="pln"><a href="#n2192">2192</a></p>
<p id="n2193" class="pln"><a href="#n2193">2193</a></p>
<p id="n2194" class="pln"><a href="#n2194">2194</a></p>
<p id="n2195" class="pln"><a href="#n2195">2195</a></p>
<p id="n2196" class="pln"><a href="#n2196">2196</a></p>
<p id="n2197" class="pln"><a href="#n2197">2197</a></p>
<p id="n2198" class="pln"><a href="#n2198">2198</a></p>
<p id="n2199" class="pln"><a href="#n2199">2199</a></p>
<p id="n2200" class="pln"><a href="#n2200">2200</a></p>
<p id="n2201" class="pln"><a href="#n2201">2201</a></p>
<p id="n2202" class="pln"><a href="#n2202">2202</a></p>
<p id="n2203" class="pln"><a href="#n2203">2203</a></p>
<p id="n2204" class="pln"><a href="#n2204">2204</a></p>
<p id="n2205" class="pln"><a href="#n2205">2205</a></p>
<p id="n2206" class="pln"><a href="#n2206">2206</a></p>
<p id="n2207" class="pln"><a href="#n2207">2207</a></p>
<p id="n2208" class="pln"><a href="#n2208">2208</a></p>
<p id="n2209" class="pln"><a href="#n2209">2209</a></p>
<p id="n2210" class="pln"><a href="#n2210">2210</a></p>
<p id="n2211" class="pln"><a href="#n2211">2211</a></p>
<p id="n2212" class="pln"><a href="#n2212">2212</a></p>
<p id="n2213" class="pln"><a href="#n2213">2213</a></p>
<p id="n2214" class="pln"><a href="#n2214">2214</a></p>
<p id="n2215" class="pln"><a href="#n2215">2215</a></p>
<p id="n2216" class="pln"><a href="#n2216">2216</a></p>
<p id="n2217" class="pln"><a href="#n2217">2217</a></p>
<p id="n2218" class="stm mis"><a href="#n2218">2218</a></p>
<p id="n2219" class="stm mis"><a href="#n2219">2219</a></p>
<p id="n2220" class="pln"><a href="#n2220">2220</a></p>
<p id="n2221" class="pln"><a href="#n2221">2221</a></p>
<p id="n2222" class="stm mis"><a href="#n2222">2222</a></p>
<p id="n2223" class="stm mis"><a href="#n2223">2223</a></p>
<p id="n2224" class="stm mis"><a href="#n2224">2224</a></p>
<p id="n2225" class="stm mis"><a href="#n2225">2225</a></p>
<p id="n2226" class="stm mis"><a href="#n2226">2226</a></p>
<p id="n2227" class="stm mis"><a href="#n2227">2227</a></p>
<p id="n2228" class="stm mis"><a href="#n2228">2228</a></p>
<p id="n2229" class="stm mis"><a href="#n2229">2229</a></p>
<p id="n2230" class="stm mis"><a href="#n2230">2230</a></p>
<p id="n2231" class="pln"><a href="#n2231">2231</a></p>
<p id="n2232" class="stm mis"><a href="#n2232">2232</a></p>
<p id="n2233" class="stm mis"><a href="#n2233">2233</a></p>
<p id="n2234" class="stm mis"><a href="#n2234">2234</a></p>
<p id="n2235" class="stm mis"><a href="#n2235">2235</a></p>
<p id="n2236" class="pln"><a href="#n2236">2236</a></p>
<p id="n2237" class="pln"><a href="#n2237">2237</a></p>
<p id="n2238" class="stm mis"><a href="#n2238">2238</a></p>
<p id="n2239" class="stm mis"><a href="#n2239">2239</a></p>
<p id="n2240" class="pln"><a href="#n2240">2240</a></p>
<p id="n2241" class="pln"><a href="#n2241">2241</a></p>
<p id="n2242" class="stm mis"><a href="#n2242">2242</a></p>
<p id="n2243" class="stm mis"><a href="#n2243">2243</a></p>
<p id="n2244" class="stm mis"><a href="#n2244">2244</a></p>
<p id="n2245" class="stm mis"><a href="#n2245">2245</a></p>
<p id="n2246" class="stm mis"><a href="#n2246">2246</a></p>
<p id="n2247" class="pln"><a href="#n2247">2247</a></p>
<p id="n2248" class="pln"><a href="#n2248">2248</a></p>
<p id="n2249" class="pln"><a href="#n2249">2249</a></p>
<p id="n2250" class="pln"><a href="#n2250">2250</a></p>
<p id="n2251" class="stm mis"><a href="#n2251">2251</a></p>
<p id="n2252" class="pln"><a href="#n2252">2252</a></p>
<p id="n2253" class="stm mis"><a href="#n2253">2253</a></p>
<p id="n2254" class="pln"><a href="#n2254">2254</a></p>
<p id="n2255" class="pln"><a href="#n2255">2255</a></p>
<p id="n2256" class="stm run hide_run"><a href="#n2256">2256</a></p>
<p id="n2257" class="pln"><a href="#n2257">2257</a></p>
<p id="n2258" class="pln"><a href="#n2258">2258</a></p>
<p id="n2259" class="pln"><a href="#n2259">2259</a></p>
<p id="n2260" class="stm mis"><a href="#n2260">2260</a></p>
<p id="n2261" class="pln"><a href="#n2261">2261</a></p>
<p id="n2262" class="pln"><a href="#n2262">2262</a></p>
<p id="n2263" class="stm run hide_run"><a href="#n2263">2263</a></p>
<p id="n2264" class="stm run hide_run"><a href="#n2264">2264</a></p>
<p id="n2265" class="pln"><a href="#n2265">2265</a></p>
<p id="n2266" class="pln"><a href="#n2266">2266</a></p>
<p id="n2267" class="pln"><a href="#n2267">2267</a></p>
<p id="n2268" class="pln"><a href="#n2268">2268</a></p>
<p id="n2269" class="pln"><a href="#n2269">2269</a></p>
<p id="n2270" class="pln"><a href="#n2270">2270</a></p>
<p id="n2271" class="pln"><a href="#n2271">2271</a></p>
<p id="n2272" class="pln"><a href="#n2272">2272</a></p>
<p id="n2273" class="pln"><a href="#n2273">2273</a></p>
<p id="n2274" class="pln"><a href="#n2274">2274</a></p>
<p id="n2275" class="pln"><a href="#n2275">2275</a></p>
<p id="n2276" class="pln"><a href="#n2276">2276</a></p>
<p id="n2277" class="pln"><a href="#n2277">2277</a></p>
<p id="n2278" class="pln"><a href="#n2278">2278</a></p>
<p id="n2279" class="pln"><a href="#n2279">2279</a></p>
<p id="n2280" class="pln"><a href="#n2280">2280</a></p>
<p id="n2281" class="stm mis"><a href="#n2281">2281</a></p>
<p id="n2282" class="stm mis"><a href="#n2282">2282</a></p>
<p id="n2283" class="stm mis"><a href="#n2283">2283</a></p>
<p id="n2284" class="pln"><a href="#n2284">2284</a></p>
<p id="n2285" class="pln"><a href="#n2285">2285</a></p>
<p id="n2286" class="stm run hide_run"><a href="#n2286">2286</a></p>
<p id="n2287" class="stm run hide_run"><a href="#n2287">2287</a></p>
<p id="n2288" class="stm run hide_run"><a href="#n2288">2288</a></p>
<p id="n2289" class="pln"><a href="#n2289">2289</a></p>
<p id="n2290" class="pln"><a href="#n2290">2290</a></p>
<p id="n2291" class="pln"><a href="#n2291">2291</a></p>
<p id="n2292" class="pln"><a href="#n2292">2292</a></p>
<p id="n2293" class="pln"><a href="#n2293">2293</a></p>
<p id="n2294" class="pln"><a href="#n2294">2294</a></p>
<p id="n2295" class="pln"><a href="#n2295">2295</a></p>
<p id="n2296" class="pln"><a href="#n2296">2296</a></p>
<p id="n2297" class="pln"><a href="#n2297">2297</a></p>
<p id="n2298" class="pln"><a href="#n2298">2298</a></p>
<p id="n2299" class="pln"><a href="#n2299">2299</a></p>
<p id="n2300" class="pln"><a href="#n2300">2300</a></p>
<p id="n2301" class="stm mis"><a href="#n2301">2301</a></p>
<p id="n2302" class="stm mis"><a href="#n2302">2302</a></p>
<p id="n2303" class="stm mis"><a href="#n2303">2303</a></p>
<p id="n2304" class="pln"><a href="#n2304">2304</a></p>
<p id="n2305" class="pln"><a href="#n2305">2305</a></p>
<p id="n2306" class="stm run hide_run"><a href="#n2306">2306</a></p>
<p id="n2307" class="stm run hide_run"><a href="#n2307">2307</a></p>
<p id="n2308" class="pln"><a href="#n2308">2308</a></p>
<p id="n2309" class="pln"><a href="#n2309">2309</a></p>
<p id="n2310" class="pln"><a href="#n2310">2310</a></p>
<p id="n2311" class="pln"><a href="#n2311">2311</a></p>
<p id="n2312" class="pln"><a href="#n2312">2312</a></p>
<p id="n2313" class="pln"><a href="#n2313">2313</a></p>
<p id="n2314" class="pln"><a href="#n2314">2314</a></p>
<p id="n2315" class="pln"><a href="#n2315">2315</a></p>
<p id="n2316" class="pln"><a href="#n2316">2316</a></p>
<p id="n2317" class="pln"><a href="#n2317">2317</a></p>
<p id="n2318" class="stm mis"><a href="#n2318">2318</a></p>
<p id="n2319" class="stm mis"><a href="#n2319">2319</a></p>
<p id="n2320" class="stm mis"><a href="#n2320">2320</a></p>
<p id="n2321" class="stm mis"><a href="#n2321">2321</a></p>
<p id="n2322" class="pln"><a href="#n2322">2322</a></p>
<p id="n2323" class="pln"><a href="#n2323">2323</a></p>
<p id="n2324" class="stm mis"><a href="#n2324">2324</a></p>
<p id="n2325" class="pln"><a href="#n2325">2325</a></p>
<p id="n2326" class="pln"><a href="#n2326">2326</a></p>
<p id="n2327" class="stm run hide_run"><a href="#n2327">2327</a></p>
<p id="n2328" class="stm run hide_run"><a href="#n2328">2328</a></p>
<p id="n2329" class="stm run hide_run"><a href="#n2329">2329</a></p>
<p id="n2330" class="pln"><a href="#n2330">2330</a></p>
<p id="n2331" class="pln"><a href="#n2331">2331</a></p>
<p id="n2332" class="pln"><a href="#n2332">2332</a></p>
<p id="n2333" class="pln"><a href="#n2333">2333</a></p>
<p id="n2334" class="pln"><a href="#n2334">2334</a></p>
<p id="n2335" class="pln"><a href="#n2335">2335</a></p>
<p id="n2336" class="pln"><a href="#n2336">2336</a></p>
<p id="n2337" class="pln"><a href="#n2337">2337</a></p>
<p id="n2338" class="pln"><a href="#n2338">2338</a></p>
<p id="n2339" class="pln"><a href="#n2339">2339</a></p>
<p id="n2340" class="pln"><a href="#n2340">2340</a></p>
<p id="n2341" class="pln"><a href="#n2341">2341</a></p>
<p id="n2342" class="pln"><a href="#n2342">2342</a></p>
<p id="n2343" class="pln"><a href="#n2343">2343</a></p>
<p id="n2344" class="pln"><a href="#n2344">2344</a></p>
<p id="n2345" class="pln"><a href="#n2345">2345</a></p>
<p id="n2346" class="pln"><a href="#n2346">2346</a></p>
<p id="n2347" class="pln"><a href="#n2347">2347</a></p>
<p id="n2348" class="pln"><a href="#n2348">2348</a></p>
<p id="n2349" class="pln"><a href="#n2349">2349</a></p>
<p id="n2350" class="pln"><a href="#n2350">2350</a></p>
<p id="n2351" class="pln"><a href="#n2351">2351</a></p>
<p id="n2352" class="pln"><a href="#n2352">2352</a></p>
<p id="n2353" class="pln"><a href="#n2353">2353</a></p>
<p id="n2354" class="pln"><a href="#n2354">2354</a></p>
<p id="n2355" class="pln"><a href="#n2355">2355</a></p>
<p id="n2356" class="pln"><a href="#n2356">2356</a></p>
<p id="n2357" class="stm mis"><a href="#n2357">2357</a></p>
<p id="n2358" class="stm mis"><a href="#n2358">2358</a></p>
<p id="n2359" class="stm mis"><a href="#n2359">2359</a></p>
<p id="n2360" class="stm mis"><a href="#n2360">2360</a></p>
<p id="n2361" class="stm mis"><a href="#n2361">2361</a></p>
<p id="n2362" class="pln"><a href="#n2362">2362</a></p>
<p id="n2363" class="stm mis"><a href="#n2363">2363</a></p>
<p id="n2364" class="stm mis"><a href="#n2364">2364</a></p>
<p id="n2365" class="stm mis"><a href="#n2365">2365</a></p>
<p id="n2366" class="pln"><a href="#n2366">2366</a></p>
<p id="n2367" class="stm mis"><a href="#n2367">2367</a></p>
<p id="n2368" class="stm mis"><a href="#n2368">2368</a></p>
<p id="n2369" class="stm mis"><a href="#n2369">2369</a></p>
<p id="n2370" class="stm mis"><a href="#n2370">2370</a></p>
<p id="n2371" class="stm mis"><a href="#n2371">2371</a></p>
<p id="n2372" class="stm mis"><a href="#n2372">2372</a></p>
<p id="n2373" class="pln"><a href="#n2373">2373</a></p>
<p id="n2374" class="pln"><a href="#n2374">2374</a></p>
<p id="n2375" class="stm run hide_run"><a href="#n2375">2375</a></p>
<p id="n2376" class="stm run hide_run"><a href="#n2376">2376</a></p>
<p id="n2377" class="pln"><a href="#n2377">2377</a></p>
<p id="n2378" class="pln"><a href="#n2378">2378</a></p>
<p id="n2379" class="pln"><a href="#n2379">2379</a></p>
<p id="n2380" class="pln"><a href="#n2380">2380</a></p>
<p id="n2381" class="pln"><a href="#n2381">2381</a></p>
<p id="n2382" class="pln"><a href="#n2382">2382</a></p>
<p id="n2383" class="pln"><a href="#n2383">2383</a></p>
<p id="n2384" class="pln"><a href="#n2384">2384</a></p>
<p id="n2385" class="pln"><a href="#n2385">2385</a></p>
<p id="n2386" class="pln"><a href="#n2386">2386</a></p>
<p id="n2387" class="pln"><a href="#n2387">2387</a></p>
<p id="n2388" class="pln"><a href="#n2388">2388</a></p>
<p id="n2389" class="pln"><a href="#n2389">2389</a></p>
<p id="n2390" class="pln"><a href="#n2390">2390</a></p>
<p id="n2391" class="pln"><a href="#n2391">2391</a></p>
<p id="n2392" class="pln"><a href="#n2392">2392</a></p>
<p id="n2393" class="pln"><a href="#n2393">2393</a></p>
<p id="n2394" class="pln"><a href="#n2394">2394</a></p>
<p id="n2395" class="pln"><a href="#n2395">2395</a></p>
<p id="n2396" class="pln"><a href="#n2396">2396</a></p>
<p id="n2397" class="pln"><a href="#n2397">2397</a></p>
<p id="n2398" class="pln"><a href="#n2398">2398</a></p>
<p id="n2399" class="pln"><a href="#n2399">2399</a></p>
<p id="n2400" class="pln"><a href="#n2400">2400</a></p>
<p id="n2401" class="pln"><a href="#n2401">2401</a></p>
<p id="n2402" class="pln"><a href="#n2402">2402</a></p>
<p id="n2403" class="pln"><a href="#n2403">2403</a></p>
<p id="n2404" class="pln"><a href="#n2404">2404</a></p>
<p id="n2405" class="pln"><a href="#n2405">2405</a></p>
<p id="n2406" class="pln"><a href="#n2406">2406</a></p>
<p id="n2407" class="pln"><a href="#n2407">2407</a></p>
<p id="n2408" class="pln"><a href="#n2408">2408</a></p>
<p id="n2409" class="pln"><a href="#n2409">2409</a></p>
<p id="n2410" class="pln"><a href="#n2410">2410</a></p>
<p id="n2411" class="pln"><a href="#n2411">2411</a></p>
<p id="n2412" class="pln"><a href="#n2412">2412</a></p>
<p id="n2413" class="pln"><a href="#n2413">2413</a></p>
<p id="n2414" class="pln"><a href="#n2414">2414</a></p>
<p id="n2415" class="pln"><a href="#n2415">2415</a></p>
<p id="n2416" class="pln"><a href="#n2416">2416</a></p>
<p id="n2417" class="pln"><a href="#n2417">2417</a></p>
<p id="n2418" class="pln"><a href="#n2418">2418</a></p>
<p id="n2419" class="pln"><a href="#n2419">2419</a></p>
<p id="n2420" class="pln"><a href="#n2420">2420</a></p>
<p id="n2421" class="stm mis"><a href="#n2421">2421</a></p>
<p id="n2422" class="stm mis"><a href="#n2422">2422</a></p>
<p id="n2423" class="stm mis"><a href="#n2423">2423</a></p>
<p id="n2424" class="pln"><a href="#n2424">2424</a></p>
<p id="n2425" class="pln"><a href="#n2425">2425</a></p>
<p id="n2426" class="pln"><a href="#n2426">2426</a></p>
<p id="n2427" class="stm run hide_run"><a href="#n2427">2427</a></p>
<p id="n2428" class="stm run hide_run"><a href="#n2428">2428</a></p>
<p id="n2429" class="stm run hide_run"><a href="#n2429">2429</a></p>
<p id="n2430" class="pln"><a href="#n2430">2430</a></p>
<p id="n2431" class="pln"><a href="#n2431">2431</a></p>
<p id="n2432" class="pln"><a href="#n2432">2432</a></p>
<p id="n2433" class="pln"><a href="#n2433">2433</a></p>
<p id="n2434" class="pln"><a href="#n2434">2434</a></p>
<p id="n2435" class="pln"><a href="#n2435">2435</a></p>
<p id="n2436" class="pln"><a href="#n2436">2436</a></p>
<p id="n2437" class="pln"><a href="#n2437">2437</a></p>
<p id="n2438" class="pln"><a href="#n2438">2438</a></p>
<p id="n2439" class="pln"><a href="#n2439">2439</a></p>
<p id="n2440" class="pln"><a href="#n2440">2440</a></p>
<p id="n2441" class="pln"><a href="#n2441">2441</a></p>
<p id="n2442" class="pln"><a href="#n2442">2442</a></p>
<p id="n2443" class="pln"><a href="#n2443">2443</a></p>
<p id="n2444" class="pln"><a href="#n2444">2444</a></p>
<p id="n2445" class="pln"><a href="#n2445">2445</a></p>
<p id="n2446" class="pln"><a href="#n2446">2446</a></p>
<p id="n2447" class="pln"><a href="#n2447">2447</a></p>
<p id="n2448" class="pln"><a href="#n2448">2448</a></p>
<p id="n2449" class="pln"><a href="#n2449">2449</a></p>
<p id="n2450" class="pln"><a href="#n2450">2450</a></p>
<p id="n2451" class="pln"><a href="#n2451">2451</a></p>
<p id="n2452" class="pln"><a href="#n2452">2452</a></p>
<p id="n2453" class="pln"><a href="#n2453">2453</a></p>
<p id="n2454" class="pln"><a href="#n2454">2454</a></p>
<p id="n2455" class="pln"><a href="#n2455">2455</a></p>
<p id="n2456" class="pln"><a href="#n2456">2456</a></p>
<p id="n2457" class="pln"><a href="#n2457">2457</a></p>
<p id="n2458" class="pln"><a href="#n2458">2458</a></p>
<p id="n2459" class="pln"><a href="#n2459">2459</a></p>
<p id="n2460" class="pln"><a href="#n2460">2460</a></p>
<p id="n2461" class="pln"><a href="#n2461">2461</a></p>
<p id="n2462" class="pln"><a href="#n2462">2462</a></p>
<p id="n2463" class="pln"><a href="#n2463">2463</a></p>
<p id="n2464" class="pln"><a href="#n2464">2464</a></p>
<p id="n2465" class="pln"><a href="#n2465">2465</a></p>
<p id="n2466" class="pln"><a href="#n2466">2466</a></p>
<p id="n2467" class="pln"><a href="#n2467">2467</a></p>
<p id="n2468" class="pln"><a href="#n2468">2468</a></p>
<p id="n2469" class="pln"><a href="#n2469">2469</a></p>
<p id="n2470" class="pln"><a href="#n2470">2470</a></p>
<p id="n2471" class="pln"><a href="#n2471">2471</a></p>
<p id="n2472" class="pln"><a href="#n2472">2472</a></p>
<p id="n2473" class="pln"><a href="#n2473">2473</a></p>
<p id="n2474" class="stm mis"><a href="#n2474">2474</a></p>
<p id="n2475" class="stm mis"><a href="#n2475">2475</a></p>
<p id="n2476" class="stm mis"><a href="#n2476">2476</a></p>
<p id="n2477" class="pln"><a href="#n2477">2477</a></p>
<p id="n2478" class="pln"><a href="#n2478">2478</a></p>
<p id="n2479" class="pln"><a href="#n2479">2479</a></p>
<p id="n2480" class="stm run hide_run"><a href="#n2480">2480</a></p>
<p id="n2481" class="stm run hide_run"><a href="#n2481">2481</a></p>
<p id="n2482" class="stm run hide_run"><a href="#n2482">2482</a></p>
<p id="n2483" class="pln"><a href="#n2483">2483</a></p>
<p id="n2484" class="pln"><a href="#n2484">2484</a></p>
<p id="n2485" class="pln"><a href="#n2485">2485</a></p>
<p id="n2486" class="pln"><a href="#n2486">2486</a></p>
<p id="n2487" class="pln"><a href="#n2487">2487</a></p>
<p id="n2488" class="pln"><a href="#n2488">2488</a></p>
<p id="n2489" class="pln"><a href="#n2489">2489</a></p>
<p id="n2490" class="pln"><a href="#n2490">2490</a></p>
<p id="n2491" class="pln"><a href="#n2491">2491</a></p>
<p id="n2492" class="pln"><a href="#n2492">2492</a></p>
<p id="n2493" class="pln"><a href="#n2493">2493</a></p>
<p id="n2494" class="pln"><a href="#n2494">2494</a></p>
<p id="n2495" class="pln"><a href="#n2495">2495</a></p>
<p id="n2496" class="pln"><a href="#n2496">2496</a></p>
<p id="n2497" class="pln"><a href="#n2497">2497</a></p>
<p id="n2498" class="pln"><a href="#n2498">2498</a></p>
<p id="n2499" class="pln"><a href="#n2499">2499</a></p>
<p id="n2500" class="pln"><a href="#n2500">2500</a></p>
<p id="n2501" class="pln"><a href="#n2501">2501</a></p>
<p id="n2502" class="pln"><a href="#n2502">2502</a></p>
<p id="n2503" class="pln"><a href="#n2503">2503</a></p>
<p id="n2504" class="pln"><a href="#n2504">2504</a></p>
<p id="n2505" class="pln"><a href="#n2505">2505</a></p>
<p id="n2506" class="pln"><a href="#n2506">2506</a></p>
<p id="n2507" class="pln"><a href="#n2507">2507</a></p>
<p id="n2508" class="pln"><a href="#n2508">2508</a></p>
<p id="n2509" class="stm mis"><a href="#n2509">2509</a></p>
<p id="n2510" class="stm mis"><a href="#n2510">2510</a></p>
<p id="n2511" class="stm mis"><a href="#n2511">2511</a></p>
<p id="n2512" class="stm mis"><a href="#n2512">2512</a></p>
<p id="n2513" class="stm mis"><a href="#n2513">2513</a></p>
<p id="n2514" class="stm mis"><a href="#n2514">2514</a></p>
<p id="n2515" class="stm mis"><a href="#n2515">2515</a></p>
<p id="n2516" class="stm mis"><a href="#n2516">2516</a></p>
<p id="n2517" class="stm mis"><a href="#n2517">2517</a></p>
<p id="n2518" class="stm mis"><a href="#n2518">2518</a></p>
<p id="n2519" class="pln"><a href="#n2519">2519</a></p>
<p id="n2520" class="stm mis"><a href="#n2520">2520</a></p>
<p id="n2521" class="pln"><a href="#n2521">2521</a></p>
<p id="n2522" class="pln"><a href="#n2522">2522</a></p>
<p id="n2523" class="pln"><a href="#n2523">2523</a></p>
<p id="n2524" class="stm run hide_run"><a href="#n2524">2524</a></p>
<p id="n2525" class="pln"><a href="#n2525">2525</a></p>
<p id="n2526" class="stm mis"><a href="#n2526">2526</a></p>
<p id="n2527" class="pln"><a href="#n2527">2527</a></p>
<p id="n2528" class="pln"><a href="#n2528">2528</a></p>
<p id="n2529" class="pln"><a href="#n2529">2529</a></p>
<p id="n2530" class="pln"><a href="#n2530">2530</a></p>
<p id="n2531" class="pln"><a href="#n2531">2531</a></p>
<p id="n2532" class="stm run hide_run"><a href="#n2532">2532</a></p>
<p id="n2533" class="pln"><a href="#n2533">2533</a></p>
<p id="n2534" class="pln"><a href="#n2534">2534</a></p>
<p id="n2535" class="pln"><a href="#n2535">2535</a></p>
<p id="n2536" class="pln"><a href="#n2536">2536</a></p>
<p id="n2537" class="pln"><a href="#n2537">2537</a></p>
<p id="n2538" class="pln"><a href="#n2538">2538</a></p>
<p id="n2539" class="pln"><a href="#n2539">2539</a></p>
<p id="n2540" class="pln"><a href="#n2540">2540</a></p>
<p id="n2541" class="pln"><a href="#n2541">2541</a></p>
<p id="n2542" class="pln"><a href="#n2542">2542</a></p>
<p id="n2543" class="stm mis"><a href="#n2543">2543</a></p>
<p id="n2544" class="stm mis"><a href="#n2544">2544</a></p>
<p id="n2545" class="stm mis"><a href="#n2545">2545</a></p>
<p id="n2546" class="stm mis"><a href="#n2546">2546</a></p>
<p id="n2547" class="stm mis"><a href="#n2547">2547</a></p>
<p id="n2548" class="pln"><a href="#n2548">2548</a></p>
<p id="n2549" class="stm mis"><a href="#n2549">2549</a></p>
<p id="n2550" class="stm mis"><a href="#n2550">2550</a></p>
<p id="n2551" class="pln"><a href="#n2551">2551</a></p>
<p id="n2552" class="stm mis"><a href="#n2552">2552</a></p>
<p id="n2553" class="stm mis"><a href="#n2553">2553</a></p>
<p id="n2554" class="stm mis"><a href="#n2554">2554</a></p>
<p id="n2555" class="stm mis"><a href="#n2555">2555</a></p>
<p id="n2556" class="pln"><a href="#n2556">2556</a></p>
<p id="n2557" class="pln"><a href="#n2557">2557</a></p>
<p id="n2558" class="pln"><a href="#n2558">2558</a></p>
<p id="n2559" class="pln"><a href="#n2559">2559</a></p>
<p id="n2560" class="pln"><a href="#n2560">2560</a></p>
<p id="n2561" class="pln"><a href="#n2561">2561</a></p>
<p id="n2562" class="pln"><a href="#n2562">2562</a></p>
<p id="n2563" class="pln"><a href="#n2563">2563</a></p>
<p id="n2564" class="pln"><a href="#n2564">2564</a></p>
<p id="n2565" class="pln"><a href="#n2565">2565</a></p>
<p id="n2566" class="stm run hide_run"><a href="#n2566">2566</a></p>
<p id="n2567" class="pln"><a href="#n2567">2567</a></p>
<p id="n2568" class="pln"><a href="#n2568">2568</a></p>
<p id="n2569" class="pln"><a href="#n2569">2569</a></p>
<p id="n2570" class="pln"><a href="#n2570">2570</a></p>
<p id="n2571" class="stm mis"><a href="#n2571">2571</a></p>
<p id="n2572" class="stm mis"><a href="#n2572">2572</a></p>
<p id="n2573" class="stm mis"><a href="#n2573">2573</a></p>
<p id="n2574" class="pln"><a href="#n2574">2574</a></p>
<p id="n2575" class="stm mis"><a href="#n2575">2575</a></p>
<p id="n2576" class="stm mis"><a href="#n2576">2576</a></p>
<p id="n2577" class="stm mis"><a href="#n2577">2577</a></p>
<p id="n2578" class="stm mis"><a href="#n2578">2578</a></p>
<p id="n2579" class="pln"><a href="#n2579">2579</a></p>
<p id="n2580" class="pln"><a href="#n2580">2580</a></p>
<p id="n2581" class="stm run hide_run"><a href="#n2581">2581</a></p>
<p id="n2582" class="stm run hide_run"><a href="#n2582">2582</a></p>
<p id="n2583" class="stm run hide_run"><a href="#n2583">2583</a></p>
<p id="n2584" class="pln"><a href="#n2584">2584</a></p>
<p id="n2585" class="pln"><a href="#n2585">2585</a></p>
<p id="n2586" class="pln"><a href="#n2586">2586</a></p>
<p id="n2587" class="pln"><a href="#n2587">2587</a></p>
<p id="n2588" class="pln"><a href="#n2588">2588</a></p>
<p id="n2589" class="pln"><a href="#n2589">2589</a></p>
<p id="n2590" class="pln"><a href="#n2590">2590</a></p>
<p id="n2591" class="pln"><a href="#n2591">2591</a></p>
<p id="n2592" class="pln"><a href="#n2592">2592</a></p>
<p id="n2593" class="pln"><a href="#n2593">2593</a></p>
<p id="n2594" class="pln"><a href="#n2594">2594</a></p>
<p id="n2595" class="pln"><a href="#n2595">2595</a></p>
<p id="n2596" class="pln"><a href="#n2596">2596</a></p>
<p id="n2597" class="pln"><a href="#n2597">2597</a></p>
<p id="n2598" class="pln"><a href="#n2598">2598</a></p>
<p id="n2599" class="pln"><a href="#n2599">2599</a></p>
<p id="n2600" class="pln"><a href="#n2600">2600</a></p>
<p id="n2601" class="pln"><a href="#n2601">2601</a></p>
<p id="n2602" class="pln"><a href="#n2602">2602</a></p>
<p id="n2603" class="pln"><a href="#n2603">2603</a></p>
<p id="n2604" class="pln"><a href="#n2604">2604</a></p>
<p id="n2605" class="pln"><a href="#n2605">2605</a></p>
<p id="n2606" class="pln"><a href="#n2606">2606</a></p>
<p id="n2607" class="pln"><a href="#n2607">2607</a></p>
<p id="n2608" class="pln"><a href="#n2608">2608</a></p>
<p id="n2609" class="pln"><a href="#n2609">2609</a></p>
<p id="n2610" class="pln"><a href="#n2610">2610</a></p>
<p id="n2611" class="pln"><a href="#n2611">2611</a></p>
<p id="n2612" class="pln"><a href="#n2612">2612</a></p>
<p id="n2613" class="pln"><a href="#n2613">2613</a></p>
<p id="n2614" class="pln"><a href="#n2614">2614</a></p>
<p id="n2615" class="pln"><a href="#n2615">2615</a></p>
<p id="n2616" class="stm mis"><a href="#n2616">2616</a></p>
<p id="n2617" class="stm mis"><a href="#n2617">2617</a></p>
<p id="n2618" class="stm mis"><a href="#n2618">2618</a></p>
<p id="n2619" class="stm mis"><a href="#n2619">2619</a></p>
<p id="n2620" class="stm mis"><a href="#n2620">2620</a></p>
<p id="n2621" class="stm mis"><a href="#n2621">2621</a></p>
<p id="n2622" class="pln"><a href="#n2622">2622</a></p>
<p id="n2623" class="pln"><a href="#n2623">2623</a></p>
<p id="n2624" class="stm run hide_run"><a href="#n2624">2624</a></p>
<p id="n2625" class="stm run hide_run"><a href="#n2625">2625</a></p>
<p id="n2626" class="stm run hide_run"><a href="#n2626">2626</a></p>
<p id="n2627" class="pln"><a href="#n2627">2627</a></p>
<p id="n2628" class="pln"><a href="#n2628">2628</a></p>
<p id="n2629" class="pln"><a href="#n2629">2629</a></p>
<p id="n2630" class="pln"><a href="#n2630">2630</a></p>
<p id="n2631" class="pln"><a href="#n2631">2631</a></p>
<p id="n2632" class="pln"><a href="#n2632">2632</a></p>
<p id="n2633" class="pln"><a href="#n2633">2633</a></p>
<p id="n2634" class="pln"><a href="#n2634">2634</a></p>
<p id="n2635" class="pln"><a href="#n2635">2635</a></p>
<p id="n2636" class="pln"><a href="#n2636">2636</a></p>
<p id="n2637" class="pln"><a href="#n2637">2637</a></p>
<p id="n2638" class="pln"><a href="#n2638">2638</a></p>
<p id="n2639" class="pln"><a href="#n2639">2639</a></p>
<p id="n2640" class="pln"><a href="#n2640">2640</a></p>
<p id="n2641" class="pln"><a href="#n2641">2641</a></p>
<p id="n2642" class="pln"><a href="#n2642">2642</a></p>
<p id="n2643" class="pln"><a href="#n2643">2643</a></p>
<p id="n2644" class="pln"><a href="#n2644">2644</a></p>
<p id="n2645" class="pln"><a href="#n2645">2645</a></p>
<p id="n2646" class="pln"><a href="#n2646">2646</a></p>
<p id="n2647" class="pln"><a href="#n2647">2647</a></p>
<p id="n2648" class="pln"><a href="#n2648">2648</a></p>
<p id="n2649" class="pln"><a href="#n2649">2649</a></p>
<p id="n2650" class="pln"><a href="#n2650">2650</a></p>
<p id="n2651" class="pln"><a href="#n2651">2651</a></p>
<p id="n2652" class="pln"><a href="#n2652">2652</a></p>
<p id="n2653" class="pln"><a href="#n2653">2653</a></p>
<p id="n2654" class="pln"><a href="#n2654">2654</a></p>
<p id="n2655" class="pln"><a href="#n2655">2655</a></p>
<p id="n2656" class="pln"><a href="#n2656">2656</a></p>
<p id="n2657" class="pln"><a href="#n2657">2657</a></p>
<p id="n2658" class="pln"><a href="#n2658">2658</a></p>
<p id="n2659" class="pln"><a href="#n2659">2659</a></p>
<p id="n2660" class="pln"><a href="#n2660">2660</a></p>
<p id="n2661" class="pln"><a href="#n2661">2661</a></p>
<p id="n2662" class="stm mis"><a href="#n2662">2662</a></p>
<p id="n2663" class="stm mis"><a href="#n2663">2663</a></p>
<p id="n2664" class="stm mis"><a href="#n2664">2664</a></p>
<p id="n2665" class="stm mis"><a href="#n2665">2665</a></p>
<p id="n2666" class="stm mis"><a href="#n2666">2666</a></p>
<p id="n2667" class="stm mis"><a href="#n2667">2667</a></p>
<p id="n2668" class="pln"><a href="#n2668">2668</a></p>
<p id="n2669" class="pln"><a href="#n2669">2669</a></p>
<p id="n2670" class="stm run hide_run"><a href="#n2670">2670</a></p>
<p id="n2671" class="stm run hide_run"><a href="#n2671">2671</a></p>
<p id="n2672" class="stm run hide_run"><a href="#n2672">2672</a></p>
<p id="n2673" class="pln"><a href="#n2673">2673</a></p>
<p id="n2674" class="pln"><a href="#n2674">2674</a></p>
<p id="n2675" class="pln"><a href="#n2675">2675</a></p>
<p id="n2676" class="pln"><a href="#n2676">2676</a></p>
<p id="n2677" class="pln"><a href="#n2677">2677</a></p>
<p id="n2678" class="pln"><a href="#n2678">2678</a></p>
<p id="n2679" class="pln"><a href="#n2679">2679</a></p>
<p id="n2680" class="pln"><a href="#n2680">2680</a></p>
<p id="n2681" class="pln"><a href="#n2681">2681</a></p>
<p id="n2682" class="pln"><a href="#n2682">2682</a></p>
<p id="n2683" class="pln"><a href="#n2683">2683</a></p>
<p id="n2684" class="pln"><a href="#n2684">2684</a></p>
<p id="n2685" class="pln"><a href="#n2685">2685</a></p>
<p id="n2686" class="pln"><a href="#n2686">2686</a></p>
<p id="n2687" class="pln"><a href="#n2687">2687</a></p>
<p id="n2688" class="pln"><a href="#n2688">2688</a></p>
<p id="n2689" class="pln"><a href="#n2689">2689</a></p>
<p id="n2690" class="pln"><a href="#n2690">2690</a></p>
<p id="n2691" class="pln"><a href="#n2691">2691</a></p>
<p id="n2692" class="pln"><a href="#n2692">2692</a></p>
<p id="n2693" class="pln"><a href="#n2693">2693</a></p>
<p id="n2694" class="pln"><a href="#n2694">2694</a></p>
<p id="n2695" class="pln"><a href="#n2695">2695</a></p>
<p id="n2696" class="pln"><a href="#n2696">2696</a></p>
<p id="n2697" class="pln"><a href="#n2697">2697</a></p>
<p id="n2698" class="pln"><a href="#n2698">2698</a></p>
<p id="n2699" class="pln"><a href="#n2699">2699</a></p>
<p id="n2700" class="pln"><a href="#n2700">2700</a></p>
<p id="n2701" class="pln"><a href="#n2701">2701</a></p>
<p id="n2702" class="pln"><a href="#n2702">2702</a></p>
<p id="n2703" class="pln"><a href="#n2703">2703</a></p>
<p id="n2704" class="pln"><a href="#n2704">2704</a></p>
<p id="n2705" class="pln"><a href="#n2705">2705</a></p>
<p id="n2706" class="pln"><a href="#n2706">2706</a></p>
<p id="n2707" class="pln"><a href="#n2707">2707</a></p>
<p id="n2708" class="pln"><a href="#n2708">2708</a></p>
<p id="n2709" class="pln"><a href="#n2709">2709</a></p>
<p id="n2710" class="pln"><a href="#n2710">2710</a></p>
<p id="n2711" class="pln"><a href="#n2711">2711</a></p>
<p id="n2712" class="pln"><a href="#n2712">2712</a></p>
<p id="n2713" class="pln"><a href="#n2713">2713</a></p>
<p id="n2714" class="pln"><a href="#n2714">2714</a></p>
<p id="n2715" class="pln"><a href="#n2715">2715</a></p>
<p id="n2716" class="pln"><a href="#n2716">2716</a></p>
<p id="n2717" class="pln"><a href="#n2717">2717</a></p>
<p id="n2718" class="pln"><a href="#n2718">2718</a></p>
<p id="n2719" class="pln"><a href="#n2719">2719</a></p>
<p id="n2720" class="pln"><a href="#n2720">2720</a></p>
<p id="n2721" class="pln"><a href="#n2721">2721</a></p>
<p id="n2722" class="pln"><a href="#n2722">2722</a></p>
<p id="n2723" class="pln"><a href="#n2723">2723</a></p>
<p id="n2724" class="pln"><a href="#n2724">2724</a></p>
<p id="n2725" class="pln"><a href="#n2725">2725</a></p>
<p id="n2726" class="pln"><a href="#n2726">2726</a></p>
<p id="n2727" class="pln"><a href="#n2727">2727</a></p>
<p id="n2728" class="pln"><a href="#n2728">2728</a></p>
<p id="n2729" class="pln"><a href="#n2729">2729</a></p>
<p id="n2730" class="pln"><a href="#n2730">2730</a></p>
<p id="n2731" class="pln"><a href="#n2731">2731</a></p>
<p id="n2732" class="stm mis"><a href="#n2732">2732</a></p>
<p id="n2733" class="stm mis"><a href="#n2733">2733</a></p>
<p id="n2734" class="pln"><a href="#n2734">2734</a></p>
<p id="n2735" class="pln"><a href="#n2735">2735</a></p>
<p id="n2736" class="pln"><a href="#n2736">2736</a></p>
<p id="n2737" class="pln"><a href="#n2737">2737</a></p>
<p id="n2738" class="pln"><a href="#n2738">2738</a></p>
<p id="n2739" class="pln"><a href="#n2739">2739</a></p>
<p id="n2740" class="stm mis"><a href="#n2740">2740</a></p>
<p id="n2741" class="pln"><a href="#n2741">2741</a></p>
<p id="n2742" class="pln"><a href="#n2742">2742</a></p>
<p id="n2743" class="pln"><a href="#n2743">2743</a></p>
<p id="n2744" class="stm run hide_run"><a href="#n2744">2744</a></p>
<p id="n2745" class="stm run hide_run"><a href="#n2745">2745</a></p>
<p id="n2746" class="stm run hide_run"><a href="#n2746">2746</a></p>
<p id="n2747" class="pln"><a href="#n2747">2747</a></p>
<p id="n2748" class="pln"><a href="#n2748">2748</a></p>
<p id="n2749" class="pln"><a href="#n2749">2749</a></p>
<p id="n2750" class="pln"><a href="#n2750">2750</a></p>
<p id="n2751" class="pln"><a href="#n2751">2751</a></p>
<p id="n2752" class="pln"><a href="#n2752">2752</a></p>
<p id="n2753" class="pln"><a href="#n2753">2753</a></p>
<p id="n2754" class="pln"><a href="#n2754">2754</a></p>
<p id="n2755" class="pln"><a href="#n2755">2755</a></p>
<p id="n2756" class="pln"><a href="#n2756">2756</a></p>
<p id="n2757" class="pln"><a href="#n2757">2757</a></p>
<p id="n2758" class="pln"><a href="#n2758">2758</a></p>
<p id="n2759" class="pln"><a href="#n2759">2759</a></p>
<p id="n2760" class="pln"><a href="#n2760">2760</a></p>
<p id="n2761" class="pln"><a href="#n2761">2761</a></p>
<p id="n2762" class="pln"><a href="#n2762">2762</a></p>
<p id="n2763" class="pln"><a href="#n2763">2763</a></p>
<p id="n2764" class="pln"><a href="#n2764">2764</a></p>
<p id="n2765" class="pln"><a href="#n2765">2765</a></p>
<p id="n2766" class="pln"><a href="#n2766">2766</a></p>
<p id="n2767" class="pln"><a href="#n2767">2767</a></p>
<p id="n2768" class="pln"><a href="#n2768">2768</a></p>
<p id="n2769" class="pln"><a href="#n2769">2769</a></p>
<p id="n2770" class="pln"><a href="#n2770">2770</a></p>
<p id="n2771" class="pln"><a href="#n2771">2771</a></p>
<p id="n2772" class="pln"><a href="#n2772">2772</a></p>
<p id="n2773" class="pln"><a href="#n2773">2773</a></p>
<p id="n2774" class="pln"><a href="#n2774">2774</a></p>
<p id="n2775" class="pln"><a href="#n2775">2775</a></p>
<p id="n2776" class="pln"><a href="#n2776">2776</a></p>
<p id="n2777" class="pln"><a href="#n2777">2777</a></p>
<p id="n2778" class="stm mis"><a href="#n2778">2778</a></p>
<p id="n2779" class="stm mis"><a href="#n2779">2779</a></p>
<p id="n2780" class="pln"><a href="#n2780">2780</a></p>
<p id="n2781" class="pln"><a href="#n2781">2781</a></p>
<p id="n2782" class="pln"><a href="#n2782">2782</a></p>
<p id="n2783" class="pln"><a href="#n2783">2783</a></p>
<p id="n2784" class="pln"><a href="#n2784">2784</a></p>
<p id="n2785" class="pln"><a href="#n2785">2785</a></p>
<p id="n2786" class="stm mis"><a href="#n2786">2786</a></p>
<p id="n2787" class="pln"><a href="#n2787">2787</a></p>
<p id="n2788" class="pln"><a href="#n2788">2788</a></p>
<p id="n2789" class="pln"><a href="#n2789">2789</a></p>
<p id="n2790" class="stm run hide_run"><a href="#n2790">2790</a></p>
<p id="n2791" class="stm run hide_run"><a href="#n2791">2791</a></p>
<p id="n2792" class="stm run hide_run"><a href="#n2792">2792</a></p>
<p id="n2793" class="pln"><a href="#n2793">2793</a></p>
<p id="n2794" class="pln"><a href="#n2794">2794</a></p>
<p id="n2795" class="pln"><a href="#n2795">2795</a></p>
<p id="n2796" class="pln"><a href="#n2796">2796</a></p>
<p id="n2797" class="pln"><a href="#n2797">2797</a></p>
<p id="n2798" class="pln"><a href="#n2798">2798</a></p>
<p id="n2799" class="pln"><a href="#n2799">2799</a></p>
<p id="n2800" class="pln"><a href="#n2800">2800</a></p>
<p id="n2801" class="pln"><a href="#n2801">2801</a></p>
<p id="n2802" class="pln"><a href="#n2802">2802</a></p>
<p id="n2803" class="pln"><a href="#n2803">2803</a></p>
<p id="n2804" class="pln"><a href="#n2804">2804</a></p>
<p id="n2805" class="pln"><a href="#n2805">2805</a></p>
<p id="n2806" class="pln"><a href="#n2806">2806</a></p>
<p id="n2807" class="pln"><a href="#n2807">2807</a></p>
<p id="n2808" class="pln"><a href="#n2808">2808</a></p>
<p id="n2809" class="pln"><a href="#n2809">2809</a></p>
<p id="n2810" class="pln"><a href="#n2810">2810</a></p>
<p id="n2811" class="pln"><a href="#n2811">2811</a></p>
<p id="n2812" class="pln"><a href="#n2812">2812</a></p>
<p id="n2813" class="pln"><a href="#n2813">2813</a></p>
<p id="n2814" class="pln"><a href="#n2814">2814</a></p>
<p id="n2815" class="pln"><a href="#n2815">2815</a></p>
<p id="n2816" class="stm mis"><a href="#n2816">2816</a></p>
<p id="n2817" class="stm mis"><a href="#n2817">2817</a></p>
<p id="n2818" class="pln"><a href="#n2818">2818</a></p>
<p id="n2819" class="pln"><a href="#n2819">2819</a></p>
<p id="n2820" class="pln"><a href="#n2820">2820</a></p>
<p id="n2821" class="pln"><a href="#n2821">2821</a></p>
<p id="n2822" class="pln"><a href="#n2822">2822</a></p>
<p id="n2823" class="pln"><a href="#n2823">2823</a></p>
<p id="n2824" class="stm mis"><a href="#n2824">2824</a></p>
<p id="n2825" class="pln"><a href="#n2825">2825</a></p>
<p id="n2826" class="pln"><a href="#n2826">2826</a></p>
<p id="n2827" class="pln"><a href="#n2827">2827</a></p>
<p id="n2828" class="stm run hide_run"><a href="#n2828">2828</a></p>
<p id="n2829" class="stm run hide_run"><a href="#n2829">2829</a></p>
<p id="n2830" class="pln"><a href="#n2830">2830</a></p>
<p id="n2831" class="pln"><a href="#n2831">2831</a></p>
<p id="n2832" class="pln"><a href="#n2832">2832</a></p>
<p id="n2833" class="pln"><a href="#n2833">2833</a></p>
<p id="n2834" class="pln"><a href="#n2834">2834</a></p>
<p id="n2835" class="pln"><a href="#n2835">2835</a></p>
<p id="n2836" class="pln"><a href="#n2836">2836</a></p>
<p id="n2837" class="pln"><a href="#n2837">2837</a></p>
<p id="n2838" class="pln"><a href="#n2838">2838</a></p>
<p id="n2839" class="pln"><a href="#n2839">2839</a></p>
<p id="n2840" class="pln"><a href="#n2840">2840</a></p>
<p id="n2841" class="pln"><a href="#n2841">2841</a></p>
<p id="n2842" class="pln"><a href="#n2842">2842</a></p>
<p id="n2843" class="pln"><a href="#n2843">2843</a></p>
<p id="n2844" class="pln"><a href="#n2844">2844</a></p>
<p id="n2845" class="pln"><a href="#n2845">2845</a></p>
<p id="n2846" class="pln"><a href="#n2846">2846</a></p>
<p id="n2847" class="pln"><a href="#n2847">2847</a></p>
<p id="n2848" class="pln"><a href="#n2848">2848</a></p>
<p id="n2849" class="pln"><a href="#n2849">2849</a></p>
<p id="n2850" class="pln"><a href="#n2850">2850</a></p>
<p id="n2851" class="pln"><a href="#n2851">2851</a></p>
<p id="n2852" class="pln"><a href="#n2852">2852</a></p>
<p id="n2853" class="pln"><a href="#n2853">2853</a></p>
<p id="n2854" class="pln"><a href="#n2854">2854</a></p>
<p id="n2855" class="pln"><a href="#n2855">2855</a></p>
<p id="n2856" class="pln"><a href="#n2856">2856</a></p>
<p id="n2857" class="pln"><a href="#n2857">2857</a></p>
<p id="n2858" class="pln"><a href="#n2858">2858</a></p>
<p id="n2859" class="pln"><a href="#n2859">2859</a></p>
<p id="n2860" class="pln"><a href="#n2860">2860</a></p>
<p id="n2861" class="pln"><a href="#n2861">2861</a></p>
<p id="n2862" class="pln"><a href="#n2862">2862</a></p>
<p id="n2863" class="pln"><a href="#n2863">2863</a></p>
<p id="n2864" class="pln"><a href="#n2864">2864</a></p>
<p id="n2865" class="pln"><a href="#n2865">2865</a></p>
<p id="n2866" class="pln"><a href="#n2866">2866</a></p>
<p id="n2867" class="pln"><a href="#n2867">2867</a></p>
<p id="n2868" class="pln"><a href="#n2868">2868</a></p>
<p id="n2869" class="pln"><a href="#n2869">2869</a></p>
<p id="n2870" class="pln"><a href="#n2870">2870</a></p>
<p id="n2871" class="pln"><a href="#n2871">2871</a></p>
<p id="n2872" class="pln"><a href="#n2872">2872</a></p>
<p id="n2873" class="pln"><a href="#n2873">2873</a></p>
<p id="n2874" class="pln"><a href="#n2874">2874</a></p>
<p id="n2875" class="pln"><a href="#n2875">2875</a></p>
<p id="n2876" class="pln"><a href="#n2876">2876</a></p>
<p id="n2877" class="pln"><a href="#n2877">2877</a></p>
<p id="n2878" class="stm mis"><a href="#n2878">2878</a></p>
<p id="n2879" class="pln"><a href="#n2879">2879</a></p>
<p id="n2880" class="pln"><a href="#n2880">2880</a></p>
<p id="n2881" class="pln"><a href="#n2881">2881</a></p>
<p id="n2882" class="pln"><a href="#n2882">2882</a></p>
<p id="n2883" class="pln"><a href="#n2883">2883</a></p>
<p id="n2884" class="pln"><a href="#n2884">2884</a></p>
<p id="n2885" class="pln"><a href="#n2885">2885</a></p>
<p id="n2886" class="pln"><a href="#n2886">2886</a></p>
<p id="n2887" class="pln"><a href="#n2887">2887</a></p>
<p id="n2888" class="pln"><a href="#n2888">2888</a></p>
<p id="n2889" class="pln"><a href="#n2889">2889</a></p>
<p id="n2890" class="pln"><a href="#n2890">2890</a></p>
<p id="n2891" class="pln"><a href="#n2891">2891</a></p>
<p id="n2892" class="pln"><a href="#n2892">2892</a></p>
<p id="n2893" class="pln"><a href="#n2893">2893</a></p>
<p id="n2894" class="pln"><a href="#n2894">2894</a></p>
<p id="n2895" class="pln"><a href="#n2895">2895</a></p>
<p id="n2896" class="pln"><a href="#n2896">2896</a></p>
<p id="n2897" class="pln"><a href="#n2897">2897</a></p>
<p id="n2898" class="pln"><a href="#n2898">2898</a></p>
<p id="n2899" class="pln"><a href="#n2899">2899</a></p>
<p id="n2900" class="pln"><a href="#n2900">2900</a></p>
<p id="n2901" class="pln"><a href="#n2901">2901</a></p>
<p id="n2902" class="stm mis"><a href="#n2902">2902</a></p>
<p id="n2903" class="stm mis"><a href="#n2903">2903</a></p>
<p id="n2904" class="stm mis"><a href="#n2904">2904</a></p>
<p id="n2905" class="stm mis"><a href="#n2905">2905</a></p>
<p id="n2906" class="stm mis"><a href="#n2906">2906</a></p>
<p id="n2907" class="stm mis"><a href="#n2907">2907</a></p>
<p id="n2908" class="stm mis"><a href="#n2908">2908</a></p>
<p id="n2909" class="stm mis"><a href="#n2909">2909</a></p>
<p id="n2910" class="stm mis"><a href="#n2910">2910</a></p>
<p id="n2911" class="stm mis"><a href="#n2911">2911</a></p>
<p id="n2912" class="stm mis"><a href="#n2912">2912</a></p>
<p id="n2913" class="pln"><a href="#n2913">2913</a></p>
<p id="n2914" class="stm mis"><a href="#n2914">2914</a></p>
<p id="n2915" class="stm mis"><a href="#n2915">2915</a></p>
<p id="n2916" class="stm mis"><a href="#n2916">2916</a></p>
<p id="n2917" class="stm mis"><a href="#n2917">2917</a></p>
<p id="n2918" class="stm mis"><a href="#n2918">2918</a></p>
<p id="n2919" class="stm mis"><a href="#n2919">2919</a></p>
<p id="n2920" class="stm mis"><a href="#n2920">2920</a></p>
<p id="n2921" class="stm mis"><a href="#n2921">2921</a></p>
<p id="n2922" class="stm mis"><a href="#n2922">2922</a></p>
<p id="n2923" class="stm mis"><a href="#n2923">2923</a></p>
<p id="n2924" class="pln"><a href="#n2924">2924</a></p>
<p id="n2925" class="stm mis"><a href="#n2925">2925</a></p>
<p id="n2926" class="stm mis"><a href="#n2926">2926</a></p>
<p id="n2927" class="stm mis"><a href="#n2927">2927</a></p>
<p id="n2928" class="stm mis"><a href="#n2928">2928</a></p>
<p id="n2929" class="stm mis"><a href="#n2929">2929</a></p>
<p id="n2930" class="stm mis"><a href="#n2930">2930</a></p>
<p id="n2931" class="stm mis"><a href="#n2931">2931</a></p>
<p id="n2932" class="stm mis"><a href="#n2932">2932</a></p>
<p id="n2933" class="stm mis"><a href="#n2933">2933</a></p>
<p id="n2934" class="stm mis"><a href="#n2934">2934</a></p>
<p id="n2935" class="stm mis"><a href="#n2935">2935</a></p>
<p id="n2936" class="stm mis"><a href="#n2936">2936</a></p>
<p id="n2937" class="stm mis"><a href="#n2937">2937</a></p>
<p id="n2938" class="pln"><a href="#n2938">2938</a></p>
<p id="n2939" class="stm mis"><a href="#n2939">2939</a></p>
<p id="n2940" class="stm mis"><a href="#n2940">2940</a></p>
<p id="n2941" class="stm mis"><a href="#n2941">2941</a></p>
<p id="n2942" class="stm mis"><a href="#n2942">2942</a></p>
<p id="n2943" class="pln"><a href="#n2943">2943</a></p>
<p id="n2944" class="stm mis"><a href="#n2944">2944</a></p>
<p id="n2945" class="pln"><a href="#n2945">2945</a></p>
<p id="n2946" class="stm mis"><a href="#n2946">2946</a></p>
<p id="n2947" class="stm mis"><a href="#n2947">2947</a></p>
<p id="n2948" class="stm mis"><a href="#n2948">2948</a></p>
<p id="n2949" class="stm mis"><a href="#n2949">2949</a></p>
<p id="n2950" class="stm mis"><a href="#n2950">2950</a></p>
<p id="n2951" class="stm mis"><a href="#n2951">2951</a></p>
<p id="n2952" class="stm mis"><a href="#n2952">2952</a></p>
<p id="n2953" class="pln"><a href="#n2953">2953</a></p>
<p id="n2954" class="stm mis"><a href="#n2954">2954</a></p>
<p id="n2955" class="pln"><a href="#n2955">2955</a></p>
<p id="n2956" class="pln"><a href="#n2956">2956</a></p>
<p id="n2957" class="stm mis"><a href="#n2957">2957</a></p>
<p id="n2958" class="stm mis"><a href="#n2958">2958</a></p>
<p id="n2959" class="pln"><a href="#n2959">2959</a></p>
<p id="n2960" class="stm mis"><a href="#n2960">2960</a></p>
<p id="n2961" class="stm mis"><a href="#n2961">2961</a></p>
<p id="n2962" class="stm mis"><a href="#n2962">2962</a></p>
<p id="n2963" class="stm mis"><a href="#n2963">2963</a></p>
<p id="n2964" class="stm mis"><a href="#n2964">2964</a></p>
<p id="n2965" class="stm mis"><a href="#n2965">2965</a></p>
<p id="n2966" class="pln"><a href="#n2966">2966</a></p>
<p id="n2967" class="stm mis"><a href="#n2967">2967</a></p>
<p id="n2968" class="stm mis"><a href="#n2968">2968</a></p>
<p id="n2969" class="stm mis"><a href="#n2969">2969</a></p>
<p id="n2970" class="stm mis"><a href="#n2970">2970</a></p>
<p id="n2971" class="pln"><a href="#n2971">2971</a></p>
<p id="n2972" class="pln"><a href="#n2972">2972</a></p>
<p id="n2973" class="stm mis"><a href="#n2973">2973</a></p>
<p id="n2974" class="pln"><a href="#n2974">2974</a></p>
<p id="n2975" class="stm mis"><a href="#n2975">2975</a></p>
<p id="n2976" class="stm mis"><a href="#n2976">2976</a></p>
<p id="n2977" class="pln"><a href="#n2977">2977</a></p>
<p id="n2978" class="stm mis"><a href="#n2978">2978</a></p>
<p id="n2979" class="stm mis"><a href="#n2979">2979</a></p>
<p id="n2980" class="stm mis"><a href="#n2980">2980</a></p>
<p id="n2981" class="stm mis"><a href="#n2981">2981</a></p>
<p id="n2982" class="stm mis"><a href="#n2982">2982</a></p>
<p id="n2983" class="stm mis"><a href="#n2983">2983</a></p>
<p id="n2984" class="pln"><a href="#n2984">2984</a></p>
<p id="n2985" class="stm mis"><a href="#n2985">2985</a></p>
<p id="n2986" class="stm mis"><a href="#n2986">2986</a></p>
<p id="n2987" class="stm mis"><a href="#n2987">2987</a></p>
<p id="n2988" class="pln"><a href="#n2988">2988</a></p>
<p id="n2989" class="stm mis"><a href="#n2989">2989</a></p>
<p id="n2990" class="stm mis"><a href="#n2990">2990</a></p>
<p id="n2991" class="stm mis"><a href="#n2991">2991</a></p>
<p id="n2992" class="pln"><a href="#n2992">2992</a></p>
<p id="n2993" class="stm mis"><a href="#n2993">2993</a></p>
<p id="n2994" class="stm mis"><a href="#n2994">2994</a></p>
<p id="n2995" class="stm mis"><a href="#n2995">2995</a></p>
<p id="n2996" class="pln"><a href="#n2996">2996</a></p>
<p id="n2997" class="pln"><a href="#n2997">2997</a></p>
<p id="n2998" class="stm run hide_run"><a href="#n2998">2998</a></p>
<p id="n2999" class="stm run hide_run"><a href="#n2999">2999</a></p>
<p id="n3000" class="pln"><a href="#n3000">3000</a></p>
<p id="n3001" class="pln"><a href="#n3001">3001</a></p>
<p id="n3002" class="pln"><a href="#n3002">3002</a></p>
<p id="n3003" class="pln"><a href="#n3003">3003</a></p>
<p id="n3004" class="pln"><a href="#n3004">3004</a></p>
<p id="n3005" class="pln"><a href="#n3005">3005</a></p>
<p id="n3006" class="pln"><a href="#n3006">3006</a></p>
<p id="n3007" class="pln"><a href="#n3007">3007</a></p>
<p id="n3008" class="pln"><a href="#n3008">3008</a></p>
<p id="n3009" class="pln"><a href="#n3009">3009</a></p>
<p id="n3010" class="pln"><a href="#n3010">3010</a></p>
<p id="n3011" class="pln"><a href="#n3011">3011</a></p>
<p id="n3012" class="pln"><a href="#n3012">3012</a></p>
<p id="n3013" class="pln"><a href="#n3013">3013</a></p>
<p id="n3014" class="pln"><a href="#n3014">3014</a></p>
<p id="n3015" class="pln"><a href="#n3015">3015</a></p>
<p id="n3016" class="pln"><a href="#n3016">3016</a></p>
<p id="n3017" class="pln"><a href="#n3017">3017</a></p>
<p id="n3018" class="pln"><a href="#n3018">3018</a></p>
<p id="n3019" class="pln"><a href="#n3019">3019</a></p>
<p id="n3020" class="pln"><a href="#n3020">3020</a></p>
<p id="n3021" class="pln"><a href="#n3021">3021</a></p>
<p id="n3022" class="pln"><a href="#n3022">3022</a></p>
<p id="n3023" class="pln"><a href="#n3023">3023</a></p>
<p id="n3024" class="pln"><a href="#n3024">3024</a></p>
<p id="n3025" class="pln"><a href="#n3025">3025</a></p>
<p id="n3026" class="stm mis"><a href="#n3026">3026</a></p>
<p id="n3027" class="stm mis"><a href="#n3027">3027</a></p>
<p id="n3028" class="stm mis"><a href="#n3028">3028</a></p>
<p id="n3029" class="stm mis"><a href="#n3029">3029</a></p>
<p id="n3030" class="stm mis"><a href="#n3030">3030</a></p>
<p id="n3031" class="pln"><a href="#n3031">3031</a></p>
<p id="n3032" class="pln"><a href="#n3032">3032</a></p>
<p id="n3033" class="pln"><a href="#n3033">3033</a></p>
<p id="n3034" class="stm mis"><a href="#n3034">3034</a></p>
<p id="n3035" class="stm mis"><a href="#n3035">3035</a></p>
<p id="n3036" class="stm mis"><a href="#n3036">3036</a></p>
<p id="n3037" class="stm mis"><a href="#n3037">3037</a></p>
<p id="n3038" class="pln"><a href="#n3038">3038</a></p>
<p id="n3039" class="pln"><a href="#n3039">3039</a></p>
<p id="n3040" class="stm run hide_run"><a href="#n3040">3040</a></p>
<p id="n3041" class="stm run hide_run"><a href="#n3041">3041</a></p>
<p id="n3042" class="pln"><a href="#n3042">3042</a></p>
<p id="n3043" class="pln"><a href="#n3043">3043</a></p>
<p id="n3044" class="pln"><a href="#n3044">3044</a></p>
<p id="n3045" class="pln"><a href="#n3045">3045</a></p>
<p id="n3046" class="pln"><a href="#n3046">3046</a></p>
<p id="n3047" class="pln"><a href="#n3047">3047</a></p>
<p id="n3048" class="pln"><a href="#n3048">3048</a></p>
<p id="n3049" class="pln"><a href="#n3049">3049</a></p>
<p id="n3050" class="pln"><a href="#n3050">3050</a></p>
<p id="n3051" class="pln"><a href="#n3051">3051</a></p>
<p id="n3052" class="pln"><a href="#n3052">3052</a></p>
<p id="n3053" class="pln"><a href="#n3053">3053</a></p>
<p id="n3054" class="pln"><a href="#n3054">3054</a></p>
<p id="n3055" class="pln"><a href="#n3055">3055</a></p>
<p id="n3056" class="pln"><a href="#n3056">3056</a></p>
<p id="n3057" class="pln"><a href="#n3057">3057</a></p>
<p id="n3058" class="pln"><a href="#n3058">3058</a></p>
<p id="n3059" class="pln"><a href="#n3059">3059</a></p>
<p id="n3060" class="pln"><a href="#n3060">3060</a></p>
<p id="n3061" class="stm mis"><a href="#n3061">3061</a></p>
<p id="n3062" class="stm mis"><a href="#n3062">3062</a></p>
<p id="n3063" class="stm mis"><a href="#n3063">3063</a></p>
<p id="n3064" class="stm mis"><a href="#n3064">3064</a></p>
<p id="n3065" class="pln"><a href="#n3065">3065</a></p>
<p id="n3066" class="pln"><a href="#n3066">3066</a></p>
<p id="n3067" class="stm mis"><a href="#n3067">3067</a></p>
<p id="n3068" class="pln"><a href="#n3068">3068</a></p>
<p id="n3069" class="pln"><a href="#n3069">3069</a></p>
<p id="n3070" class="stm run hide_run"><a href="#n3070">3070</a></p>
<p id="n3071" class="stm run hide_run"><a href="#n3071">3071</a></p>
<p id="n3072" class="pln"><a href="#n3072">3072</a></p>
<p id="n3073" class="pln"><a href="#n3073">3073</a></p>
<p id="n3074" class="pln"><a href="#n3074">3074</a></p>
<p id="n3075" class="pln"><a href="#n3075">3075</a></p>
<p id="n3076" class="pln"><a href="#n3076">3076</a></p>
<p id="n3077" class="pln"><a href="#n3077">3077</a></p>
<p id="n3078" class="pln"><a href="#n3078">3078</a></p>
<p id="n3079" class="pln"><a href="#n3079">3079</a></p>
<p id="n3080" class="pln"><a href="#n3080">3080</a></p>
<p id="n3081" class="pln"><a href="#n3081">3081</a></p>
<p id="n3082" class="pln"><a href="#n3082">3082</a></p>
<p id="n3083" class="pln"><a href="#n3083">3083</a></p>
<p id="n3084" class="pln"><a href="#n3084">3084</a></p>
<p id="n3085" class="pln"><a href="#n3085">3085</a></p>
<p id="n3086" class="pln"><a href="#n3086">3086</a></p>
<p id="n3087" class="pln"><a href="#n3087">3087</a></p>
<p id="n3088" class="pln"><a href="#n3088">3088</a></p>
<p id="n3089" class="pln"><a href="#n3089">3089</a></p>
<p id="n3090" class="pln"><a href="#n3090">3090</a></p>
<p id="n3091" class="stm mis"><a href="#n3091">3091</a></p>
<p id="n3092" class="stm mis"><a href="#n3092">3092</a></p>
<p id="n3093" class="stm mis"><a href="#n3093">3093</a></p>
<p id="n3094" class="stm mis"><a href="#n3094">3094</a></p>
<p id="n3095" class="pln"><a href="#n3095">3095</a></p>
<p id="n3096" class="pln"><a href="#n3096">3096</a></p>
<p id="n3097" class="stm mis"><a href="#n3097">3097</a></p>
<p id="n3098" class="pln"><a href="#n3098">3098</a></p>
<p id="n3099" class="pln"><a href="#n3099">3099</a></p>
<p id="n3100" class="pln"><a href="#n3100">3100</a></p>
<p id="n3101" class="pln"><a href="#n3101">3101</a></p>
<p id="n3102" class="stm run hide_run"><a href="#n3102">3102</a></p>
<p id="n3103" class="stm run hide_run"><a href="#n3103">3103</a></p>
<p id="n3104" class="stm run hide_run"><a href="#n3104">3104</a></p>
<p id="n3105" class="stm run hide_run"><a href="#n3105">3105</a></p>
<p id="n3106" class="stm run hide_run"><a href="#n3106">3106</a></p>
<p id="n3107" class="stm run hide_run"><a href="#n3107">3107</a></p>

            </td>
            <td class="text">
<p id="t1" class="pln"><span class="com"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span><span class="strut">&nbsp;</span></p>
<p id="t2" class="pln"><span class="com">#</span><span class="strut">&nbsp;</span></p>
<p id="t3" class="pln"><span class="com"># Licensed under the Apache License, Version 2.0 (the "License");</span><span class="strut">&nbsp;</span></p>
<p id="t4" class="pln"><span class="com"># you may not use this file except in compliance with the License.</span><span class="strut">&nbsp;</span></p>
<p id="t5" class="pln"><span class="com"># You may obtain a copy of the License at</span><span class="strut">&nbsp;</span></p>
<p id="t6" class="pln"><span class="com">#</span><span class="strut">&nbsp;</span></p>
<p id="t7" class="pln"><span class="com">#     http://www.apache.org/licenses/LICENSE-2.0</span><span class="strut">&nbsp;</span></p>
<p id="t8" class="pln"><span class="com">#</span><span class="strut">&nbsp;</span></p>
<p id="t9" class="pln"><span class="com"># Unless required by applicable law or agreed to in writing, software</span><span class="strut">&nbsp;</span></p>
<p id="t10" class="pln"><span class="com"># distributed under the License is distributed on an "AS IS" BASIS,</span><span class="strut">&nbsp;</span></p>
<p id="t11" class="pln"><span class="com"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><span class="strut">&nbsp;</span></p>
<p id="t12" class="pln"><span class="com"># See the License for the specific language governing permissions and</span><span class="strut">&nbsp;</span></p>
<p id="t13" class="pln"><span class="com"># limitations under the License.</span><span class="strut">&nbsp;</span></p>
<p id="t14" class="pln"><span class="com"># ==============================================================================</span><span class="strut">&nbsp;</span></p>
<p id="t15" class="stm run hide_run"><span class="str">"""Basic arithmetic operators.</span><span class="strut">&nbsp;</span></p>
<p id="t16" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t17" class="pln"><span class="str">See the [python/math_ops](python/math_ops) guide.</span><span class="strut">&nbsp;</span></p>
<p id="t18" class="pln"><span class="str">"""</span><span class="strut">&nbsp;</span></p>
<p id="t19" class="stm run hide_run"><span class="key">from</span> <span class="nam">__future__</span> <span class="key">import</span> <span class="nam">absolute_import</span><span class="strut">&nbsp;</span></p>
<p id="t20" class="stm run hide_run"><span class="key">from</span> <span class="nam">__future__</span> <span class="key">import</span> <span class="nam">division</span><span class="strut">&nbsp;</span></p>
<p id="t21" class="stm run hide_run"><span class="key">from</span> <span class="nam">__future__</span> <span class="key">import</span> <span class="nam">print_function</span><span class="strut">&nbsp;</span></p>
<p id="t22" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t23" class="stm run hide_run"><span class="key">import</span> <span class="nam">numpy</span> <span class="key">as</span> <span class="nam">np</span><span class="strut">&nbsp;</span></p>
<p id="t24" class="stm run hide_run"><span class="key">from</span> <span class="nam">six</span><span class="op">.</span><span class="nam">moves</span> <span class="key">import</span> <span class="nam">xrange</span>  <span class="com"># pylint: disable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t25" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t26" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">eager</span> <span class="key">import</span> <span class="nam">context</span><span class="strut">&nbsp;</span></p>
<p id="t27" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">framework</span> <span class="key">import</span> <span class="nam">common_shapes</span><span class="strut">&nbsp;</span></p>
<p id="t28" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">framework</span> <span class="key">import</span> <span class="nam">constant_op</span><span class="strut">&nbsp;</span></p>
<p id="t29" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">framework</span> <span class="key">import</span> <span class="nam">dtypes</span><span class="strut">&nbsp;</span></p>
<p id="t30" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">framework</span> <span class="key">import</span> <span class="nam">graph_util</span><span class="strut">&nbsp;</span></p>
<p id="t31" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">framework</span> <span class="key">import</span> <span class="nam">ops</span><span class="strut">&nbsp;</span></p>
<p id="t32" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">framework</span> <span class="key">import</span> <span class="nam">sparse_tensor</span><span class="strut">&nbsp;</span></p>
<p id="t33" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">framework</span> <span class="key">import</span> <span class="nam">tensor_shape</span><span class="strut">&nbsp;</span></p>
<p id="t34" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">ops</span> <span class="key">import</span> <span class="nam">array_ops</span><span class="strut">&nbsp;</span></p>
<p id="t35" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">ops</span> <span class="key">import</span> <span class="nam">gen_data_flow_ops</span><span class="strut">&nbsp;</span></p>
<p id="t36" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">ops</span> <span class="key">import</span> <span class="nam">gen_math_ops</span><span class="strut">&nbsp;</span></p>
<p id="t37" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">ops</span> <span class="key">import</span> <span class="nam">gen_nn_ops</span><span class="strut">&nbsp;</span></p>
<p id="t38" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">ops</span> <span class="key">import</span> <span class="nam">gen_sparse_ops</span><span class="strut">&nbsp;</span></p>
<p id="t39" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">ops</span> <span class="key">import</span> <span class="nam">gen_spectral_ops</span><span class="strut">&nbsp;</span></p>
<p id="t40" class="pln"><span class="com"># go/tf-wildcard-import</span><span class="strut">&nbsp;</span></p>
<p id="t41" class="pln"><span class="com"># pylint: disable=wildcard-import</span><span class="strut">&nbsp;</span></p>
<p id="t42" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">ops</span><span class="op">.</span><span class="nam">gen_math_ops</span> <span class="key">import</span> <span class="op">*</span><span class="strut">&nbsp;</span></p>
<p id="t43" class="pln"><span class="com"># pylint: enable=wildcard-import</span><span class="strut">&nbsp;</span></p>
<p id="t44" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">platform</span> <span class="key">import</span> <span class="nam">tf_logging</span> <span class="key">as</span> <span class="nam">logging</span><span class="strut">&nbsp;</span></p>
<p id="t45" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">util</span> <span class="key">import</span> <span class="nam">compat</span><span class="strut">&nbsp;</span></p>
<p id="t46" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">util</span> <span class="key">import</span> <span class="nam">deprecation</span><span class="strut">&nbsp;</span></p>
<p id="t47" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">util</span> <span class="key">import</span> <span class="nam">nest</span><span class="strut">&nbsp;</span></p>
<p id="t48" class="stm run hide_run"><span class="key">from</span> <span class="nam">tensorflow</span><span class="op">.</span><span class="nam">python</span><span class="op">.</span><span class="nam">util</span><span class="op">.</span><span class="nam">tf_export</span> <span class="key">import</span> <span class="nam">tf_export</span><span class="strut">&nbsp;</span></p>
<p id="t49" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t50" class="pln"><span class="com"># Aliases for some automatically-generated names.</span><span class="strut">&nbsp;</span></p>
<p id="t51" class="stm run hide_run"><span class="nam">linspace</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">lin_space</span><span class="strut">&nbsp;</span></p>
<p id="t52" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t53" class="stm run hide_run"><span class="nam">arg_max</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated</span><span class="op">(</span><span class="key">None</span><span class="op">,</span> <span class="str">"Use `argmax` instead"</span><span class="op">)</span><span class="op">(</span><span class="nam">arg_max</span><span class="op">)</span>  <span class="com"># pylint: disable=used-before-assignment</span><span class="strut">&nbsp;</span></p>
<p id="t54" class="stm run hide_run"><span class="nam">arg_min</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated</span><span class="op">(</span><span class="key">None</span><span class="op">,</span> <span class="str">"Use `argmin` instead"</span><span class="op">)</span><span class="op">(</span><span class="nam">arg_min</span><span class="op">)</span>  <span class="com"># pylint: disable=used-before-assignment</span><span class="strut">&nbsp;</span></p>
<p id="t55" class="stm run hide_run"><span class="nam">tf_export</span><span class="op">(</span><span class="str">"arg_max"</span><span class="op">)</span><span class="op">(</span><span class="nam">arg_max</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t56" class="stm run hide_run"><span class="nam">tf_export</span><span class="op">(</span><span class="str">"arg_min"</span><span class="op">)</span><span class="op">(</span><span class="nam">arg_min</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t57" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t58" class="pln"><span class="com"># This is set by resource_variable_ops.py. It is included in this way since</span><span class="strut">&nbsp;</span></p>
<p id="t59" class="pln"><span class="com"># there is a circular dependency between math_ops and resource_variable_ops</span><span class="strut">&nbsp;</span></p>
<p id="t60" class="stm run hide_run"><span class="nam">_resource_variable_type</span> <span class="op">=</span> <span class="key">None</span><span class="strut">&nbsp;</span></p>
<p id="t61" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t62" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t63" class="stm run hide_run"><span class="key">def</span> <span class="nam">_set_doc</span><span class="op">(</span><span class="nam">doc</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t64" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t65" class="stm run hide_run">  <span class="key">def</span> <span class="nam">_decorator</span><span class="op">(</span><span class="nam">func</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t66" class="stm run hide_run">    <span class="nam">func</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">=</span> <span class="nam">doc</span><span class="strut">&nbsp;</span></p>
<p id="t67" class="stm run hide_run">    <span class="key">return</span> <span class="nam">func</span><span class="strut">&nbsp;</span></p>
<p id="t68" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t69" class="stm run hide_run">  <span class="key">return</span> <span class="nam">_decorator</span><span class="strut">&nbsp;</span></p>
<p id="t70" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t71" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t72" class="pln"><span class="com"># pylint: disable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t73" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.argmax"</span><span class="op">,</span> <span class="str">"argmax"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t74" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="key">None</span><span class="op">,</span> <span class="str">"Use the `axis` argument instead"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t75" class="pln">                             <span class="str">"dimension"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t76" class="stm run hide_run"><span class="op">@</span><span class="nam">_set_doc</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t77" class="pln">    <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">arg_max</span><span class="op">.</span><span class="nam">__doc__</span><span class="op">.</span><span class="nam">replace</span><span class="op">(</span><span class="str">"dimensions"</span><span class="op">,</span> <span class="str">"axes"</span><span class="op">)</span><span class="op">.</span><span class="nam">replace</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t78" class="pln">        <span class="str">"dimension"</span><span class="op">,</span> <span class="str">"axis"</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t79" class="stm run hide_run"><span class="key">def</span> <span class="nam">argmax</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t80" class="pln">           <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t81" class="pln">           <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t82" class="pln">           <span class="nam">dimension</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t83" class="pln">           <span class="nam">output_type</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int64</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t84" class="stm mis">  <span class="nam">axis</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t85" class="pln">      <span class="str">"axis"</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="str">"dimension"</span><span class="op">,</span> <span class="nam">dimension</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t86" class="stm mis">  <span class="key">if</span> <span class="nam">axis</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t87" class="stm mis">    <span class="nam">axis</span> <span class="op">=</span> <span class="num">0</span><span class="strut">&nbsp;</span></p>
<p id="t88" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">arg_max</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">,</span> <span class="nam">output_type</span><span class="op">=</span><span class="nam">output_type</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t89" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t90" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t91" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.argmin"</span><span class="op">,</span> <span class="str">"argmin"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t92" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="key">None</span><span class="op">,</span> <span class="str">"Use the `axis` argument instead"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t93" class="pln">                             <span class="str">"dimension"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t94" class="stm run hide_run"><span class="op">@</span><span class="nam">_set_doc</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t95" class="pln">    <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">arg_min</span><span class="op">.</span><span class="nam">__doc__</span><span class="op">.</span><span class="nam">replace</span><span class="op">(</span><span class="str">"dimensions"</span><span class="op">,</span> <span class="str">"axes"</span><span class="op">)</span><span class="op">.</span><span class="nam">replace</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t96" class="pln">        <span class="str">"dimension"</span><span class="op">,</span> <span class="str">"axis"</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t97" class="stm run hide_run"><span class="key">def</span> <span class="nam">argmin</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t98" class="pln">           <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t99" class="pln">           <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t100" class="pln">           <span class="nam">dimension</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t101" class="pln">           <span class="nam">output_type</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int64</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t102" class="stm mis">  <span class="nam">axis</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t103" class="pln">      <span class="str">"axis"</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="str">"dimension"</span><span class="op">,</span> <span class="nam">dimension</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t104" class="stm mis">  <span class="key">if</span> <span class="nam">axis</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t105" class="stm mis">    <span class="nam">axis</span> <span class="op">=</span> <span class="num">0</span><span class="strut">&nbsp;</span></p>
<p id="t106" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">arg_min</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">,</span> <span class="nam">output_type</span><span class="op">=</span><span class="nam">output_type</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t107" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t108" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t109" class="pln"><span class="com"># pylint: enable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t110" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t111" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t112" class="pln"><span class="com"># pylint: disable=anomalous-backslash-in-string,protected-access</span><span class="strut">&nbsp;</span></p>
<p id="t113" class="pln"><span class="com"># pylint: disable=g-docstring-has-escape</span><span class="strut">&nbsp;</span></p>
<p id="t114" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.abs"</span><span class="op">,</span> <span class="str">"abs"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t115" class="stm run hide_run"><span class="key">def</span> <span class="nam">abs</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span>  <span class="com"># pylint: disable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t116" class="pln">  <span class="str">r"""Computes the absolute value of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t117" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t118" class="pln"><span class="str">  Given a tensor `x` of complex numbers, this operation returns a tensor of type</span><span class="strut">&nbsp;</span></p>
<p id="t119" class="pln"><span class="str">  `float32` or `float64` that is the absolute value of each element in `x`. All</span><span class="strut">&nbsp;</span></p>
<p id="t120" class="pln"><span class="str">  elements in `x` must be complex numbers of the form \\(a + bj\\). The</span><span class="strut">&nbsp;</span></p>
<p id="t121" class="pln"><span class="str">  absolute value is computed as \\( \sqrt{a^2 + b^2}\\).  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t122" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t123" class="pln"><span class="str">  x = tf.constant([[-2.25 + 4.75j], [-3.25 + 5.75j]])</span><span class="strut">&nbsp;</span></p>
<p id="t124" class="pln"><span class="str">  tf.abs(x)  # [5.25594902, 6.60492229]</span><span class="strut">&nbsp;</span></p>
<p id="t125" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t126" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t127" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t128" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` of type `float16`, `float32`, `float64`,</span><span class="strut">&nbsp;</span></p>
<p id="t129" class="pln"><span class="str">      `int32`, `int64`, `complex64` or `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t130" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t131" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t132" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t133" class="pln"><span class="str">    A `Tensor` or `SparseTensor` the same size and type as `x` with absolute</span><span class="strut">&nbsp;</span></p>
<p id="t134" class="pln"><span class="str">      values.</span><span class="strut">&nbsp;</span></p>
<p id="t135" class="pln"><span class="str">    Note, for `complex64` or `complex128` input, the returned `Tensor` will be</span><span class="strut">&nbsp;</span></p>
<p id="t136" class="pln"><span class="str">      of type `float32` or `float64`, respectively.</span><span class="strut">&nbsp;</span></p>
<p id="t137" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t138" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Abs"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t139" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t140" class="stm mis">      <span class="key">if</span> <span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_complex</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t141" class="stm mis">        <span class="nam">x_abs</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">complex_abs</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t142" class="pln">            <span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">Tout</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">real_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t143" class="stm mis">        <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t144" class="pln">            <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_abs</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t145" class="stm mis">      <span class="nam">x_abs</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_abs</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t146" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t147" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_abs</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t148" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t149" class="stm mis">      <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t150" class="stm mis">      <span class="key">if</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_complex</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t151" class="stm mis">        <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">complex_abs</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">Tout</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">real_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t152" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_abs</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t153" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t154" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t155" class="pln"><span class="com"># pylint: enable=g-docstring-has-escape</span><span class="strut">&nbsp;</span></p>
<p id="t156" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t157" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t158" class="pln"><span class="com"># pylint: disable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t159" class="stm run hide_run"><span class="key">def</span> <span class="nam">_bucketize</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">boundaries</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t160" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">bucketize</span><span class="op">(</span><span class="nam">input</span><span class="op">=</span><span class="nam">input</span><span class="op">,</span> <span class="nam">boundaries</span><span class="op">=</span><span class="nam">boundaries</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t161" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t162" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t163" class="pln"><span class="com"># pylint: enable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t164" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t165" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t166" class="stm run hide_run"><span class="key">class</span> <span class="nam">DivideDelegateWithName</span><span class="op">(</span><span class="nam">object</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t167" class="pln">  <span class="str">"""Use Python2/Python3 division delegation to implement divide for tensors."""</span><span class="strut">&nbsp;</span></p>
<p id="t168" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t169" class="stm run hide_run">  <span class="key">def</span> <span class="nam">__init__</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t170" class="pln">    <span class="str">"""Construct DivideDelegateWithName.</span><span class="strut">&nbsp;</span></p>
<p id="t171" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t172" class="pln"><span class="str">    Args:</span><span class="strut">&nbsp;</span></p>
<p id="t173" class="pln"><span class="str">      x: Tensor to use as left operand in operator overloads</span><span class="strut">&nbsp;</span></p>
<p id="t174" class="pln"><span class="str">      name: The name that is preferred for the op created.</span><span class="strut">&nbsp;</span></p>
<p id="t175" class="pln"><span class="str">    """</span><span class="strut">&nbsp;</span></p>
<p id="t176" class="stm mis">    <span class="nam">self</span><span class="op">.</span><span class="nam">x</span> <span class="op">=</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t177" class="stm mis">    <span class="nam">self</span><span class="op">.</span><span class="nam">name</span> <span class="op">=</span> <span class="nam">name</span><span class="strut">&nbsp;</span></p>
<p id="t178" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t179" class="stm run hide_run">  <span class="key">def</span> <span class="nam">__truediv__</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">y</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t180" class="stm mis">    <span class="key">return</span> <span class="nam">_truediv_python3</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t181" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t182" class="stm run hide_run">  <span class="key">def</span> <span class="nam">__floordiv__</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">y</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t183" class="stm mis">    <span class="key">return</span> <span class="nam">floordiv</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t184" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t185" class="stm run hide_run">  <span class="key">def</span> <span class="nam">__div__</span><span class="op">(</span><span class="nam">self</span><span class="op">,</span> <span class="nam">y</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t186" class="stm mis">    <span class="key">return</span> <span class="nam">_div_python2</span><span class="op">(</span><span class="nam">self</span><span class="op">.</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">self</span><span class="op">.</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t187" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t188" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t189" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.divide"</span><span class="op">,</span> <span class="str">"divide"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t190" class="stm run hide_run"><span class="key">def</span> <span class="nam">divide</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t191" class="pln">  <span class="str">"""Computes Python style division of `x` by `y`."""</span><span class="strut">&nbsp;</span></p>
<p id="t192" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t193" class="stm mis">  <span class="key">if</span> <span class="nam">name</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t194" class="pln">    <span class="com"># Cannot use tensors operator overload, because it has no way to track</span><span class="strut">&nbsp;</span></p>
<p id="t195" class="pln">    <span class="com"># override names. Use a dummy class to track the runtime division behavior</span><span class="strut">&nbsp;</span></p>
<p id="t196" class="stm mis">    <span class="key">return</span> <span class="nam">DivideDelegateWithName</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span> <span class="op">/</span> <span class="nam">y</span><span class="strut">&nbsp;</span></p>
<p id="t197" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t198" class="stm mis">    <span class="key">return</span> <span class="nam">x</span> <span class="op">/</span> <span class="nam">y</span><span class="strut">&nbsp;</span></p>
<p id="t199" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t200" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t201" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.multiply"</span><span class="op">,</span> <span class="str">"multiply"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t202" class="stm run hide_run"><span class="key">def</span> <span class="nam">multiply</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t203" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">mul</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t204" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t205" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t206" class="stm run hide_run"><span class="nam">multiply</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">mul</span><span class="op">.</span><span class="nam">__doc__</span><span class="op">.</span><span class="nam">replace</span><span class="op">(</span><span class="str">"Multiply"</span><span class="op">,</span> <span class="str">"`tf.multiply`"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t207" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t208" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t209" class="pln"><span class="com"># TODO(aselle): put deprecation in after another round of global code changes</span><span class="strut">&nbsp;</span></p>
<p id="t210" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t211" class="pln">    <span class="str">"2016-12-30"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t212" class="pln">    <span class="str">"`tf.mul(x, y)` is deprecated, please use `tf.multiply(x, y)` or `x * y`"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t213" class="stm run hide_run"><span class="key">def</span> <span class="nam">_mul</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t214" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">mul</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t215" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t216" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t217" class="stm run hide_run"><span class="nam">_mul</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">=</span> <span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t218" class="pln">    <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">mul</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">+</span> <span class="op">(</span><span class="str">""</span> <span class="key">if</span> <span class="nam">_mul</span><span class="op">.</span><span class="nam">__doc__</span> <span class="key">is</span> <span class="key">None</span> <span class="key">else</span> <span class="nam">_mul</span><span class="op">.</span><span class="nam">__doc__</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t219" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t220" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t221" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.subtract"</span><span class="op">,</span> <span class="str">"subtract"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t222" class="stm run hide_run"><span class="key">def</span> <span class="nam">subtract</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t223" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sub</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t224" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t225" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t226" class="stm run hide_run"><span class="nam">subtract</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sub</span><span class="op">.</span><span class="nam">__doc__</span><span class="op">.</span><span class="nam">replace</span><span class="op">(</span><span class="str">"`Sub`"</span><span class="op">,</span> <span class="str">"`tf.subtract`"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t227" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t228" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t229" class="pln"><span class="com"># TODO(aselle): put deprecation in after another round of global code changes</span><span class="strut">&nbsp;</span></p>
<p id="t230" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t231" class="pln">    <span class="str">"2016-12-30"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t232" class="pln">    <span class="str">"`tf.sub(x, y)` is deprecated, please use `tf.subtract(x, y)` or `x - y`"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t233" class="stm run hide_run"><span class="key">def</span> <span class="nam">_sub</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t234" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sub</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t235" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t236" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t237" class="stm run hide_run"><span class="nam">_sub</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">=</span> <span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t238" class="pln">    <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sub</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">+</span> <span class="op">(</span><span class="str">""</span> <span class="key">if</span> <span class="nam">_sub</span><span class="op">.</span><span class="nam">__doc__</span> <span class="key">is</span> <span class="key">None</span> <span class="key">else</span> <span class="nam">_sub</span><span class="op">.</span><span class="nam">__doc__</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t239" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t240" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t241" class="pln"><span class="com"># pylint: disable=g-docstring-has-escape</span><span class="strut">&nbsp;</span></p>
<p id="t242" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.negative"</span><span class="op">,</span> <span class="str">"negative"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t243" class="stm run hide_run"><span class="key">def</span> <span class="nam">negative</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t244" class="pln">  <span class="str">"""Computes numerical negative value element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t245" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t246" class="pln"><span class="str">  I.e., \\(y = -x\\).</span><span class="strut">&nbsp;</span></p>
<p id="t247" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t248" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t249" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span><span class="strut">&nbsp;</span></p>
<p id="t250" class="pln"><span class="str">      `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t251" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t252" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t253" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t254" class="pln"><span class="str">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t255" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t256" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Neg"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t257" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t258" class="stm mis">      <span class="nam">x_neg</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">neg</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t259" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t260" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_neg</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t261" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t262" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">neg</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t263" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t264" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t265" class="pln"><span class="com"># pylint: enable=g-docstring-has-escape</span><span class="strut">&nbsp;</span></p>
<p id="t266" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t267" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t268" class="pln"><span class="com"># pylint: disable=g-docstring-has-escape</span><span class="strut">&nbsp;</span></p>
<p id="t269" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t270" class="pln">    <span class="str">"2016-12-30"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t271" class="pln">    <span class="str">"`tf.neg(x)` is deprecated, please use `tf.negative(x)` or `-x`"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t272" class="stm run hide_run"><span class="key">def</span> <span class="nam">_neg</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t273" class="pln">  <span class="str">"""Computes numerical negative value element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t274" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t275" class="pln"><span class="str">  I.e., \\(y = -x\\).</span><span class="strut">&nbsp;</span></p>
<p id="t276" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t277" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t278" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span><span class="strut">&nbsp;</span></p>
<p id="t279" class="pln"><span class="str">      `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t280" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t281" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t282" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t283" class="pln"><span class="str">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t284" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t285" class="stm mis">  <span class="key">return</span> <span class="nam">negative</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t286" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t287" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t288" class="pln"><span class="com"># pylint: enable=g-docstring-has-escape</span><span class="strut">&nbsp;</span></p>
<p id="t289" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t290" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t291" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.sign"</span><span class="op">,</span> <span class="str">"sign"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t292" class="stm run hide_run"><span class="key">def</span> <span class="nam">sign</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t293" class="pln">  <span class="str">"""Returns an element-wise indication of the sign of a number.</span><span class="strut">&nbsp;</span></p>
<p id="t294" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t295" class="pln"><span class="str">  `y = sign(x) = -1` if `x &lt; 0`; 0 if `x == 0` or `tf.is_nan(x)`; 1 if `x > 0`.</span><span class="strut">&nbsp;</span></p>
<p id="t296" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t297" class="pln"><span class="str">  Zero is returned for NaN inputs.</span><span class="strut">&nbsp;</span></p>
<p id="t298" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t299" class="pln"><span class="str">  For complex numbers, `y = sign(x) = x / |x|` if `x != 0`, otherwise `y = 0`.</span><span class="strut">&nbsp;</span></p>
<p id="t300" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t301" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t302" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span><span class="strut">&nbsp;</span></p>
<p id="t303" class="pln"><span class="str">      `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t304" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t305" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t306" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t307" class="pln"><span class="str">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t308" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t309" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t310" class="pln"><span class="str">  Equivalent to numpy.sign except for the behavior for input values of NaN.</span><span class="strut">&nbsp;</span></p>
<p id="t311" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t312" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t313" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Sign"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t314" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t315" class="stm mis">      <span class="nam">x_sign</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sign</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t316" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t317" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_sign</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t318" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t319" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sign</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t320" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t321" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t322" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.square"</span><span class="op">,</span> <span class="str">"square"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t323" class="stm run hide_run"><span class="key">def</span> <span class="nam">square</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t324" class="pln">  <span class="str">r"""Computes square of x element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t325" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t326" class="pln"><span class="str">  I.e., \\(y = x * x = x^2\\).</span><span class="strut">&nbsp;</span></p>
<p id="t327" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t328" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t329" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span><span class="strut">&nbsp;</span></p>
<p id="t330" class="pln"><span class="str">      `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t331" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t332" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t333" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t334" class="pln"><span class="str">    A `Tensor` or `SparseTensor`. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t335" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t336" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Square"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t337" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t338" class="stm mis">      <span class="nam">x_square</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">square</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t339" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t340" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_square</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t341" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t342" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">square</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t343" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t344" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t345" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.sqrt"</span><span class="op">,</span> <span class="str">"sqrt"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t346" class="stm run hide_run"><span class="key">def</span> <span class="nam">sqrt</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t347" class="pln">  <span class="str">r"""Computes square root of x element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t348" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t349" class="pln"><span class="str">  I.e., \\(y = \sqrt{x} = x^{1/2}\\).</span><span class="strut">&nbsp;</span></p>
<p id="t350" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t351" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t352" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span><span class="strut">&nbsp;</span></p>
<p id="t353" class="pln"><span class="str">      `float32`, `float64`, `complex64`, `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t354" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t355" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t356" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t357" class="pln"><span class="str">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t358" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t359" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Sqrt"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t360" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t361" class="stm mis">      <span class="nam">x_sqrt</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sqrt</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t362" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t363" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_sqrt</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t364" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t365" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sqrt</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t366" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t367" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t368" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.erf"</span><span class="op">,</span> <span class="str">"erf"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t369" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"erf"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t370" class="stm run hide_run"><span class="key">def</span> <span class="nam">erf</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t371" class="pln">  <span class="str">"""Computes the Gauss error function of `x` element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t372" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t373" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t374" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span><span class="strut">&nbsp;</span></p>
<p id="t375" class="pln"><span class="str">      `float32`, `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t376" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t377" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t378" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t379" class="pln"><span class="str">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t380" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t381" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Erf"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t382" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t383" class="stm mis">      <span class="nam">x_erf</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">erf</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t384" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t385" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_erf</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t386" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t387" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">erf</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t388" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t389" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t390" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.scalar_mul"</span><span class="op">,</span> <span class="str">"scalar_mul"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t391" class="pln"><span class="key">def</span> <span class="nam">scalar_mul</span><span class="op">(</span><span class="nam">scalar</span><span class="op">,</span> <span class="nam">x</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t392" class="pln">  <span class="str">"""Multiplies a scalar times a `Tensor` or `IndexedSlices` object.</span><span class="strut">&nbsp;</span></p>
<p id="t393" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t394" class="pln"><span class="str">  Intended for use in gradient code which might deal with `IndexedSlices`</span><span class="strut">&nbsp;</span></p>
<p id="t395" class="pln"><span class="str">  objects, which are easy to multiply by a scalar but more expensive to</span><span class="strut">&nbsp;</span></p>
<p id="t396" class="pln"><span class="str">  multiply with arbitrary tensors.</span><span class="strut">&nbsp;</span></p>
<p id="t397" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t398" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t399" class="pln"><span class="str">    scalar: A 0-D scalar `Tensor`. Must have known shape.</span><span class="strut">&nbsp;</span></p>
<p id="t400" class="pln"><span class="str">    x: A `Tensor` or `IndexedSlices` to be scaled.</span><span class="strut">&nbsp;</span></p>
<p id="t401" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t402" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t403" class="pln"><span class="str">    `scalar * x` of the same type (`Tensor` or `IndexedSlices`) as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t404" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t405" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t406" class="pln"><span class="str">    ValueError: if scalar is not a 0-D `scalar`.</span><span class="strut">&nbsp;</span></p>
<p id="t407" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t408" class="stm mis">  <span class="nam">scalar</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t409" class="pln">      <span class="nam">scalar</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"scalar"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t410" class="stm mis">  <span class="nam">shape</span> <span class="op">=</span> <span class="nam">scalar</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t411" class="stm mis">  <span class="key">if</span> <span class="nam">shape</span><span class="op">.</span><span class="nam">ndims</span> <span class="op">==</span> <span class="num">0</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t412" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t413" class="stm mis">      <span class="key">return</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">(</span><span class="nam">scalar</span> <span class="op">*</span> <span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t414" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t415" class="stm mis">      <span class="key">return</span> <span class="nam">scalar</span> <span class="op">*</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t416" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t417" class="stm mis">    <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Only scalar multiply works, got shape %s"</span> <span class="op">%</span> <span class="nam">shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t418" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t419" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t420" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.pow"</span><span class="op">,</span> <span class="str">"pow"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t421" class="stm run hide_run"><span class="key">def</span> <span class="nam">pow</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span>  <span class="com"># pylint: disable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t422" class="pln">  <span class="str">r"""Computes the power of one value to another.</span><span class="strut">&nbsp;</span></p>
<p id="t423" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t424" class="pln"><span class="str">  Given a tensor `x` and a tensor `y`, this operation computes \\(x^y\\) for</span><span class="strut">&nbsp;</span></p>
<p id="t425" class="pln"><span class="str">  corresponding elements in `x` and `y`. For example:</span><span class="strut">&nbsp;</span></p>
<p id="t426" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t427" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t428" class="pln"><span class="str">  x = tf.constant([[2, 2], [3, 3]])</span><span class="strut">&nbsp;</span></p>
<p id="t429" class="pln"><span class="str">  y = tf.constant([[8, 16], [2, 3]])</span><span class="strut">&nbsp;</span></p>
<p id="t430" class="pln"><span class="str">  tf.pow(x, y)  # [[256, 65536], [9, 27]]</span><span class="strut">&nbsp;</span></p>
<p id="t431" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t432" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t433" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t434" class="pln"><span class="str">    x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,</span><span class="strut">&nbsp;</span></p>
<p id="t435" class="pln"><span class="str">     `complex64`, or `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t436" class="pln"><span class="str">    y: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, `int64`,</span><span class="strut">&nbsp;</span></p>
<p id="t437" class="pln"><span class="str">     `complex64`, or `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t438" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t439" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t440" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t441" class="pln"><span class="str">    A `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t442" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t443" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Pow"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t444" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_pow</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t445" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t446" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t447" class="pln"><span class="com"># pylint: disable=redefined-builtin,redefined-outer-name</span><span class="strut">&nbsp;</span></p>
<p id="t448" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"dtypes.complex"</span><span class="op">,</span> <span class="str">"complex"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t449" class="stm run hide_run"><span class="key">def</span> <span class="nam">complex</span><span class="op">(</span><span class="nam">real</span><span class="op">,</span> <span class="nam">imag</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t450" class="pln">  <span class="str">r"""Converts two real numbers to a complex number.</span><span class="strut">&nbsp;</span></p>
<p id="t451" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t452" class="pln"><span class="str">  Given a tensor `real` representing the real part of a complex number, and a</span><span class="strut">&nbsp;</span></p>
<p id="t453" class="pln"><span class="str">  tensor `imag` representing the imaginary part of a complex number, this</span><span class="strut">&nbsp;</span></p>
<p id="t454" class="pln"><span class="str">  operation returns complex numbers elementwise of the form \\(a + bj\\), where</span><span class="strut">&nbsp;</span></p>
<p id="t455" class="pln"><span class="str">  *a* represents the `real` part and *b* represents the `imag` part.</span><span class="strut">&nbsp;</span></p>
<p id="t456" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t457" class="pln"><span class="str">  The input tensors `real` and `imag` must have the same shape.</span><span class="strut">&nbsp;</span></p>
<p id="t458" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t459" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t460" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t461" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t462" class="pln"><span class="str">  real = tf.constant([2.25, 3.25])</span><span class="strut">&nbsp;</span></p>
<p id="t463" class="pln"><span class="str">  imag = tf.constant([4.75, 5.75])</span><span class="strut">&nbsp;</span></p>
<p id="t464" class="pln"><span class="str">  tf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]</span><span class="strut">&nbsp;</span></p>
<p id="t465" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t466" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t467" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t468" class="pln"><span class="str">    real: A `Tensor`. Must be one of the following types: `float32`,</span><span class="strut">&nbsp;</span></p>
<p id="t469" class="pln"><span class="str">      `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t470" class="pln"><span class="str">    imag: A `Tensor`. Must have the same type as `real`.</span><span class="strut">&nbsp;</span></p>
<p id="t471" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t472" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t473" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t474" class="pln"><span class="str">    A `Tensor` of type `complex64` or `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t475" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t476" class="stm mis">  <span class="nam">real</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">real</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"real"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t477" class="stm mis">  <span class="nam">imag</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">imag</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"imag"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t478" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Complex"</span><span class="op">,</span> <span class="op">[</span><span class="nam">real</span><span class="op">,</span> <span class="nam">imag</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t479" class="stm mis">    <span class="nam">input_types</span> <span class="op">=</span> <span class="op">(</span><span class="nam">real</span><span class="op">.</span><span class="nam">dtype</span><span class="op">,</span> <span class="nam">imag</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t480" class="stm mis">    <span class="key">if</span> <span class="nam">input_types</span> <span class="op">==</span> <span class="op">(</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">float64</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float64</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t481" class="stm mis">      <span class="nam">Tout</span> <span class="op">=</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">complex128</span><span class="strut">&nbsp;</span></p>
<p id="t482" class="stm mis">    <span class="key">elif</span> <span class="nam">input_types</span> <span class="op">==</span> <span class="op">(</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t483" class="stm mis">      <span class="nam">Tout</span> <span class="op">=</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">complex64</span><span class="strut">&nbsp;</span></p>
<p id="t484" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t485" class="stm mis">      <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"real and imag have incorrect types: "</span><span class="strut">&nbsp;</span></p>
<p id="t486" class="pln">                      <span class="str">"{} {}"</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="nam">real</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">name</span><span class="op">,</span> <span class="nam">imag</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">name</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t487" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_complex</span><span class="op">(</span><span class="nam">real</span><span class="op">,</span> <span class="nam">imag</span><span class="op">,</span> <span class="nam">Tout</span><span class="op">=</span><span class="nam">Tout</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t488" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t489" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t490" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.real"</span><span class="op">,</span> <span class="str">"real"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t491" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"real"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t492" class="stm run hide_run"><span class="key">def</span> <span class="nam">real</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t493" class="pln">  <span class="str">r"""Returns the real part of a complex (or real) tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t494" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t495" class="pln"><span class="str">  Given a tensor `input`, this operation returns a tensor of type `float` that</span><span class="strut">&nbsp;</span></p>
<p id="t496" class="pln"><span class="str">  is the real part of each element in `input` considered as a complex number.</span><span class="strut">&nbsp;</span></p>
<p id="t497" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t498" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t499" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t500" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t501" class="pln"><span class="str">  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])</span><span class="strut">&nbsp;</span></p>
<p id="t502" class="pln"><span class="str">  tf.real(x)  # [-2.25, 3.25]</span><span class="strut">&nbsp;</span></p>
<p id="t503" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t504" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t505" class="pln"><span class="str">  If `input` is already real, it is returned unchanged.</span><span class="strut">&nbsp;</span></p>
<p id="t506" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t507" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t508" class="pln"><span class="str">    input: A `Tensor`. Must have numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t509" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t510" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t511" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t512" class="pln"><span class="str">    A `Tensor` of type `float32` or `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t513" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t514" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Real"</span><span class="op">,</span> <span class="op">[</span><span class="nam">input</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t515" class="stm mis">    <span class="key">if</span> <span class="nam">input</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_complex</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t516" class="stm mis">      <span class="nam">real_dtype</span> <span class="op">=</span> <span class="nam">input</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">real_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t517" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">real</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">Tout</span><span class="op">=</span><span class="nam">real_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t518" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t519" class="stm mis">      <span class="key">return</span> <span class="nam">input</span><span class="strut">&nbsp;</span></p>
<p id="t520" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t521" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t522" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.imag"</span><span class="op">,</span> <span class="str">"imag"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t523" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"imag"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t524" class="stm run hide_run"><span class="key">def</span> <span class="nam">imag</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t525" class="pln">  <span class="str">r"""Returns the imaginary part of a complex (or real) tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t526" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t527" class="pln"><span class="str">  Given a tensor `input`, this operation returns a tensor of type `float` that</span><span class="strut">&nbsp;</span></p>
<p id="t528" class="pln"><span class="str">  is the imaginary part of each element in `input` considered as a complex</span><span class="strut">&nbsp;</span></p>
<p id="t529" class="pln"><span class="str">  number. If `input` is real, a tensor of all zeros is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t530" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t531" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t532" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t533" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t534" class="pln"><span class="str">  x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])</span><span class="strut">&nbsp;</span></p>
<p id="t535" class="pln"><span class="str">  tf.imag(x)  # [4.75, 5.75]</span><span class="strut">&nbsp;</span></p>
<p id="t536" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t537" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t538" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t539" class="pln"><span class="str">    input: A `Tensor`. Must be one of the following types: `float`, `double`,</span><span class="strut">&nbsp;</span></p>
<p id="t540" class="pln"><span class="str">      `complex64`, `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t541" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t542" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t543" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t544" class="pln"><span class="str">    A `Tensor` of type `float32` or `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t545" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t546" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Imag"</span><span class="op">,</span> <span class="op">[</span><span class="nam">input</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t547" class="stm mis">    <span class="key">if</span> <span class="nam">input</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_complex</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t548" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">imag</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">Tout</span><span class="op">=</span><span class="nam">input</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">real_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t549" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t550" class="stm mis">      <span class="key">return</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span><span class="nam">input</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t551" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t552" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t553" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.angle"</span><span class="op">,</span> <span class="str">"angle"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t554" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"angle"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t555" class="stm run hide_run"><span class="key">def</span> <span class="nam">angle</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t556" class="pln">  <span class="str">r"""Returns the element-wise argument of a complex (or real) tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t557" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t558" class="pln"><span class="str">  Given a tensor `input`, this operation returns a tensor of type `float` that</span><span class="strut">&nbsp;</span></p>
<p id="t559" class="pln"><span class="str">  is the argument of each element in `input` considered as a complex number.</span><span class="strut">&nbsp;</span></p>
<p id="t560" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t561" class="pln"><span class="str">  The elements in `input` are considered to be complex numbers of the form</span><span class="strut">&nbsp;</span></p>
<p id="t562" class="pln"><span class="str">  \\(a + bj\\), where *a* is the real part and *b* is the imaginary part.</span><span class="strut">&nbsp;</span></p>
<p id="t563" class="pln"><span class="str">  If `input` is real then *b* is zero by definition.</span><span class="strut">&nbsp;</span></p>
<p id="t564" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t565" class="pln"><span class="str">  The argument returned by this function is of the form \\(atan2(b, a)\\).</span><span class="strut">&nbsp;</span></p>
<p id="t566" class="pln"><span class="str">  If `input` is real, a tensor of all zeros is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t567" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t568" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t569" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t570" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t571" class="pln"><span class="str">  # tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]</span><span class="strut">&nbsp;</span></p>
<p id="t572" class="pln"><span class="str">  tf.angle(input) ==> [2.0132, 1.056]</span><span class="strut">&nbsp;</span></p>
<p id="t573" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t574" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t575" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t576" class="pln"><span class="str">    input: A `Tensor`. Must be one of the following types: `float`, `double`,</span><span class="strut">&nbsp;</span></p>
<p id="t577" class="pln"><span class="str">      `complex64`, `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t578" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t579" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t580" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t581" class="pln"><span class="str">    A `Tensor` of type `float32` or `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t582" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t583" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Angle"</span><span class="op">,</span> <span class="op">[</span><span class="nam">input</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t584" class="stm mis">    <span class="key">if</span> <span class="nam">input</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_complex</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t585" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">angle</span><span class="op">(</span><span class="nam">input</span><span class="op">,</span> <span class="nam">Tout</span><span class="op">=</span><span class="nam">input</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">real_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t586" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t587" class="stm mis">      <span class="key">return</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span><span class="nam">input</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t588" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t589" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t590" class="pln"><span class="com"># pylint: enable=redefined-outer-name,redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t591" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t592" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t593" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.round"</span><span class="op">,</span> <span class="str">"round"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t594" class="stm run hide_run"><span class="key">def</span> <span class="nam">round</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span>  <span class="com"># pylint: disable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t595" class="pln">  <span class="str">"""Rounds the values of a tensor to the nearest integer, element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t596" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t597" class="pln"><span class="str">  Rounds half to even.  Also known as bankers rounding. If you want to round</span><span class="strut">&nbsp;</span></p>
<p id="t598" class="pln"><span class="str">  according to the current system rounding mode use tf::cint.</span><span class="strut">&nbsp;</span></p>
<p id="t599" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t600" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t601" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t602" class="pln"><span class="str">  x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])</span><span class="strut">&nbsp;</span></p>
<p id="t603" class="pln"><span class="str">  tf.round(x)  # [ 1.0, 2.0, 2.0, 2.0, -4.0 ]</span><span class="strut">&nbsp;</span></p>
<p id="t604" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t605" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t606" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t607" class="pln"><span class="str">    x: A `Tensor` of type `float16`, `float32`, `float64`, `int32`, or `int64`.</span><span class="strut">&nbsp;</span></p>
<p id="t608" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t609" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t610" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t611" class="pln"><span class="str">    A `Tensor` of same shape and type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t612" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t613" class="stm mis">  <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t614" class="stm mis">  <span class="key">if</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_integer</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t615" class="stm mis">    <span class="key">return</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t616" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t617" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">round</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t618" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t619" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t620" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"dtypes.cast"</span><span class="op">,</span> <span class="str">"cast"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t621" class="stm run hide_run"><span class="key">def</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t622" class="pln">  <span class="str">"""Casts a tensor to a new type.</span><span class="strut">&nbsp;</span></p>
<p id="t623" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t624" class="pln"><span class="str">  The operation casts `x` (in case of `Tensor`) or `x.values`</span><span class="strut">&nbsp;</span></p>
<p id="t625" class="pln"><span class="str">  (in case of `SparseTensor` or `IndexedSlices`) to `dtype`.</span><span class="strut">&nbsp;</span></p>
<p id="t626" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t627" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t628" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t629" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t630" class="pln"><span class="str">  x = tf.constant([1.8, 2.2], dtype=tf.float32)</span><span class="strut">&nbsp;</span></p>
<p id="t631" class="pln"><span class="str">  tf.cast(x, tf.int32)  # [1, 2], dtype=tf.int32</span><span class="strut">&nbsp;</span></p>
<p id="t632" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t633" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t634" class="pln"><span class="str">  The operation supports data types (for `x` and `dtype`) of</span><span class="strut">&nbsp;</span></p>
<p id="t635" class="pln"><span class="str">  `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`, `int64`,</span><span class="strut">&nbsp;</span></p>
<p id="t636" class="pln"><span class="str">  `float16`, `float32`, `float64`, `complex64`, `complex128`, `bfloat16`.</span><span class="strut">&nbsp;</span></p>
<p id="t637" class="pln"><span class="str">  In case of casting from complex types (`complex64`, `complex128`) to real</span><span class="strut">&nbsp;</span></p>
<p id="t638" class="pln"><span class="str">  types, only the real part of `x` is returned. In case of casting from real</span><span class="strut">&nbsp;</span></p>
<p id="t639" class="pln"><span class="str">  types to complex types (`complex64`, `complex128`), the imaginary part of the</span><span class="strut">&nbsp;</span></p>
<p id="t640" class="pln"><span class="str">  returned value is set to `0`. The handling of complex types here matches the</span><span class="strut">&nbsp;</span></p>
<p id="t641" class="pln"><span class="str">  behavior of numpy.</span><span class="strut">&nbsp;</span></p>
<p id="t642" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t643" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t644" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` or `IndexedSlices` of numeric type. It could</span><span class="strut">&nbsp;</span></p>
<p id="t645" class="pln"><span class="str">      be `uint8`, `uint16`, `uint32`, `uint64`, `int8`, `int16`, `int32`,</span><span class="strut">&nbsp;</span></p>
<p id="t646" class="pln"><span class="str">      `int64`, `float16`, `float32`, `float64`, `complex64`, `complex128`,</span><span class="strut">&nbsp;</span></p>
<p id="t647" class="pln"><span class="str">      `bfloat16`.</span><span class="strut">&nbsp;</span></p>
<p id="t648" class="pln"><span class="str">    dtype: The destination type. The list of supported dtypes is the same as</span><span class="strut">&nbsp;</span></p>
<p id="t649" class="pln"><span class="str">      `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t650" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t651" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t652" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t653" class="pln"><span class="str">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` and</span><span class="strut">&nbsp;</span></p>
<p id="t654" class="pln"><span class="str">      same type as `dtype`.</span><span class="strut">&nbsp;</span></p>
<p id="t655" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t656" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t657" class="pln"><span class="str">    TypeError: If `x` cannot be cast to the `dtype`.</span><span class="strut">&nbsp;</span></p>
<p id="t658" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t659" class="stm mis">  <span class="nam">base_type</span> <span class="op">=</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">as_dtype</span><span class="op">(</span><span class="nam">dtype</span><span class="op">)</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t660" class="stm mis">  <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t661" class="pln">                <span class="op">(</span><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="nam">_resource_variable_type</span><span class="op">)</span><span class="op">)</span> <span class="key">and</span> <span class="nam">base_type</span> <span class="op">==</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t662" class="stm mis">    <span class="key">return</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t663" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Cast"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t664" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t665" class="stm mis">      <span class="nam">values_cast</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">base_type</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t666" class="stm mis">      <span class="nam">x</span> <span class="op">=</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values_cast</span><span class="op">,</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t667" class="stm mis">    <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t668" class="stm mis">      <span class="nam">values_cast</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">base_type</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t669" class="stm mis">      <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">(</span><span class="nam">values_cast</span><span class="op">,</span> <span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t670" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t671" class="pln">      <span class="com"># TODO(josh11b): If x is not already a Tensor, we could return</span><span class="strut">&nbsp;</span></p>
<p id="t672" class="pln">      <span class="com"># ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that</span><span class="strut">&nbsp;</span></p>
<p id="t673" class="pln">      <span class="com"># allows some conversions that cast() can't do, e.g. casting numbers to</span><span class="strut">&nbsp;</span></p>
<p id="t674" class="pln">      <span class="com"># strings.</span><span class="strut">&nbsp;</span></p>
<p id="t675" class="stm mis">      <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t676" class="stm mis">      <span class="key">if</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span> <span class="op">!=</span> <span class="nam">base_type</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t677" class="stm mis">        <span class="nam">x</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">base_type</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t678" class="stm mis">    <span class="key">if</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_complex</span> <span class="key">and</span> <span class="nam">base_type</span><span class="op">.</span><span class="nam">is_floating</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t679" class="stm mis">      <span class="nam">logging</span><span class="op">.</span><span class="nam">warn</span><span class="op">(</span><span class="str">"Casting complex to real discards imaginary part."</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t680" class="stm mis">    <span class="key">return</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t681" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t682" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t683" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"dtypes.saturate_cast"</span><span class="op">,</span> <span class="str">"saturate_cast"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t684" class="stm run hide_run"><span class="key">def</span> <span class="nam">saturate_cast</span><span class="op">(</span><span class="nam">value</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t685" class="pln">  <span class="str">"""Performs a safe saturating cast of `value` to `dtype`.</span><span class="strut">&nbsp;</span></p>
<p id="t686" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t687" class="pln"><span class="str">  This function casts the input to `dtype` without applying any scaling.  If</span><span class="strut">&nbsp;</span></p>
<p id="t688" class="pln"><span class="str">  there is a danger that values would over or underflow in the cast, this op</span><span class="strut">&nbsp;</span></p>
<p id="t689" class="pln"><span class="str">  applies the appropriate clamping before the cast.</span><span class="strut">&nbsp;</span></p>
<p id="t690" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t691" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t692" class="pln"><span class="str">    value: A `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t693" class="pln"><span class="str">    dtype: The desired output `DType`.</span><span class="strut">&nbsp;</span></p>
<p id="t694" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t695" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t696" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t697" class="pln"><span class="str">    `value` safely cast to `dtype`.</span><span class="strut">&nbsp;</span></p>
<p id="t698" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t699" class="pln">  <span class="com"># When casting to a type with smaller representable range, clamp.</span><span class="strut">&nbsp;</span></p>
<p id="t700" class="pln">  <span class="com"># Note that this covers casting to unsigned types as well.</span><span class="strut">&nbsp;</span></p>
<p id="t701" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"saturate_cast"</span><span class="op">,</span> <span class="op">[</span><span class="nam">value</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t702" class="stm mis">    <span class="nam">value</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">value</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"value"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t703" class="stm mis">    <span class="nam">dtype</span> <span class="op">=</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">as_dtype</span><span class="op">(</span><span class="nam">dtype</span><span class="op">)</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t704" class="stm mis">    <span class="key">if</span> <span class="nam">value</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">min</span> <span class="op">&lt;</span> <span class="nam">dtype</span><span class="op">.</span><span class="nam">min</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t705" class="stm mis">      <span class="nam">value</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">maximum</span><span class="op">(</span><span class="nam">value</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t706" class="pln">                                   <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t707" class="pln">                                       <span class="nam">dtype</span><span class="op">.</span><span class="nam">min</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">value</span><span class="op">.</span><span class="nam">dtype</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t708" class="pln">                                       <span class="nam">name</span><span class="op">=</span><span class="str">"min"</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t709" class="stm mis">    <span class="key">if</span> <span class="nam">value</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">max</span> <span class="op">></span> <span class="nam">dtype</span><span class="op">.</span><span class="nam">max</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t710" class="stm mis">      <span class="nam">value</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">minimum</span><span class="op">(</span><span class="nam">value</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t711" class="pln">                                   <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t712" class="pln">                                       <span class="nam">dtype</span><span class="op">.</span><span class="nam">max</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">value</span><span class="op">.</span><span class="nam">dtype</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t713" class="pln">                                       <span class="nam">name</span><span class="op">=</span><span class="str">"max"</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t714" class="stm mis">    <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">value</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t715" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t716" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t717" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"to_float"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t718" class="stm run hide_run"><span class="key">def</span> <span class="nam">to_float</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"ToFloat"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t719" class="pln">  <span class="str">"""Casts a tensor to type `float32`.</span><span class="strut">&nbsp;</span></p>
<p id="t720" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t721" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t722" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span><span class="strut">&nbsp;</span></p>
<p id="t723" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t724" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t725" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t726" class="pln"><span class="str">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span><span class="strut">&nbsp;</span></p>
<p id="t727" class="pln"><span class="str">    type `float32`.</span><span class="strut">&nbsp;</span></p>
<p id="t728" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t729" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t730" class="pln"><span class="str">    TypeError: If `x` cannot be cast to the `float32`.</span><span class="strut">&nbsp;</span></p>
<p id="t731" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t732" class="stm mis">  <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t733" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t734" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t735" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"to_double"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t736" class="stm run hide_run"><span class="key">def</span> <span class="nam">to_double</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"ToDouble"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t737" class="pln">  <span class="str">"""Casts a tensor to type `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t738" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t739" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t740" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span><span class="strut">&nbsp;</span></p>
<p id="t741" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t742" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t743" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t744" class="pln"><span class="str">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span><span class="strut">&nbsp;</span></p>
<p id="t745" class="pln"><span class="str">    type `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t746" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t747" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t748" class="pln"><span class="str">    TypeError: If `x` cannot be cast to the `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t749" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t750" class="stm mis">  <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float64</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t751" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t752" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t753" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"to_int32"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t754" class="stm run hide_run"><span class="key">def</span> <span class="nam">to_int32</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"ToInt32"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t755" class="pln">  <span class="str">"""Casts a tensor to type `int32`.</span><span class="strut">&nbsp;</span></p>
<p id="t756" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t757" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t758" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span><span class="strut">&nbsp;</span></p>
<p id="t759" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t760" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t761" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t762" class="pln"><span class="str">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span><span class="strut">&nbsp;</span></p>
<p id="t763" class="pln"><span class="str">    type `int32`.</span><span class="strut">&nbsp;</span></p>
<p id="t764" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t765" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t766" class="pln"><span class="str">    TypeError: If `x` cannot be cast to the `int32`.</span><span class="strut">&nbsp;</span></p>
<p id="t767" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t768" class="stm mis">  <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t769" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t770" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t771" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"to_int64"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t772" class="stm run hide_run"><span class="key">def</span> <span class="nam">to_int64</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"ToInt64"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t773" class="pln">  <span class="str">"""Casts a tensor to type `int64`.</span><span class="strut">&nbsp;</span></p>
<p id="t774" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t775" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t776" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span><span class="strut">&nbsp;</span></p>
<p id="t777" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t778" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t779" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t780" class="pln"><span class="str">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span><span class="strut">&nbsp;</span></p>
<p id="t781" class="pln"><span class="str">    type `int64`.</span><span class="strut">&nbsp;</span></p>
<p id="t782" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t783" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t784" class="pln"><span class="str">    TypeError: If `x` cannot be cast to the `int64`.</span><span class="strut">&nbsp;</span></p>
<p id="t785" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t786" class="stm mis">  <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int64</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t787" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t788" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t789" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"to_bfloat16"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t790" class="stm run hide_run"><span class="key">def</span> <span class="nam">to_bfloat16</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"ToBFloat16"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t791" class="pln">  <span class="str">"""Casts a tensor to type `bfloat16`.</span><span class="strut">&nbsp;</span></p>
<p id="t792" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t793" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t794" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span><span class="strut">&nbsp;</span></p>
<p id="t795" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t796" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t797" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t798" class="pln"><span class="str">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span><span class="strut">&nbsp;</span></p>
<p id="t799" class="pln"><span class="str">    type `bfloat16`.</span><span class="strut">&nbsp;</span></p>
<p id="t800" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t801" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t802" class="pln"><span class="str">    TypeError: If `x` cannot be cast to the `bfloat16`.</span><span class="strut">&nbsp;</span></p>
<p id="t803" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t804" class="stm mis">  <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">bfloat16</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t805" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t806" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t807" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"to_complex64"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t808" class="stm run hide_run"><span class="key">def</span> <span class="nam">to_complex64</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"ToComplex64"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t809" class="pln">  <span class="str">"""Casts a tensor to type `complex64`.</span><span class="strut">&nbsp;</span></p>
<p id="t810" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t811" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t812" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span><span class="strut">&nbsp;</span></p>
<p id="t813" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t814" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t815" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t816" class="pln"><span class="str">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span><span class="strut">&nbsp;</span></p>
<p id="t817" class="pln"><span class="str">    type `complex64`.</span><span class="strut">&nbsp;</span></p>
<p id="t818" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t819" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t820" class="pln"><span class="str">    TypeError: If `x` cannot be cast to the `complex64`.</span><span class="strut">&nbsp;</span></p>
<p id="t821" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t822" class="stm mis">  <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">complex64</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t823" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t824" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t825" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"to_complex128"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t826" class="stm run hide_run"><span class="key">def</span> <span class="nam">to_complex128</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"ToComplex128"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t827" class="pln">  <span class="str">"""Casts a tensor to type `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t828" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t829" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t830" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor` or `IndexedSlices`.</span><span class="strut">&nbsp;</span></p>
<p id="t831" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t832" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t833" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t834" class="pln"><span class="str">    A `Tensor` or `SparseTensor` or `IndexedSlices` with same shape as `x` with</span><span class="strut">&nbsp;</span></p>
<p id="t835" class="pln"><span class="str">    type `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t836" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t837" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t838" class="pln"><span class="str">    TypeError: If `x` cannot be cast to the `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t839" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t840" class="stm mis">  <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">complex128</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t841" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t842" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t843" class="stm run hide_run"><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__neg__"</span><span class="op">,</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">neg</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t844" class="stm run hide_run"><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__abs__"</span><span class="op">,</span> <span class="nam">abs</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t845" class="pln"><span class="com"># __invert__ corresponds to the ~ operator.  Here we follow the numpy convention</span><span class="strut">&nbsp;</span></p>
<p id="t846" class="pln"><span class="com"># ~ marks an elementwise bit-wise inverse.  This is only implemented for boolean</span><span class="strut">&nbsp;</span></p>
<p id="t847" class="pln"><span class="com"># tensors and will throw a TypeError if used on nonboolean arrays</span><span class="strut">&nbsp;</span></p>
<p id="t848" class="stm run hide_run"><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__invert__"</span><span class="op">,</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">logical_not</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t849" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t850" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t851" class="stm run hide_run"><span class="key">def</span> <span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">func</span><span class="op">,</span> <span class="nam">op_name</span><span class="op">,</span> <span class="nam">clazz_object</span><span class="op">=</span><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t852" class="pln">  <span class="str">"""Register operators with different tensor and scalar versions.</span><span class="strut">&nbsp;</span></p>
<p id="t853" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t854" class="pln"><span class="str">  If `clazz_object` is `SparseTensor`, assumes `func` takes `(sp_indices,</span><span class="strut">&nbsp;</span></p>
<p id="t855" class="pln"><span class="str">  sp_values, sp_shape, dense)` and outputs `(new_sp_values)`.</span><span class="strut">&nbsp;</span></p>
<p id="t856" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t857" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t858" class="pln"><span class="str">    func: the operator</span><span class="strut">&nbsp;</span></p>
<p id="t859" class="pln"><span class="str">    op_name: name of the operator being overridden</span><span class="strut">&nbsp;</span></p>
<p id="t860" class="pln"><span class="str">    clazz_object: class to override for.  Either `Tensor` or `SparseTensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t861" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t862" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t863" class="stm run hide_run">  <span class="key">def</span> <span class="nam">binary_op_wrapper</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t864" class="stm run hide_run">    <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="key">None</span><span class="op">,</span> <span class="nam">op_name</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t865" class="stm run hide_run">      <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span> <span class="key">and</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t866" class="stm run hide_run">        <span class="key">return</span> <span class="nam">func</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t867" class="stm mis">      <span class="key">elif</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t868" class="stm mis">        <span class="key">try</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t869" class="stm mis">          <span class="nam">y</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"y"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t870" class="stm mis">        <span class="key">except</span> <span class="nam">TypeError</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t871" class="pln">          <span class="com"># If the RHS is not a tensor, it might be a tensor aware object</span><span class="strut">&nbsp;</span></p>
<p id="t872" class="pln">          <span class="com"># that can implement the operator with knowledge of itself</span><span class="strut">&nbsp;</span></p>
<p id="t873" class="pln">          <span class="com"># and the tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t874" class="stm mis">          <span class="key">if</span> <span class="nam">hasattr</span><span class="op">(</span><span class="nam">type</span><span class="op">(</span><span class="nam">y</span><span class="op">)</span><span class="op">,</span> <span class="str">"__r%s__"</span> <span class="op">%</span> <span class="nam">op_name</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t875" class="stm mis">            <span class="key">return</span> <span class="nam">NotImplemented</span><span class="strut">&nbsp;</span></p>
<p id="t876" class="pln">          <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t877" class="stm mis">            <span class="key">raise</span><span class="strut">&nbsp;</span></p>
<p id="t878" class="stm mis">      <span class="key">return</span> <span class="nam">func</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t879" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t880" class="stm run hide_run">  <span class="key">def</span> <span class="nam">binary_op_wrapper_sparse</span><span class="op">(</span><span class="nam">sp_x</span><span class="op">,</span> <span class="nam">y</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t881" class="stm mis">    <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="key">None</span><span class="op">,</span> <span class="nam">op_name</span><span class="op">,</span> <span class="op">[</span><span class="nam">sp_x</span><span class="op">,</span> <span class="nam">y</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t882" class="stm mis">      <span class="nam">y</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">sp_x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"y"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t883" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="nam">sp_x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t884" class="pln">                                        <span class="nam">func</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t885" class="pln">                                            <span class="nam">sp_x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t886" class="pln">                                            <span class="nam">sp_x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t887" class="pln">                                            <span class="nam">sp_x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t888" class="pln">                                            <span class="nam">y</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t889" class="pln">                                            <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="op">,</span> <span class="nam">sp_x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t890" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t891" class="stm run hide_run">  <span class="key">def</span> <span class="nam">r_binary_op_wrapper</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">x</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t892" class="stm mis">    <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="key">None</span><span class="op">,</span> <span class="nam">op_name</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t893" class="stm mis">      <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">y</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t894" class="stm mis">      <span class="key">return</span> <span class="nam">func</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t895" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t896" class="pln">  <span class="com"># Propagate func.__doc__ to the wrappers</span><span class="strut">&nbsp;</span></p>
<p id="t897" class="stm run hide_run">  <span class="key">try</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t898" class="stm run hide_run">    <span class="nam">doc</span> <span class="op">=</span> <span class="nam">func</span><span class="op">.</span><span class="nam">__doc__</span><span class="strut">&nbsp;</span></p>
<p id="t899" class="stm mis">  <span class="key">except</span> <span class="nam">AttributeError</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t900" class="stm mis">    <span class="nam">doc</span> <span class="op">=</span> <span class="key">None</span><span class="strut">&nbsp;</span></p>
<p id="t901" class="stm run hide_run">  <span class="nam">binary_op_wrapper</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">=</span> <span class="nam">doc</span><span class="strut">&nbsp;</span></p>
<p id="t902" class="stm run hide_run">  <span class="nam">r_binary_op_wrapper</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">=</span> <span class="nam">doc</span><span class="strut">&nbsp;</span></p>
<p id="t903" class="stm run hide_run">  <span class="nam">binary_op_wrapper_sparse</span><span class="op">.</span><span class="nam">__doc__</span> <span class="op">=</span> <span class="nam">doc</span><span class="strut">&nbsp;</span></p>
<p id="t904" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t905" class="stm run hide_run">  <span class="key">if</span> <span class="nam">clazz_object</span> <span class="key">is</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t906" class="stm run hide_run">    <span class="nam">clazz_object</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__%s__"</span> <span class="op">%</span> <span class="nam">op_name</span><span class="op">,</span> <span class="nam">binary_op_wrapper</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t907" class="stm run hide_run">    <span class="key">del</span> <span class="nam">binary_op_wrapper</span><span class="strut">&nbsp;</span></p>
<p id="t908" class="stm run hide_run">    <span class="nam">clazz_object</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__r%s__"</span> <span class="op">%</span> <span class="nam">op_name</span><span class="op">,</span> <span class="nam">r_binary_op_wrapper</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t909" class="stm run hide_run">    <span class="key">del</span> <span class="nam">r_binary_op_wrapper</span><span class="strut">&nbsp;</span></p>
<p id="t910" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t911" class="stm run hide_run">    <span class="nam">clazz_object</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__%s__"</span> <span class="op">%</span> <span class="nam">op_name</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t912" class="pln">                                    <span class="nam">binary_op_wrapper_sparse</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t913" class="stm run hide_run">    <span class="key">del</span> <span class="nam">binary_op_wrapper_sparse</span><span class="strut">&nbsp;</span></p>
<p id="t914" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t915" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t916" class="pln"><span class="com"># Conversion table for __truediv__.  None entries mean no conversion required.</span><span class="strut">&nbsp;</span></p>
<p id="t917" class="stm run hide_run"><span class="nam">_TRUEDIV_TABLE</span> <span class="op">=</span> <span class="op">{</span><span class="strut">&nbsp;</span></p>
<p id="t918" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">uint8</span><span class="op">:</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t919" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int8</span><span class="op">:</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t920" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">uint16</span><span class="op">:</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t921" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int16</span><span class="op">:</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t922" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">:</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float64</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t923" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int64</span><span class="op">:</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float64</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t924" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">bfloat16</span><span class="op">:</span> <span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t925" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float16</span><span class="op">:</span> <span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t926" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">:</span> <span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t927" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float64</span><span class="op">:</span> <span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t928" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">complex64</span><span class="op">:</span> <span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t929" class="pln">    <span class="nam">dtypes</span><span class="op">.</span><span class="nam">complex128</span><span class="op">:</span> <span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t930" class="pln"><span class="op">}</span><span class="strut">&nbsp;</span></p>
<p id="t931" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t932" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t933" class="pln"><span class="com"># NOTE: the support of "sparse (true)div dense" is currently not baked in into</span><span class="strut">&nbsp;</span></p>
<p id="t934" class="pln"><span class="com"># "tf.(true_)div()".  Until such an API decision is made, the supported usage is</span><span class="strut">&nbsp;</span></p>
<p id="t935" class="pln"><span class="com"># to explicitly use the "/" operator to invoke either truediv or div.</span><span class="strut">&nbsp;</span></p>
<p id="t936" class="stm run hide_run"><span class="key">def</span> <span class="nam">_sparse_dense_truediv</span><span class="op">(</span><span class="nam">sp_indices</span><span class="op">,</span> <span class="nam">sp_values</span><span class="op">,</span> <span class="nam">sp_shape</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t937" class="pln">  <span class="str">"""Internal helper function for 'sp_t / dense_t'."""</span><span class="strut">&nbsp;</span></p>
<p id="t938" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"truediv"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t939" class="pln">                      <span class="op">[</span><span class="nam">sp_indices</span><span class="op">,</span> <span class="nam">sp_values</span><span class="op">,</span> <span class="nam">sp_shape</span><span class="op">,</span> <span class="nam">y</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t940" class="stm mis">    <span class="nam">sp_values</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">sp_values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"sp_values"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t941" class="stm mis">    <span class="nam">y</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"y"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t942" class="stm mis">    <span class="nam">x_dtype</span> <span class="op">=</span> <span class="nam">sp_values</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t943" class="stm mis">    <span class="nam">y_dtype</span> <span class="op">=</span> <span class="nam">y</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t944" class="stm mis">    <span class="key">if</span> <span class="nam">x_dtype</span> <span class="op">!=</span> <span class="nam">y_dtype</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t945" class="stm mis">      <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"x and y must have the same dtype, got %r != %r"</span> <span class="op">%</span><span class="strut">&nbsp;</span></p>
<p id="t946" class="pln">                      <span class="op">(</span><span class="nam">x_dtype</span><span class="op">,</span> <span class="nam">y_dtype</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t947" class="stm mis">    <span class="key">try</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t948" class="stm mis">      <span class="nam">dtype</span> <span class="op">=</span> <span class="nam">_TRUEDIV_TABLE</span><span class="op">[</span><span class="nam">x_dtype</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t949" class="stm mis">    <span class="key">except</span> <span class="nam">KeyError</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t950" class="stm mis">      <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"Invalid dtype %r in __truediv__"</span> <span class="op">%</span> <span class="nam">x_dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t951" class="stm mis">    <span class="key">if</span> <span class="nam">dtype</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t952" class="stm mis">      <span class="nam">sp_values</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">sp_values</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t953" class="stm mis">      <span class="nam">y</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t954" class="stm mis">    <span class="key">return</span> <span class="nam">gen_sparse_ops</span><span class="op">.</span><span class="nam">sparse_dense_cwise_div</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t955" class="pln">        <span class="nam">sp_indices</span><span class="op">,</span> <span class="nam">sp_values</span><span class="op">,</span> <span class="nam">sp_shape</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t956" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t957" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t958" class="stm run hide_run"><span class="key">def</span> <span class="nam">_truediv_python3</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t959" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"truediv"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t960" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t961" class="stm mis">    <span class="nam">y</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"y"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t962" class="stm mis">    <span class="nam">x_dtype</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t963" class="stm mis">    <span class="nam">y_dtype</span> <span class="op">=</span> <span class="nam">y</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t964" class="stm mis">    <span class="key">if</span> <span class="nam">x_dtype</span> <span class="op">!=</span> <span class="nam">y_dtype</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t965" class="stm mis">      <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"x and y must have the same dtype, got %r != %r"</span> <span class="op">%</span><span class="strut">&nbsp;</span></p>
<p id="t966" class="pln">                      <span class="op">(</span><span class="nam">x_dtype</span><span class="op">,</span> <span class="nam">y_dtype</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t967" class="stm mis">    <span class="key">try</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t968" class="stm mis">      <span class="nam">dtype</span> <span class="op">=</span> <span class="nam">_TRUEDIV_TABLE</span><span class="op">[</span><span class="nam">x_dtype</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t969" class="stm mis">    <span class="key">except</span> <span class="nam">KeyError</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t970" class="stm mis">      <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"Invalid dtype %r in __truediv__"</span> <span class="op">%</span> <span class="nam">x_dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t971" class="stm mis">    <span class="key">if</span> <span class="nam">dtype</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t972" class="stm mis">      <span class="nam">x</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t973" class="stm mis">      <span class="nam">y</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t974" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">real_div</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t975" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t976" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t977" class="stm run hide_run"><span class="key">def</span> <span class="nam">_div_python2</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t978" class="pln">  <span class="str">"""Divide two values using Python 2 semantics. Used for Tensor.__div__.</span><span class="strut">&nbsp;</span></p>
<p id="t979" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t980" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t981" class="pln"><span class="str">    x: `Tensor` numerator of real numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t982" class="pln"><span class="str">    y: `Tensor` denominator of real numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t983" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t984" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t985" class="pln"><span class="str">    `x / y` returns the quotient of x and y.</span><span class="strut">&nbsp;</span></p>
<p id="t986" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t987" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t988" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"div"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t989" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t990" class="stm mis">    <span class="nam">y</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"y"</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t991" class="stm mis">    <span class="nam">x_dtype</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t992" class="stm mis">    <span class="nam">y_dtype</span> <span class="op">=</span> <span class="nam">y</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t993" class="stm mis">    <span class="key">if</span> <span class="nam">x_dtype</span> <span class="op">!=</span> <span class="nam">y_dtype</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t994" class="stm mis">      <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"x and y must have the same dtype, got %r != %r"</span> <span class="op">%</span><span class="strut">&nbsp;</span></p>
<p id="t995" class="pln">                      <span class="op">(</span><span class="nam">x_dtype</span><span class="op">,</span> <span class="nam">y_dtype</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t996" class="stm mis">    <span class="key">if</span> <span class="nam">x_dtype</span><span class="op">.</span><span class="nam">is_floating</span> <span class="key">or</span> <span class="nam">x_dtype</span><span class="op">.</span><span class="nam">is_complex</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t997" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">real_div</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t998" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t999" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">floor_div</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1000" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1001" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1002" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.truediv"</span><span class="op">,</span> <span class="str">"truediv"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1003" class="stm run hide_run"><span class="key">def</span> <span class="nam">truediv</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1004" class="pln">  <span class="str">"""Divides x / y elementwise (using Python 3 division operator semantics).</span><span class="strut">&nbsp;</span></p>
<p id="t1005" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1006" class="pln"><span class="str">  NOTE: Prefer using the Tensor operator or tf.divide which obey Python</span><span class="strut">&nbsp;</span></p>
<p id="t1007" class="pln"><span class="str">  division operator semantics.</span><span class="strut">&nbsp;</span></p>
<p id="t1008" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1009" class="pln"><span class="str">  This function forces Python 3 division operator semantics where all integer</span><span class="strut">&nbsp;</span></p>
<p id="t1010" class="pln"><span class="str">  arguments are cast to floating types first.   This op is generated by normal</span><span class="strut">&nbsp;</span></p>
<p id="t1011" class="pln"><span class="str">  `x / y` division in Python 3 and in Python 2.7 with</span><span class="strut">&nbsp;</span></p>
<p id="t1012" class="pln"><span class="str">  `from __future__ import division`.  If you want integer division that rounds</span><span class="strut">&nbsp;</span></p>
<p id="t1013" class="pln"><span class="str">  down, use `x // y` or `tf.math.floordiv`.</span><span class="strut">&nbsp;</span></p>
<p id="t1014" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1015" class="pln"><span class="str">  `x` and `y` must have the same numeric type.  If the inputs are floating</span><span class="strut">&nbsp;</span></p>
<p id="t1016" class="pln"><span class="str">  point, the output will have the same type.  If the inputs are integral, the</span><span class="strut">&nbsp;</span></p>
<p id="t1017" class="pln"><span class="str">  inputs are cast to `float32` for `int8` and `int16` and `float64` for `int32`</span><span class="strut">&nbsp;</span></p>
<p id="t1018" class="pln"><span class="str">  and `int64` (matching the behavior of Numpy).</span><span class="strut">&nbsp;</span></p>
<p id="t1019" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1020" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1021" class="pln"><span class="str">    x: `Tensor` numerator of numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1022" class="pln"><span class="str">    y: `Tensor` denominator of numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1023" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1024" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1025" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1026" class="pln"><span class="str">    `x / y` evaluated in floating point.</span><span class="strut">&nbsp;</span></p>
<p id="t1027" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1028" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t1029" class="pln"><span class="str">    TypeError: If `x` and `y` have different dtypes.</span><span class="strut">&nbsp;</span></p>
<p id="t1030" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1031" class="stm mis">  <span class="key">return</span> <span class="nam">_truediv_python3</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1032" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1033" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1034" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"div"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1035" class="stm run hide_run"><span class="key">def</span> <span class="nam">div</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1036" class="pln">  <span class="str">"""Divides x / y elementwise (using Python 2 division operator semantics).</span><span class="strut">&nbsp;</span></p>
<p id="t1037" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1038" class="pln"><span class="str">  NOTE: Prefer using the Tensor division operator or tf.divide which obey Python</span><span class="strut">&nbsp;</span></p>
<p id="t1039" class="pln"><span class="str">  division operator semantics.</span><span class="strut">&nbsp;</span></p>
<p id="t1040" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1041" class="pln"><span class="str">  This function divides `x` and `y`, forcing Python 2.7 semantics. That is,</span><span class="strut">&nbsp;</span></p>
<p id="t1042" class="pln"><span class="str">  if one of `x` or `y` is a float, then the result will be a float.</span><span class="strut">&nbsp;</span></p>
<p id="t1043" class="pln"><span class="str">  Otherwise, the output will be an integer type. Flooring semantics are used</span><span class="strut">&nbsp;</span></p>
<p id="t1044" class="pln"><span class="str">  for integer division.</span><span class="strut">&nbsp;</span></p>
<p id="t1045" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1046" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1047" class="pln"><span class="str">    x: `Tensor` numerator of real numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1048" class="pln"><span class="str">    y: `Tensor` denominator of real numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1049" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1050" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1051" class="pln"><span class="str">    `x / y` returns the quotient of x and y.</span><span class="strut">&nbsp;</span></p>
<p id="t1052" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1053" class="stm mis">  <span class="key">return</span> <span class="nam">_div_python2</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1054" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1055" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1056" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"div_no_nan"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1057" class="stm run hide_run"><span class="key">def</span> <span class="nam">div_no_nan</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1058" class="pln">  <span class="str">"""Computes an unsafe divide which returns 0 if the y is zero.</span><span class="strut">&nbsp;</span></p>
<p id="t1059" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1060" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1061" class="pln"><span class="str">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t1062" class="pln"><span class="str">    y: A `Tensor` whose dtype is compatible with `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t1063" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1064" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1065" class="pln"><span class="str">    The element-wise value of the x divided by y.</span><span class="strut">&nbsp;</span></p>
<p id="t1066" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1067" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1068" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"div_no_nan"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1069" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1070" class="stm mis">    <span class="nam">y</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"y"</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1071" class="stm mis">    <span class="nam">x_dtype</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t1072" class="stm mis">    <span class="nam">y_dtype</span> <span class="op">=</span> <span class="nam">y</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">base_dtype</span><span class="strut">&nbsp;</span></p>
<p id="t1073" class="stm mis">    <span class="key">if</span> <span class="nam">x_dtype</span> <span class="op">!=</span> <span class="nam">y_dtype</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1074" class="stm mis">      <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"x and y must have the same dtype, got %r != %r"</span> <span class="op">%</span><span class="strut">&nbsp;</span></p>
<p id="t1075" class="pln">                      <span class="op">(</span><span class="nam">x_dtype</span><span class="op">,</span> <span class="nam">y_dtype</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1076" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">div_no_nan</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1077" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1078" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1079" class="pln"><span class="com"># TODO(aselle): This should be removed</span><span class="strut">&nbsp;</span></p>
<p id="t1080" class="stm run hide_run"><span class="nam">mod</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">floor_mod</span><span class="strut">&nbsp;</span></p>
<p id="t1081" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1082" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1083" class="pln"><span class="com"># TODO(aselle): Deprecate this once all internal functionality uses</span><span class="strut">&nbsp;</span></p>
<p id="t1084" class="pln"><span class="com"># tf.truncatediv</span><span class="strut">&nbsp;</span></p>
<p id="t1085" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.floordiv"</span><span class="op">,</span> <span class="str">"floordiv"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1086" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"floordiv"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1087" class="stm run hide_run"><span class="key">def</span> <span class="nam">floordiv</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1088" class="pln">  <span class="str">"""Divides `x / y` elementwise, rounding toward the most negative integer.</span><span class="strut">&nbsp;</span></p>
<p id="t1089" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1090" class="pln"><span class="str">  The same as `tf.div(x,y)` for integers, but uses `tf.floor(tf.div(x,y))` for</span><span class="strut">&nbsp;</span></p>
<p id="t1091" class="pln"><span class="str">  floating point arguments so that the result is always an integer (though</span><span class="strut">&nbsp;</span></p>
<p id="t1092" class="pln"><span class="str">  possibly an integer represented as floating point).  This op is generated by</span><span class="strut">&nbsp;</span></p>
<p id="t1093" class="pln"><span class="str">  `x // y` floor division in Python 3 and in Python 2.7 with</span><span class="strut">&nbsp;</span></p>
<p id="t1094" class="pln"><span class="str">  `from __future__ import division`.</span><span class="strut">&nbsp;</span></p>
<p id="t1095" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1096" class="pln"><span class="str">  `x` and `y` must have the same type, and the result will have the same type</span><span class="strut">&nbsp;</span></p>
<p id="t1097" class="pln"><span class="str">  as well.</span><span class="strut">&nbsp;</span></p>
<p id="t1098" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1099" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1100" class="pln"><span class="str">    x: `Tensor` numerator of real numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1101" class="pln"><span class="str">    y: `Tensor` denominator of real numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1102" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1103" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1104" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1105" class="pln"><span class="str">    `x / y` rounded down.</span><span class="strut">&nbsp;</span></p>
<p id="t1106" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1107" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t1108" class="pln"><span class="str">    TypeError: If the inputs are complex.</span><span class="strut">&nbsp;</span></p>
<p id="t1109" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1110" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"floordiv"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1111" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">floor_div</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1112" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1113" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1114" class="stm run hide_run"><span class="nam">realdiv</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">real_div</span><span class="strut">&nbsp;</span></p>
<p id="t1115" class="stm run hide_run"><span class="nam">tf_export</span><span class="op">(</span><span class="str">"realdiv"</span><span class="op">)</span><span class="op">(</span><span class="nam">realdiv</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1116" class="stm run hide_run"><span class="nam">truncatediv</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">truncate_div</span><span class="strut">&nbsp;</span></p>
<p id="t1117" class="stm run hide_run"><span class="nam">tf_export</span><span class="op">(</span><span class="str">"truncatediv"</span><span class="op">)</span><span class="op">(</span><span class="nam">truncatediv</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1118" class="pln"><span class="com"># TODO(aselle): Rename this to floordiv when we can.</span><span class="strut">&nbsp;</span></p>
<p id="t1119" class="stm run hide_run"><span class="nam">floor_div</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">floor_div</span><span class="strut">&nbsp;</span></p>
<p id="t1120" class="stm run hide_run"><span class="nam">tf_export</span><span class="op">(</span><span class="str">"floor_div"</span><span class="op">)</span><span class="op">(</span><span class="nam">floor_div</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1121" class="stm run hide_run"><span class="nam">truncatemod</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">truncate_mod</span><span class="strut">&nbsp;</span></p>
<p id="t1122" class="stm run hide_run"><span class="nam">tf_export</span><span class="op">(</span><span class="str">"truncatemod"</span><span class="op">)</span><span class="op">(</span><span class="nam">truncatemod</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1123" class="stm run hide_run"><span class="nam">floormod</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">floor_mod</span><span class="strut">&nbsp;</span></p>
<p id="t1124" class="stm run hide_run"><span class="nam">tf_export</span><span class="op">(</span><span class="str">"floormod"</span><span class="op">,</span> <span class="str">"mod"</span><span class="op">)</span><span class="op">(</span><span class="nam">floormod</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1125" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1126" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1127" class="stm run hide_run"><span class="key">def</span> <span class="nam">_mul_dispatch</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1128" class="pln">  <span class="str">"""Dispatches cwise mul for "Dense*Dense" and "Dense*Sparse"."""</span><span class="strut">&nbsp;</span></p>
<p id="t1129" class="stm run hide_run">  <span class="nam">is_tensor_y</span> <span class="op">=</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1130" class="stm run hide_run">  <span class="key">if</span> <span class="nam">is_tensor_y</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1131" class="stm run hide_run">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">mul</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1132" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1133" class="stm mis">    <span class="key">assert</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">y</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span>  <span class="com"># Case: Dense * Sparse.</span><span class="strut">&nbsp;</span></p>
<p id="t1134" class="stm mis">    <span class="nam">new_vals</span> <span class="op">=</span> <span class="nam">gen_sparse_ops</span><span class="op">.</span><span class="nam">sparse_dense_cwise_mul</span><span class="op">(</span><span class="nam">y</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">y</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1135" class="pln">                                                     <span class="nam">y</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">,</span> <span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1136" class="stm mis">    <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="nam">y</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">new_vals</span><span class="op">,</span> <span class="nam">y</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1137" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1138" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1139" class="pln"><span class="com"># NOTE(aselle): When integer division is added for sparse_dense_cwise,</span><span class="strut">&nbsp;</span></p>
<p id="t1140" class="pln"><span class="com"># div, truediv, and floordiv should be delegated appropriately for</span><span class="strut">&nbsp;</span></p>
<p id="t1141" class="pln"><span class="com"># Python sematnics, analogous to dense cwise tensor operations.</span><span class="strut">&nbsp;</span></p>
<p id="t1142" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">gen_sparse_ops</span><span class="op">.</span><span class="nam">sparse_dense_cwise_div</span><span class="op">,</span> <span class="str">"div"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1143" class="pln">                              <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1144" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">_sparse_dense_truediv</span><span class="op">,</span> <span class="str">"truediv"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1145" class="pln">                              <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1146" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">gen_sparse_ops</span><span class="op">.</span><span class="nam">sparse_dense_cwise_mul</span><span class="op">,</span> <span class="str">"mul"</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1147" class="pln">                              <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1148" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1149" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">add</span><span class="op">,</span> <span class="str">"add"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1150" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sub</span><span class="op">,</span> <span class="str">"sub"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1151" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">_mul_dispatch</span><span class="op">,</span> <span class="str">"mul"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1152" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">_div_python2</span><span class="op">,</span> <span class="str">"div"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1153" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">_truediv_python3</span><span class="op">,</span> <span class="str">"truediv"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1154" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">floordiv</span><span class="op">,</span> <span class="str">"floordiv"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1155" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">floor_mod</span><span class="op">,</span> <span class="str">"mod"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1156" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">pow</span><span class="op">,</span> <span class="str">"pow"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1157" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1158" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1159" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.logical_xor"</span><span class="op">,</span> <span class="str">"logical_xor"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1160" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"logical_xor"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1161" class="stm run hide_run"><span class="key">def</span> <span class="nam">logical_xor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"LogicalXor"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1162" class="pln">  <span class="str">"""x ^ y = (x | y) &amp; ~(x &amp; y)."""</span><span class="strut">&nbsp;</span></p>
<p id="t1163" class="pln">  <span class="com"># TODO(alemi) Make this a cwise op if people end up relying on it.</span><span class="strut">&nbsp;</span></p>
<p id="t1164" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">logical_and</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1165" class="pln">      <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">logical_or</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1166" class="pln">      <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">logical_not</span><span class="op">(</span><span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">logical_and</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">y</span><span class="op">)</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1167" class="pln">      <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1168" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1169" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1170" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">logical_and</span><span class="op">,</span> <span class="str">"and"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1171" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">logical_or</span><span class="op">,</span> <span class="str">"or"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1172" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">logical_xor</span><span class="op">,</span> <span class="str">"xor"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1173" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1174" class="stm run hide_run"><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__lt__"</span><span class="op">,</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">less</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1175" class="stm run hide_run"><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__le__"</span><span class="op">,</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">less_equal</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1176" class="stm run hide_run"><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__gt__"</span><span class="op">,</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">greater</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1177" class="stm run hide_run"><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">.</span><span class="nam">_override_operator</span><span class="op">(</span><span class="str">"__ge__"</span><span class="op">,</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">greater_equal</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1178" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1179" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1180" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"range"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1181" class="stm run hide_run"><span class="key">def</span> <span class="nam">range</span><span class="op">(</span><span class="nam">start</span><span class="op">,</span> <span class="nam">limit</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">delta</span><span class="op">=</span><span class="num">1</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"range"</span><span class="op">)</span><span class="op">:</span>  <span class="com"># pylint: disable=redefined-builtin</span><span class="strut">&nbsp;</span></p>
<p id="t1182" class="pln">  <span class="str">"""Creates a sequence of numbers.</span><span class="strut">&nbsp;</span></p>
<p id="t1183" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1184" class="pln"><span class="str">  Creates a sequence of numbers that begins at `start` and extends by</span><span class="strut">&nbsp;</span></p>
<p id="t1185" class="pln"><span class="str">  increments of `delta` up to but not including `limit`.</span><span class="strut">&nbsp;</span></p>
<p id="t1186" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1187" class="pln"><span class="str">  The dtype of the resulting tensor is inferred from the inputs unless</span><span class="strut">&nbsp;</span></p>
<p id="t1188" class="pln"><span class="str">  it is provided explicitly.</span><span class="strut">&nbsp;</span></p>
<p id="t1189" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1190" class="pln"><span class="str">  Like the Python builtin `range`, `start` defaults to 0, so that</span><span class="strut">&nbsp;</span></p>
<p id="t1191" class="pln"><span class="str">  `range(n) = range(0, n)`.</span><span class="strut">&nbsp;</span></p>
<p id="t1192" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1193" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1194" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1195" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1196" class="pln"><span class="str">  start = 3</span><span class="strut">&nbsp;</span></p>
<p id="t1197" class="pln"><span class="str">  limit = 18</span><span class="strut">&nbsp;</span></p>
<p id="t1198" class="pln"><span class="str">  delta = 3</span><span class="strut">&nbsp;</span></p>
<p id="t1199" class="pln"><span class="str">  tf.range(start, limit, delta)  # [3, 6, 9, 12, 15]</span><span class="strut">&nbsp;</span></p>
<p id="t1200" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1201" class="pln"><span class="str">  start = 3</span><span class="strut">&nbsp;</span></p>
<p id="t1202" class="pln"><span class="str">  limit = 1</span><span class="strut">&nbsp;</span></p>
<p id="t1203" class="pln"><span class="str">  delta = -0.5</span><span class="strut">&nbsp;</span></p>
<p id="t1204" class="pln"><span class="str">  tf.range(start, limit, delta)  # [3, 2.5, 2, 1.5]</span><span class="strut">&nbsp;</span></p>
<p id="t1205" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1206" class="pln"><span class="str">  limit = 5</span><span class="strut">&nbsp;</span></p>
<p id="t1207" class="pln"><span class="str">  tf.range(limit)  # [0, 1, 2, 3, 4]</span><span class="strut">&nbsp;</span></p>
<p id="t1208" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1209" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1210" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1211" class="pln"><span class="str">    start: A 0-D `Tensor` (scalar). Acts as first entry in the range if</span><span class="strut">&nbsp;</span></p>
<p id="t1212" class="pln"><span class="str">      `limit` is not None; otherwise, acts as range limit and first entry</span><span class="strut">&nbsp;</span></p>
<p id="t1213" class="pln"><span class="str">      defaults to 0.</span><span class="strut">&nbsp;</span></p>
<p id="t1214" class="pln"><span class="str">    limit: A 0-D `Tensor` (scalar). Upper limit of sequence,</span><span class="strut">&nbsp;</span></p>
<p id="t1215" class="pln"><span class="str">      exclusive. If None, defaults to the value of `start` while the first</span><span class="strut">&nbsp;</span></p>
<p id="t1216" class="pln"><span class="str">      entry of the range defaults to 0.</span><span class="strut">&nbsp;</span></p>
<p id="t1217" class="pln"><span class="str">    delta: A 0-D `Tensor` (scalar). Number that increments</span><span class="strut">&nbsp;</span></p>
<p id="t1218" class="pln"><span class="str">      `start`. Defaults to 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1219" class="pln"><span class="str">    dtype: The type of the elements of the resulting tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1220" class="pln"><span class="str">    name: A name for the operation. Defaults to "range".</span><span class="strut">&nbsp;</span></p>
<p id="t1221" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1222" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1223" class="pln"><span class="str">    An 1-D `Tensor` of type `dtype`.</span><span class="strut">&nbsp;</span></p>
<p id="t1224" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1225" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t1226" class="pln"><span class="str">  Equivalent to np.arange</span><span class="strut">&nbsp;</span></p>
<p id="t1227" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t1228" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1229" class="stm mis">  <span class="key">if</span> <span class="nam">limit</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1230" class="stm mis">    <span class="nam">start</span><span class="op">,</span> <span class="nam">limit</span> <span class="op">=</span> <span class="num">0</span><span class="op">,</span> <span class="nam">start</span><span class="strut">&nbsp;</span></p>
<p id="t1231" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1232" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Range"</span><span class="op">,</span> <span class="op">[</span><span class="nam">start</span><span class="op">,</span> <span class="nam">limit</span><span class="op">,</span> <span class="nam">delta</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1233" class="stm mis">    <span class="nam">start</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">start</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"start"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1234" class="stm mis">    <span class="nam">limit</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">limit</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"limit"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1235" class="stm mis">    <span class="nam">delta</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">delta</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtype</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"delta"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1236" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1237" class="pln">    <span class="com"># infer dtype if not explicitly provided</span><span class="strut">&nbsp;</span></p>
<p id="t1238" class="stm mis">    <span class="key">if</span> <span class="nam">dtype</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1239" class="stm mis">      <span class="nam">dtype_hierarchy</span> <span class="op">=</span> <span class="op">[</span><span class="strut">&nbsp;</span></p>
<p id="t1240" class="pln">          <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int64</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float64</span><span class="strut">&nbsp;</span></p>
<p id="t1241" class="pln">      <span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t1242" class="stm mis">      <span class="key">assert</span> <span class="nam">all</span><span class="op">(</span><span class="nam">arg</span><span class="op">.</span><span class="nam">dtype</span> <span class="key">in</span> <span class="nam">dtype_hierarchy</span> <span class="key">for</span> <span class="nam">arg</span> <span class="key">in</span> <span class="op">[</span><span class="nam">start</span><span class="op">,</span> <span class="nam">limit</span><span class="op">,</span> <span class="nam">delta</span><span class="op">]</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1243" class="stm mis">      <span class="nam">inferred_dtype</span> <span class="op">=</span> <span class="nam">max</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1244" class="pln">          <span class="op">[</span><span class="nam">arg</span><span class="op">.</span><span class="nam">dtype</span> <span class="key">for</span> <span class="nam">arg</span> <span class="key">in</span> <span class="op">[</span><span class="nam">start</span><span class="op">,</span> <span class="nam">limit</span><span class="op">,</span> <span class="nam">delta</span><span class="op">]</span><span class="op">]</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1245" class="pln">          <span class="nam">key</span><span class="op">=</span><span class="nam">dtype_hierarchy</span><span class="op">.</span><span class="nam">index</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1246" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1247" class="stm mis">      <span class="nam">start</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">start</span><span class="op">,</span> <span class="nam">inferred_dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1248" class="stm mis">      <span class="nam">limit</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">limit</span><span class="op">,</span> <span class="nam">inferred_dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1249" class="stm mis">      <span class="nam">delta</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">delta</span><span class="op">,</span> <span class="nam">inferred_dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1250" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1251" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_range</span><span class="op">(</span><span class="nam">start</span><span class="op">,</span> <span class="nam">limit</span><span class="op">,</span> <span class="nam">delta</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1252" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1253" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1254" class="pln"><span class="com"># Reduction operations</span><span class="strut">&nbsp;</span></p>
<p id="t1255" class="stm run hide_run"><span class="key">def</span> <span class="nam">_ReductionDims</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1256" class="pln">  <span class="str">"""Returns range(0, rank(x)) if reduction_indices is None."""</span><span class="strut">&nbsp;</span></p>
<p id="t1257" class="pln">  <span class="com"># TODO(aselle): Remove this after deprecation</span><span class="strut">&nbsp;</span></p>
<p id="t1258" class="stm mis">  <span class="key">if</span> <span class="nam">reduction_indices</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1259" class="stm mis">    <span class="key">if</span> <span class="nam">axis</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1260" class="stm mis">      <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Can't specify both axis' and 'reduction_indices'."</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1261" class="stm mis">    <span class="nam">axis</span> <span class="op">=</span> <span class="nam">reduction_indices</span><span class="strut">&nbsp;</span></p>
<p id="t1262" class="stm mis">  <span class="key">if</span> <span class="nam">axis</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1263" class="stm mis">    <span class="key">return</span> <span class="nam">axis</span><span class="strut">&nbsp;</span></p>
<p id="t1264" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1265" class="pln">    <span class="com"># Fast path: avoid creating Rank and Range ops if ndims is known.</span><span class="strut">&nbsp;</span></p>
<p id="t1266" class="stm mis">    <span class="nam">rank</span> <span class="op">=</span> <span class="nam">common_shapes</span><span class="op">.</span><span class="nam">rank</span><span class="op">(</span><span class="nam">x</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1267" class="stm mis">    <span class="key">if</span> <span class="nam">rank</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1268" class="stm mis">      <span class="key">return</span> <span class="nam">constant_op</span><span class="op">.</span><span class="nam">constant</span><span class="op">(</span><span class="nam">np</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">rank</span><span class="op">)</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1269" class="stm mis">    <span class="key">if</span> <span class="op">(</span><span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span> <span class="key">and</span><span class="strut">&nbsp;</span></p>
<p id="t1270" class="pln">        <span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">is_fully_defined</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1271" class="stm mis">      <span class="nam">rank</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">value</span>  <span class="com"># sparse.dense_shape is 1-D.</span><span class="strut">&nbsp;</span></p>
<p id="t1272" class="stm mis">      <span class="key">return</span> <span class="nam">constant_op</span><span class="op">.</span><span class="nam">constant</span><span class="op">(</span><span class="nam">np</span><span class="op">.</span><span class="nam">arange</span><span class="op">(</span><span class="nam">rank</span><span class="op">)</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1273" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1274" class="pln">    <span class="com"># Otherwise, we rely on Range and Rank to do the right thing at run-time.</span><span class="strut">&nbsp;</span></p>
<p id="t1275" class="stm mis">    <span class="key">return</span> <span class="nam">range</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">rank</span><span class="op">(</span><span class="nam">x</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1276" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1277" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1278" class="stm run hide_run"><span class="key">def</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span> <span class="nam">output</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1279" class="pln">  <span class="str">"""Set a reduction's output shape to be a scalar if we are certain."""</span><span class="strut">&nbsp;</span></p>
<p id="t1280" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">common_shapes</span><span class="op">.</span><span class="nam">has_fully_defined_shape</span><span class="op">(</span><span class="nam">output</span><span class="op">)</span> <span class="key">and</span> <span class="op">(</span><span class="key">not</span> <span class="nam">keepdims</span><span class="op">)</span> <span class="key">and</span> <span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1281" class="pln">      <span class="nam">axis</span> <span class="key">is</span> <span class="key">None</span><span class="op">)</span> <span class="key">and</span> <span class="op">(</span><span class="nam">reduction_indices</span> <span class="key">is</span> <span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1282" class="stm mis">    <span class="nam">output</span><span class="op">.</span><span class="nam">set_shape</span><span class="op">(</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1283" class="stm mis">  <span class="key">return</span> <span class="nam">output</span><span class="strut">&nbsp;</span></p>
<p id="t1284" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1285" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1286" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.reduce_sum"</span><span class="op">,</span> <span class="str">"reduce_sum"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1287" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1288" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1289" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduce_sum</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1290" class="pln">               <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1291" class="pln">               <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1292" class="pln">               <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1293" class="pln">               <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1294" class="pln">               <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1295" class="pln">  <span class="str">"""Computes the sum of elements across dimensions of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1296" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1297" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1298" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1299" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1300" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1301" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1302" class="pln"><span class="str">  If `axis` is None, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1303" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1304" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1305" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1306" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1307" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1308" class="pln"><span class="str">  x = tf.constant([[1, 1, 1], [1, 1, 1]])</span><span class="strut">&nbsp;</span></p>
<p id="t1309" class="pln"><span class="str">  tf.reduce_sum(x)  # 6</span><span class="strut">&nbsp;</span></p>
<p id="t1310" class="pln"><span class="str">  tf.reduce_sum(x, 0)  # [2, 2, 2]</span><span class="strut">&nbsp;</span></p>
<p id="t1311" class="pln"><span class="str">  tf.reduce_sum(x, 1)  # [3, 3]</span><span class="strut">&nbsp;</span></p>
<p id="t1312" class="pln"><span class="str">  tf.reduce_sum(x, 1, keepdims=True)  # [[3], [3]]</span><span class="strut">&nbsp;</span></p>
<p id="t1313" class="pln"><span class="str">  tf.reduce_sum(x, [0, 1])  # 6</span><span class="strut">&nbsp;</span></p>
<p id="t1314" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1315" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1316" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1317" class="pln"><span class="str">    input_tensor: The tensor to reduce. Should have numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1318" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1319" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1320" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1321" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1322" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1323" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1324" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1325" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1326" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1327" class="pln"><span class="str">    The reduced tensor, of the same dtype as the input_tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1328" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1329" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t1330" class="pln"><span class="str">  Equivalent to np.sum apart the fact that numpy upcast uint8 and int32 to</span><span class="strut">&nbsp;</span></p>
<p id="t1331" class="pln"><span class="str">  int64 while tensorflow returns the same dtype as the input.</span><span class="strut">&nbsp;</span></p>
<p id="t1332" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t1333" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1334" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1335" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1336" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1337" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1338" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1339" class="stm mis">  <span class="key">return</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1340" class="pln">                               <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_sum</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1341" class="pln">                                   <span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1342" class="pln">                                   <span class="nam">_ReductionDims</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1343" class="pln">                                                  <span class="nam">reduction_indices</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1344" class="pln">                                   <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1345" class="pln">                                   <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1346" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1347" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1348" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.count_nonzero"</span><span class="op">,</span> <span class="str">"count_nonzero"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1349" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1350" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1351" class="stm run hide_run"><span class="key">def</span> <span class="nam">count_nonzero</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1352" class="pln">                  <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1353" class="pln">                  <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1354" class="pln">                  <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int64</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1355" class="pln">                  <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1356" class="pln">                  <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1357" class="pln">                  <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1358" class="pln">  <span class="str">"""Computes number of nonzero elements across dimensions of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1359" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1360" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1361" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1362" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1363" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1364" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1365" class="pln"><span class="str">  If `axis` has no entries, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1366" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1367" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1368" class="pln"><span class="str">  **NOTE** Floating point comparison to zero is done by exact floating point</span><span class="strut">&nbsp;</span></p>
<p id="t1369" class="pln"><span class="str">  equality check.  Small values are **not** rounded to zero for purposes of</span><span class="strut">&nbsp;</span></p>
<p id="t1370" class="pln"><span class="str">  the nonzero check.</span><span class="strut">&nbsp;</span></p>
<p id="t1371" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1372" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1373" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1374" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1375" class="pln"><span class="str">  x = tf.constant([[0, 1, 0], [1, 1, 0]])</span><span class="strut">&nbsp;</span></p>
<p id="t1376" class="pln"><span class="str">  tf.count_nonzero(x)  # 3</span><span class="strut">&nbsp;</span></p>
<p id="t1377" class="pln"><span class="str">  tf.count_nonzero(x, 0)  # [1, 2, 0]</span><span class="strut">&nbsp;</span></p>
<p id="t1378" class="pln"><span class="str">  tf.count_nonzero(x, 1)  # [1, 2]</span><span class="strut">&nbsp;</span></p>
<p id="t1379" class="pln"><span class="str">  tf.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]</span><span class="strut">&nbsp;</span></p>
<p id="t1380" class="pln"><span class="str">  tf.count_nonzero(x, [0, 1])  # 3</span><span class="strut">&nbsp;</span></p>
<p id="t1381" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1382" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1383" class="pln"><span class="str">  **NOTE** Strings are compared against zero-length empty string `""`. Any</span><span class="strut">&nbsp;</span></p>
<p id="t1384" class="pln"><span class="str">  string with a size greater than zero is already considered as nonzero.</span><span class="strut">&nbsp;</span></p>
<p id="t1385" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1386" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1387" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1388" class="pln"><span class="str">  x = tf.constant(["", "a", "  ", "b", ""])</span><span class="strut">&nbsp;</span></p>
<p id="t1389" class="pln"><span class="str">  tf.count_nonzero(x) # 3, with "a", "  ", and "b" as nonzero strings.</span><span class="strut">&nbsp;</span></p>
<p id="t1390" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1391" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1392" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1393" class="pln"><span class="str">    input_tensor: The tensor to reduce. Should be of numeric type, `bool`,</span><span class="strut">&nbsp;</span></p>
<p id="t1394" class="pln"><span class="str">      or `string`.</span><span class="strut">&nbsp;</span></p>
<p id="t1395" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1396" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1397" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1398" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1399" class="pln"><span class="str">    dtype: The output dtype; defaults to `tf.int64`.</span><span class="strut">&nbsp;</span></p>
<p id="t1400" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1401" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1402" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1403" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1404" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1405" class="pln"><span class="str">    The reduced tensor (number of nonzero values).</span><span class="strut">&nbsp;</span></p>
<p id="t1406" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1407" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1408" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1409" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1410" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1411" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1412" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"count_nonzero"</span><span class="op">,</span> <span class="op">[</span><span class="nam">input_tensor</span><span class="op">]</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1413" class="stm mis">    <span class="nam">input_tensor</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"input_tensor"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1414" class="pln">    <span class="com"># A scalar of 'zero' is enough as `not_equal` will broadcast.</span><span class="strut">&nbsp;</span></p>
<p id="t1415" class="stm mis">    <span class="nam">zero</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">zeros</span><span class="op">(</span><span class="op">[</span><span class="op">]</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">input_tensor</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1416" class="stm mis">    <span class="key">return</span> <span class="nam">cast</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1417" class="pln">        <span class="nam">reduce_sum</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1418" class="pln">            <span class="com"># int64 reduction happens on GPU</span><span class="strut">&nbsp;</span></p>
<p id="t1419" class="pln">            <span class="nam">to_int64</span><span class="op">(</span><span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">not_equal</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">zero</span><span class="op">)</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1420" class="pln">            <span class="nam">axis</span><span class="op">=</span><span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1421" class="pln">            <span class="nam">keepdims</span><span class="op">=</span><span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1422" class="pln">            <span class="nam">reduction_indices</span><span class="op">=</span><span class="nam">reduction_indices</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1423" class="pln">        <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1424" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1425" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1426" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.reduce_mean"</span><span class="op">,</span> <span class="str">"reduce_mean"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1427" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1428" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1429" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduce_mean</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1430" class="pln">                <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1431" class="pln">                <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1432" class="pln">                <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1433" class="pln">                <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1434" class="pln">                <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1435" class="pln">  <span class="str">"""Computes the mean of elements across dimensions of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1436" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1437" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1438" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1439" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1440" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1441" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1442" class="pln"><span class="str">  If `axis` is None, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1443" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1444" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1445" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1446" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1447" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1448" class="pln"><span class="str">  x = tf.constant([[1., 1.], [2., 2.]])</span><span class="strut">&nbsp;</span></p>
<p id="t1449" class="pln"><span class="str">  tf.reduce_mean(x)  # 1.5</span><span class="strut">&nbsp;</span></p>
<p id="t1450" class="pln"><span class="str">  tf.reduce_mean(x, 0)  # [1.5, 1.5]</span><span class="strut">&nbsp;</span></p>
<p id="t1451" class="pln"><span class="str">  tf.reduce_mean(x, 1)  # [1.,  2.]</span><span class="strut">&nbsp;</span></p>
<p id="t1452" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1453" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1454" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1455" class="pln"><span class="str">    input_tensor: The tensor to reduce. Should have numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1456" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1457" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1458" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1459" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1460" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1461" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1462" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1463" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1464" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1465" class="pln"><span class="str">    The reduced tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1466" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1467" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t1468" class="pln"><span class="str">  Equivalent to np.mean</span><span class="strut">&nbsp;</span></p>
<p id="t1469" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1470" class="pln"><span class="str">  Please note that `np.mean` has a `dtype` parameter that could be used to</span><span class="strut">&nbsp;</span></p>
<p id="t1471" class="pln"><span class="str">  specify the output type. By default this is `dtype=float64`. On the other</span><span class="strut">&nbsp;</span></p>
<p id="t1472" class="pln"><span class="str">  hand, `tf.reduce_mean` has an aggressive type inference from `input_tensor`,</span><span class="strut">&nbsp;</span></p>
<p id="t1473" class="pln"><span class="str">  for example:</span><span class="strut">&nbsp;</span></p>
<p id="t1474" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1475" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1476" class="pln"><span class="str">  x = tf.constant([1, 0, 1, 0])</span><span class="strut">&nbsp;</span></p>
<p id="t1477" class="pln"><span class="str">  tf.reduce_mean(x)  # 0</span><span class="strut">&nbsp;</span></p>
<p id="t1478" class="pln"><span class="str">  y = tf.constant([1., 0., 1., 0.])</span><span class="strut">&nbsp;</span></p>
<p id="t1479" class="pln"><span class="str">  tf.reduce_mean(y)  # 0.5</span><span class="strut">&nbsp;</span></p>
<p id="t1480" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1481" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1482" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t1483" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1484" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1485" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1486" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1487" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1488" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1489" class="stm mis">  <span class="key">return</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1490" class="pln">                               <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">mean</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1491" class="pln">                                   <span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1492" class="pln">                                   <span class="nam">_ReductionDims</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1493" class="pln">                                                  <span class="nam">reduction_indices</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1494" class="pln">                                   <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1495" class="pln">                                   <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1496" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1497" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1498" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.reduce_prod"</span><span class="op">,</span> <span class="str">"reduce_prod"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1499" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1500" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1501" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduce_prod</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1502" class="pln">                <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1503" class="pln">                <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1504" class="pln">                <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1505" class="pln">                <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1506" class="pln">                <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1507" class="pln">  <span class="str">"""Computes the product of elements across dimensions of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1508" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1509" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1510" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1511" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1512" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1513" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1514" class="pln"><span class="str">  If `axis` is None, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1515" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1516" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1517" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1518" class="pln"><span class="str">    input_tensor: The tensor to reduce. Should have numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1519" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1520" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1521" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1522" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1523" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1524" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1525" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1526" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1527" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1528" class="pln"><span class="str">    The reduced tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1529" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1530" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t1531" class="pln"><span class="str">  Equivalent to np.prod</span><span class="strut">&nbsp;</span></p>
<p id="t1532" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t1533" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1534" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1535" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1536" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1537" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1538" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1539" class="stm mis">  <span class="key">return</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1540" class="pln">                               <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">prod</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1541" class="pln">                                   <span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1542" class="pln">                                   <span class="nam">_ReductionDims</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1543" class="pln">                                                  <span class="nam">reduction_indices</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1544" class="pln">                                   <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1545" class="pln">                                   <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1546" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1547" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1548" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.reduce_min"</span><span class="op">,</span> <span class="str">"reduce_min"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1549" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1550" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1551" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduce_min</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1552" class="pln">               <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1553" class="pln">               <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1554" class="pln">               <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1555" class="pln">               <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1556" class="pln">               <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1557" class="pln">  <span class="str">"""Computes the minimum of elements across dimensions of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1558" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1559" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1560" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1561" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1562" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1563" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1564" class="pln"><span class="str">  If `axis` is None, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1565" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1566" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1567" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1568" class="pln"><span class="str">    input_tensor: The tensor to reduce. Should have real numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1569" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1570" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1571" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1572" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1573" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1574" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1575" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1576" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1577" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1578" class="pln"><span class="str">    The reduced tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1579" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1580" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t1581" class="pln"><span class="str">  Equivalent to np.min</span><span class="strut">&nbsp;</span></p>
<p id="t1582" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t1583" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1584" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1585" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1586" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1587" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1588" class="stm mis">  <span class="key">return</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1589" class="pln">                               <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_min</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1590" class="pln">                                   <span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1591" class="pln">                                   <span class="nam">_ReductionDims</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1592" class="pln">                                                  <span class="nam">reduction_indices</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1593" class="pln">                                   <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1594" class="pln">                                   <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1595" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1596" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1597" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.reduce_max"</span><span class="op">,</span> <span class="str">"reduce_max"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1598" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1599" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1600" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduce_max</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1601" class="pln">               <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1602" class="pln">               <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1603" class="pln">               <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1604" class="pln">               <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1605" class="pln">               <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1606" class="pln">  <span class="str">"""Computes the maximum of elements across dimensions of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1607" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1608" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1609" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1610" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1611" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1612" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1613" class="pln"><span class="str">  If `axis` is None, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1614" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1615" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1616" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1617" class="pln"><span class="str">    input_tensor: The tensor to reduce. Should have real numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1618" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1619" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1620" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1621" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1622" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1623" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1624" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1625" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1626" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1627" class="pln"><span class="str">    The reduced tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1628" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1629" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t1630" class="pln"><span class="str">  Equivalent to np.max</span><span class="strut">&nbsp;</span></p>
<p id="t1631" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t1632" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1633" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1634" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1635" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1636" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1637" class="stm mis">  <span class="key">return</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1638" class="pln">                               <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_max</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1639" class="pln">                                   <span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1640" class="pln">                                   <span class="nam">_ReductionDims</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1641" class="pln">                                                  <span class="nam">reduction_indices</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1642" class="pln">                                   <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1643" class="pln">                                   <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1644" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1645" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1646" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.reduce_all"</span><span class="op">,</span> <span class="str">"reduce_all"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1647" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1648" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1649" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduce_all</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1650" class="pln">               <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1651" class="pln">               <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1652" class="pln">               <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1653" class="pln">               <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1654" class="pln">               <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1655" class="pln">  <span class="str">"""Computes the "logical and" of elements across dimensions of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1656" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1657" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1658" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1659" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1660" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1661" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1662" class="pln"><span class="str">  If `axis` is None, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1663" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1664" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1665" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1666" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1667" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1668" class="pln"><span class="str">  x = tf.constant([[True,  True], [False, False]])</span><span class="strut">&nbsp;</span></p>
<p id="t1669" class="pln"><span class="str">  tf.reduce_all(x)  # False</span><span class="strut">&nbsp;</span></p>
<p id="t1670" class="pln"><span class="str">  tf.reduce_all(x, 0)  # [False, False]</span><span class="strut">&nbsp;</span></p>
<p id="t1671" class="pln"><span class="str">  tf.reduce_all(x, 1)  # [True, False]</span><span class="strut">&nbsp;</span></p>
<p id="t1672" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1673" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1674" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1675" class="pln"><span class="str">    input_tensor: The boolean tensor to reduce.</span><span class="strut">&nbsp;</span></p>
<p id="t1676" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1677" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1678" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1679" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1680" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1681" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1682" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1683" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1684" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1685" class="pln"><span class="str">    The reduced tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1686" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1687" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t1688" class="pln"><span class="str">  Equivalent to np.all</span><span class="strut">&nbsp;</span></p>
<p id="t1689" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t1690" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1691" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1692" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1693" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1694" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1695" class="stm mis">  <span class="key">return</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1696" class="pln">                               <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_all</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1697" class="pln">                                   <span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1698" class="pln">                                   <span class="nam">_ReductionDims</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1699" class="pln">                                                  <span class="nam">reduction_indices</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1700" class="pln">                                   <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1701" class="pln">                                   <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1702" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1703" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1704" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.reduce_any"</span><span class="op">,</span> <span class="str">"reduce_any"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1705" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1706" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1707" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduce_any</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1708" class="pln">               <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1709" class="pln">               <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1710" class="pln">               <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1711" class="pln">               <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1712" class="pln">               <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1713" class="pln">  <span class="str">"""Computes the "logical or" of elements across dimensions of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1714" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1715" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1716" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1717" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1718" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1719" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1720" class="pln"><span class="str">  If `axis` is None, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1721" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1722" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1723" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1724" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1725" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1726" class="pln"><span class="str">  x = tf.constant([[True,  True], [False, False]])</span><span class="strut">&nbsp;</span></p>
<p id="t1727" class="pln"><span class="str">  tf.reduce_any(x)  # True</span><span class="strut">&nbsp;</span></p>
<p id="t1728" class="pln"><span class="str">  tf.reduce_any(x, 0)  # [True, True]</span><span class="strut">&nbsp;</span></p>
<p id="t1729" class="pln"><span class="str">  tf.reduce_any(x, 1)  # [True, False]</span><span class="strut">&nbsp;</span></p>
<p id="t1730" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1731" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1732" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1733" class="pln"><span class="str">    input_tensor: The boolean tensor to reduce.</span><span class="strut">&nbsp;</span></p>
<p id="t1734" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1735" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1736" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1737" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1738" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1739" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1740" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1741" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1742" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1743" class="pln"><span class="str">    The reduced tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1744" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1745" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t1746" class="pln"><span class="str">  Equivalent to np.any</span><span class="strut">&nbsp;</span></p>
<p id="t1747" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t1748" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1749" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1750" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1751" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1752" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1753" class="stm mis">  <span class="key">return</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1754" class="pln">                               <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">_any</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1755" class="pln">                                   <span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1756" class="pln">                                   <span class="nam">_ReductionDims</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1757" class="pln">                                                  <span class="nam">reduction_indices</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1758" class="pln">                                   <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1759" class="pln">                                   <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1760" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1761" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1762" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.reduce_logsumexp"</span><span class="op">,</span> <span class="str">"reduce_logsumexp"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1763" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_args</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1764" class="pln">    <span class="key">None</span><span class="op">,</span> <span class="str">"keep_dims is deprecated, use keepdims instead"</span><span class="op">,</span> <span class="str">"keep_dims"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1765" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduce_logsumexp</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1766" class="pln">                     <span class="nam">axis</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1767" class="pln">                     <span class="nam">keepdims</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1768" class="pln">                     <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1769" class="pln">                     <span class="nam">reduction_indices</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1770" class="pln">                     <span class="nam">keep_dims</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1771" class="pln">  <span class="str">"""Computes log(sum(exp(elements across dimensions of a tensor))).</span><span class="strut">&nbsp;</span></p>
<p id="t1772" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1773" class="pln"><span class="str">  Reduces `input_tensor` along the dimensions given in `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t1774" class="pln"><span class="str">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span><span class="strut">&nbsp;</span></p>
<p id="t1775" class="pln"><span class="str">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t1776" class="pln"><span class="str">  are retained with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1777" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1778" class="pln"><span class="str">  If `axis` has no entries, all dimensions are reduced, and a</span><span class="strut">&nbsp;</span></p>
<p id="t1779" class="pln"><span class="str">  tensor with a single element is returned.</span><span class="strut">&nbsp;</span></p>
<p id="t1780" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1781" class="pln"><span class="str">  This function is more numerically stable than log(sum(exp(input))). It avoids</span><span class="strut">&nbsp;</span></p>
<p id="t1782" class="pln"><span class="str">  overflows caused by taking the exp of large inputs and underflows caused by</span><span class="strut">&nbsp;</span></p>
<p id="t1783" class="pln"><span class="str">  taking the log of small inputs.</span><span class="strut">&nbsp;</span></p>
<p id="t1784" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1785" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1786" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1787" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1788" class="pln"><span class="str">  x = tf.constant([[0., 0., 0.], [0., 0., 0.]])</span><span class="strut">&nbsp;</span></p>
<p id="t1789" class="pln"><span class="str">  tf.reduce_logsumexp(x)  # log(6)</span><span class="strut">&nbsp;</span></p>
<p id="t1790" class="pln"><span class="str">  tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]</span><span class="strut">&nbsp;</span></p>
<p id="t1791" class="pln"><span class="str">  tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]</span><span class="strut">&nbsp;</span></p>
<p id="t1792" class="pln"><span class="str">  tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]</span><span class="strut">&nbsp;</span></p>
<p id="t1793" class="pln"><span class="str">  tf.reduce_logsumexp(x, [0, 1])  # log(6)</span><span class="strut">&nbsp;</span></p>
<p id="t1794" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1795" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1796" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1797" class="pln"><span class="str">    input_tensor: The tensor to reduce. Should have numeric type.</span><span class="strut">&nbsp;</span></p>
<p id="t1798" class="pln"><span class="str">    axis: The dimensions to reduce. If `None` (the default),</span><span class="strut">&nbsp;</span></p>
<p id="t1799" class="pln"><span class="str">      reduces all dimensions. Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t1800" class="pln"><span class="str">      `[-rank(input_tensor), rank(input_tensor))`.</span><span class="strut">&nbsp;</span></p>
<p id="t1801" class="pln"><span class="str">    keepdims: If true, retains reduced dimensions with length 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1802" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1803" class="pln"><span class="str">    reduction_indices: The old (deprecated) name for axis.</span><span class="strut">&nbsp;</span></p>
<p id="t1804" class="pln"><span class="str">    keep_dims: Deprecated alias for `keepdims`.</span><span class="strut">&nbsp;</span></p>
<p id="t1805" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1806" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1807" class="pln"><span class="str">    The reduced tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1808" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1809" class="stm mis">  <span class="nam">keepdims</span> <span class="op">=</span> <span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_argument_lookup</span><span class="op">(</span><span class="str">"keepdims"</span><span class="op">,</span> <span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1810" class="pln">                                                    <span class="str">"keep_dims"</span><span class="op">,</span> <span class="nam">keep_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1811" class="stm mis">  <span class="key">if</span> <span class="nam">keepdims</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1812" class="stm mis">    <span class="nam">keepdims</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t1813" class="stm mis">  <span class="nam">input_tensor</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1814" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"ReduceLogSumExp"</span><span class="op">,</span> <span class="op">[</span><span class="nam">input_tensor</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1815" class="stm mis">    <span class="nam">raw_max</span> <span class="op">=</span> <span class="nam">reduce_max</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1816" class="pln">        <span class="nam">input_tensor</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1817" class="pln">        <span class="nam">axis</span><span class="op">=</span><span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1818" class="pln">        <span class="nam">reduction_indices</span><span class="op">=</span><span class="nam">reduction_indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1819" class="pln">        <span class="nam">keepdims</span><span class="op">=</span><span class="key">True</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1820" class="stm mis">    <span class="nam">my_max</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">stop_gradient</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1821" class="pln">        <span class="nam">array_ops</span><span class="op">.</span><span class="nam">where</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1822" class="pln">            <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">is_finite</span><span class="op">(</span><span class="nam">raw_max</span><span class="op">)</span><span class="op">,</span> <span class="nam">raw_max</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1823" class="pln">            <span class="nam">array_ops</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span><span class="nam">raw_max</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1824" class="stm mis">    <span class="nam">result</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">log</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1825" class="pln">        <span class="nam">reduce_sum</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t1826" class="pln">            <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">exp</span><span class="op">(</span><span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sub</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">my_max</span><span class="op">)</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1827" class="pln">            <span class="nam">axis</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1828" class="pln">            <span class="nam">keepdims</span><span class="op">=</span><span class="nam">keepdims</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1829" class="pln">            <span class="nam">reduction_indices</span><span class="op">=</span><span class="nam">reduction_indices</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1830" class="stm mis">    <span class="key">if</span> <span class="key">not</span> <span class="nam">keepdims</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1831" class="stm mis">      <span class="nam">my_max</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="nam">my_max</span><span class="op">,</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">shape</span><span class="op">(</span><span class="nam">result</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1832" class="stm mis">    <span class="nam">result</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">add</span><span class="op">(</span><span class="nam">result</span><span class="op">,</span> <span class="nam">my_max</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1833" class="stm mis">    <span class="key">return</span> <span class="nam">_may_reduce_to_scalar</span><span class="op">(</span><span class="nam">keepdims</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">reduction_indices</span><span class="op">,</span> <span class="nam">result</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1834" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1835" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1836" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"linalg.trace"</span><span class="op">,</span> <span class="str">"trace"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1837" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"trace"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1838" class="stm run hide_run"><span class="key">def</span> <span class="nam">trace</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1839" class="pln">  <span class="str">"""Compute the trace of a tensor `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t1840" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1841" class="pln"><span class="str">  `trace(x)` returns the sum along the main diagonal of each inner-most matrix</span><span class="strut">&nbsp;</span></p>
<p id="t1842" class="pln"><span class="str">  in x. If x is of rank `k` with shape `[I, J, K, ..., L, M, N]`, then output</span><span class="strut">&nbsp;</span></p>
<p id="t1843" class="pln"><span class="str">  is a tensor of rank `k-2` with dimensions `[I, J, K, ..., L]` where</span><span class="strut">&nbsp;</span></p>
<p id="t1844" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1845" class="pln"><span class="str">  `output[i, j, k, ..., l] = trace(x[i, j, i, ..., l, :, :])`</span><span class="strut">&nbsp;</span></p>
<p id="t1846" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1847" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1848" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1849" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1850" class="pln"><span class="str">  x = tf.constant([[1, 2], [3, 4]])</span><span class="strut">&nbsp;</span></p>
<p id="t1851" class="pln"><span class="str">  tf.linalg.trace(x)  # 5</span><span class="strut">&nbsp;</span></p>
<p id="t1852" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1853" class="pln"><span class="str">  x = tf.constant([[1, 2, 3],</span><span class="strut">&nbsp;</span></p>
<p id="t1854" class="pln"><span class="str">                   [4, 5, 6],</span><span class="strut">&nbsp;</span></p>
<p id="t1855" class="pln"><span class="str">                   [7, 8, 9]])</span><span class="strut">&nbsp;</span></p>
<p id="t1856" class="pln"><span class="str">  tf.linalg.trace(x)  # 15</span><span class="strut">&nbsp;</span></p>
<p id="t1857" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1858" class="pln"><span class="str">  x = tf.constant([[[1, 2, 3],</span><span class="strut">&nbsp;</span></p>
<p id="t1859" class="pln"><span class="str">                    [4, 5, 6],</span><span class="strut">&nbsp;</span></p>
<p id="t1860" class="pln"><span class="str">                    [7, 8, 9]],</span><span class="strut">&nbsp;</span></p>
<p id="t1861" class="pln"><span class="str">                   [[-1, -2, -3],</span><span class="strut">&nbsp;</span></p>
<p id="t1862" class="pln"><span class="str">                    [-4, -5, -6],</span><span class="strut">&nbsp;</span></p>
<p id="t1863" class="pln"><span class="str">                    [-7, -8, -9]]])</span><span class="strut">&nbsp;</span></p>
<p id="t1864" class="pln"><span class="str">  tf.linalg.trace(x)  # [15, -15]</span><span class="strut">&nbsp;</span></p>
<p id="t1865" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1866" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1867" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1868" class="pln"><span class="str">    x: tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1869" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1870" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1871" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1872" class="pln"><span class="str">    The trace of input tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t1873" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1874" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Trace"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1875" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1876" class="stm mis">    <span class="key">return</span> <span class="nam">reduce_sum</span><span class="op">(</span><span class="nam">array_ops</span><span class="op">.</span><span class="nam">matrix_diag_part</span><span class="op">(</span><span class="nam">x</span><span class="op">)</span><span class="op">,</span> <span class="op">[</span><span class="op">-</span><span class="num">1</span><span class="op">]</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1877" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1878" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1879" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"linalg.matmul"</span><span class="op">,</span> <span class="str">"matmul"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1880" class="stm run hide_run"><span class="key">def</span> <span class="nam">matmul</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1881" class="pln">           <span class="nam">b</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1882" class="pln">           <span class="nam">transpose_a</span><span class="op">=</span><span class="key">False</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1883" class="pln">           <span class="nam">transpose_b</span><span class="op">=</span><span class="key">False</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1884" class="pln">           <span class="nam">adjoint_a</span><span class="op">=</span><span class="key">False</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1885" class="pln">           <span class="nam">adjoint_b</span><span class="op">=</span><span class="key">False</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1886" class="pln">           <span class="nam">a_is_sparse</span><span class="op">=</span><span class="key">False</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1887" class="pln">           <span class="nam">b_is_sparse</span><span class="op">=</span><span class="key">False</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t1888" class="pln">           <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1889" class="pln">  <span class="str">"""Multiplies matrix `a` by matrix `b`, producing `a` * `b`.</span><span class="strut">&nbsp;</span></p>
<p id="t1890" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1891" class="pln"><span class="str">  The inputs must, following any transpositions, be tensors of rank >= 2</span><span class="strut">&nbsp;</span></p>
<p id="t1892" class="pln"><span class="str">  where the inner 2 dimensions specify valid matrix multiplication arguments,</span><span class="strut">&nbsp;</span></p>
<p id="t1893" class="pln"><span class="str">  and any further outer dimensions match.</span><span class="strut">&nbsp;</span></p>
<p id="t1894" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1895" class="pln"><span class="str">  Both matrices must be of the same type. The supported types are:</span><span class="strut">&nbsp;</span></p>
<p id="t1896" class="pln"><span class="str">  `float16`, `float32`, `float64`, `int32`, `complex64`, `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t1897" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1898" class="pln"><span class="str">  Either matrix can be transposed or adjointed (conjugated and transposed) on</span><span class="strut">&nbsp;</span></p>
<p id="t1899" class="pln"><span class="str">  the fly by setting one of the corresponding flag to `True`. These are `False`</span><span class="strut">&nbsp;</span></p>
<p id="t1900" class="pln"><span class="str">  by default.</span><span class="strut">&nbsp;</span></p>
<p id="t1901" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1902" class="pln"><span class="str">  If one or both of the matrices contain a lot of zeros, a more efficient</span><span class="strut">&nbsp;</span></p>
<p id="t1903" class="pln"><span class="str">  multiplication algorithm can be used by setting the corresponding</span><span class="strut">&nbsp;</span></p>
<p id="t1904" class="pln"><span class="str">  `a_is_sparse` or `b_is_sparse` flag to `True`. These are `False` by default.</span><span class="strut">&nbsp;</span></p>
<p id="t1905" class="pln"><span class="str">  This optimization is only available for plain matrices (rank-2 tensors) with</span><span class="strut">&nbsp;</span></p>
<p id="t1906" class="pln"><span class="str">  datatypes `bfloat16` or `float32`.</span><span class="strut">&nbsp;</span></p>
<p id="t1907" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1908" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t1909" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1910" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t1911" class="pln"><span class="str">  # 2-D tensor `a`</span><span class="strut">&nbsp;</span></p>
<p id="t1912" class="pln"><span class="str">  # [[1, 2, 3],</span><span class="strut">&nbsp;</span></p>
<p id="t1913" class="pln"><span class="str">  #  [4, 5, 6]]</span><span class="strut">&nbsp;</span></p>
<p id="t1914" class="pln"><span class="str">  a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])</span><span class="strut">&nbsp;</span></p>
<p id="t1915" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1916" class="pln"><span class="str">  # 2-D tensor `b`</span><span class="strut">&nbsp;</span></p>
<p id="t1917" class="pln"><span class="str">  # [[ 7,  8],</span><span class="strut">&nbsp;</span></p>
<p id="t1918" class="pln"><span class="str">  #  [ 9, 10],</span><span class="strut">&nbsp;</span></p>
<p id="t1919" class="pln"><span class="str">  #  [11, 12]]</span><span class="strut">&nbsp;</span></p>
<p id="t1920" class="pln"><span class="str">  b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])</span><span class="strut">&nbsp;</span></p>
<p id="t1921" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1922" class="pln"><span class="str">  # `a` * `b`</span><span class="strut">&nbsp;</span></p>
<p id="t1923" class="pln"><span class="str">  # [[ 58,  64],</span><span class="strut">&nbsp;</span></p>
<p id="t1924" class="pln"><span class="str">  #  [139, 154]]</span><span class="strut">&nbsp;</span></p>
<p id="t1925" class="pln"><span class="str">  c = tf.matmul(a, b)</span><span class="strut">&nbsp;</span></p>
<p id="t1926" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1927" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1928" class="pln"><span class="str">  # 3-D tensor `a`</span><span class="strut">&nbsp;</span></p>
<p id="t1929" class="pln"><span class="str">  # [[[ 1,  2,  3],</span><span class="strut">&nbsp;</span></p>
<p id="t1930" class="pln"><span class="str">  #   [ 4,  5,  6]],</span><span class="strut">&nbsp;</span></p>
<p id="t1931" class="pln"><span class="str">  #  [[ 7,  8,  9],</span><span class="strut">&nbsp;</span></p>
<p id="t1932" class="pln"><span class="str">  #   [10, 11, 12]]]</span><span class="strut">&nbsp;</span></p>
<p id="t1933" class="pln"><span class="str">  a = tf.constant(np.arange(1, 13, dtype=np.int32),</span><span class="strut">&nbsp;</span></p>
<p id="t1934" class="pln"><span class="str">                  shape=[2, 2, 3])</span><span class="strut">&nbsp;</span></p>
<p id="t1935" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1936" class="pln"><span class="str">  # 3-D tensor `b`</span><span class="strut">&nbsp;</span></p>
<p id="t1937" class="pln"><span class="str">  # [[[13, 14],</span><span class="strut">&nbsp;</span></p>
<p id="t1938" class="pln"><span class="str">  #   [15, 16],</span><span class="strut">&nbsp;</span></p>
<p id="t1939" class="pln"><span class="str">  #   [17, 18]],</span><span class="strut">&nbsp;</span></p>
<p id="t1940" class="pln"><span class="str">  #  [[19, 20],</span><span class="strut">&nbsp;</span></p>
<p id="t1941" class="pln"><span class="str">  #   [21, 22],</span><span class="strut">&nbsp;</span></p>
<p id="t1942" class="pln"><span class="str">  #   [23, 24]]]</span><span class="strut">&nbsp;</span></p>
<p id="t1943" class="pln"><span class="str">  b = tf.constant(np.arange(13, 25, dtype=np.int32),</span><span class="strut">&nbsp;</span></p>
<p id="t1944" class="pln"><span class="str">                  shape=[2, 3, 2])</span><span class="strut">&nbsp;</span></p>
<p id="t1945" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1946" class="pln"><span class="str">  # `a` * `b`</span><span class="strut">&nbsp;</span></p>
<p id="t1947" class="pln"><span class="str">  # [[[ 94, 100],</span><span class="strut">&nbsp;</span></p>
<p id="t1948" class="pln"><span class="str">  #   [229, 244]],</span><span class="strut">&nbsp;</span></p>
<p id="t1949" class="pln"><span class="str">  #  [[508, 532],</span><span class="strut">&nbsp;</span></p>
<p id="t1950" class="pln"><span class="str">  #   [697, 730]]]</span><span class="strut">&nbsp;</span></p>
<p id="t1951" class="pln"><span class="str">  c = tf.matmul(a, b)</span><span class="strut">&nbsp;</span></p>
<p id="t1952" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1953" class="pln"><span class="str">  # Since python >= 3.5 the @ operator is supported (see PEP 465).</span><span class="strut">&nbsp;</span></p>
<p id="t1954" class="pln"><span class="str">  # In TensorFlow, it simply calls the `tf.matmul()` function, so the</span><span class="strut">&nbsp;</span></p>
<p id="t1955" class="pln"><span class="str">  # following lines are equivalent:</span><span class="strut">&nbsp;</span></p>
<p id="t1956" class="pln"><span class="str">  d = a @ b @ [[10.], [11.]]</span><span class="strut">&nbsp;</span></p>
<p id="t1957" class="pln"><span class="str">  d = tf.matmul(tf.matmul(a, b), [[10.], [11.]])</span><span class="strut">&nbsp;</span></p>
<p id="t1958" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t1959" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1960" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t1961" class="pln"><span class="str">    a: `Tensor` of type `float16`, `float32`, `float64`, `int32`, `complex64`,</span><span class="strut">&nbsp;</span></p>
<p id="t1962" class="pln"><span class="str">      `complex128` and rank > 1.</span><span class="strut">&nbsp;</span></p>
<p id="t1963" class="pln"><span class="str">    b: `Tensor` with same type and rank as `a`.</span><span class="strut">&nbsp;</span></p>
<p id="t1964" class="pln"><span class="str">    transpose_a: If `True`, `a` is transposed before multiplication.</span><span class="strut">&nbsp;</span></p>
<p id="t1965" class="pln"><span class="str">    transpose_b: If `True`, `b` is transposed before multiplication.</span><span class="strut">&nbsp;</span></p>
<p id="t1966" class="pln"><span class="str">    adjoint_a: If `True`, `a` is conjugated and transposed before</span><span class="strut">&nbsp;</span></p>
<p id="t1967" class="pln"><span class="str">      multiplication.</span><span class="strut">&nbsp;</span></p>
<p id="t1968" class="pln"><span class="str">    adjoint_b: If `True`, `b` is conjugated and transposed before</span><span class="strut">&nbsp;</span></p>
<p id="t1969" class="pln"><span class="str">      multiplication.</span><span class="strut">&nbsp;</span></p>
<p id="t1970" class="pln"><span class="str">    a_is_sparse: If `True`, `a` is treated as a sparse matrix.</span><span class="strut">&nbsp;</span></p>
<p id="t1971" class="pln"><span class="str">    b_is_sparse: If `True`, `b` is treated as a sparse matrix.</span><span class="strut">&nbsp;</span></p>
<p id="t1972" class="pln"><span class="str">    name: Name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t1973" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1974" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t1975" class="pln"><span class="str">    A `Tensor` of the same type as `a` and `b` where each inner-most matrix is</span><span class="strut">&nbsp;</span></p>
<p id="t1976" class="pln"><span class="str">    the product of the corresponding matrices in `a` and `b`, e.g. if all</span><span class="strut">&nbsp;</span></p>
<p id="t1977" class="pln"><span class="str">    transpose or adjoint attributes are `False`:</span><span class="strut">&nbsp;</span></p>
<p id="t1978" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1979" class="pln"><span class="str">    `output`[..., i, j] = sum_k (`a`[..., i, k] * `b`[..., k, j]),</span><span class="strut">&nbsp;</span></p>
<p id="t1980" class="pln"><span class="str">    for all indices i, j.</span><span class="strut">&nbsp;</span></p>
<p id="t1981" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1982" class="pln"><span class="str">    Note: This is matrix product, not element-wise product.</span><span class="strut">&nbsp;</span></p>
<p id="t1983" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1984" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1985" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t1986" class="pln"><span class="str">    ValueError: If transpose_a and adjoint_a, or transpose_b and adjoint_b</span><span class="strut">&nbsp;</span></p>
<p id="t1987" class="pln"><span class="str">      are both set to True.</span><span class="strut">&nbsp;</span></p>
<p id="t1988" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t1989" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"MatMul"</span><span class="op">,</span> <span class="op">[</span><span class="nam">a</span><span class="op">,</span> <span class="nam">b</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1990" class="stm mis">    <span class="key">if</span> <span class="nam">transpose_a</span> <span class="key">and</span> <span class="nam">adjoint_a</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1991" class="stm mis">      <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Only one of transpose_a and adjoint_a can be True."</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1992" class="stm mis">    <span class="key">if</span> <span class="nam">transpose_b</span> <span class="key">and</span> <span class="nam">adjoint_b</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1993" class="stm mis">      <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"Only one of transpose_b and adjoint_b can be True."</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1994" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t1995" class="stm mis">    <span class="key">if</span> <span class="nam">context</span><span class="op">.</span><span class="nam">executing_eagerly</span><span class="op">(</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1996" class="stm mis">      <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="op">(</span><span class="nam">ops</span><span class="op">.</span><span class="nam">EagerTensor</span><span class="op">,</span> <span class="nam">_resource_variable_type</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1997" class="stm mis">        <span class="nam">a</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"a"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t1998" class="stm mis">      <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">b</span><span class="op">,</span> <span class="op">(</span><span class="nam">ops</span><span class="op">.</span><span class="nam">EagerTensor</span><span class="op">,</span> <span class="nam">_resource_variable_type</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t1999" class="stm mis">        <span class="nam">b</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">b</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"b"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2000" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2001" class="stm mis">      <span class="nam">a</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"a"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2002" class="stm mis">      <span class="nam">b</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">b</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"b"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2003" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2004" class="pln">    <span class="com"># TODO(apassos) remove _shape_tuple here when it is not needed.</span><span class="strut">&nbsp;</span></p>
<p id="t2005" class="stm mis">    <span class="nam">a_shape</span> <span class="op">=</span> <span class="nam">a</span><span class="op">.</span><span class="nam">_shape_tuple</span><span class="op">(</span><span class="op">)</span>  <span class="com"># pylint: disable=protected-access</span><span class="strut">&nbsp;</span></p>
<p id="t2006" class="stm mis">    <span class="nam">b_shape</span> <span class="op">=</span> <span class="nam">b</span><span class="op">.</span><span class="nam">_shape_tuple</span><span class="op">(</span><span class="op">)</span>  <span class="com"># pylint: disable=protected-access</span><span class="strut">&nbsp;</span></p>
<p id="t2007" class="stm mis">    <span class="key">if</span> <span class="op">(</span><span class="key">not</span> <span class="nam">a_is_sparse</span> <span class="key">and</span><span class="strut">&nbsp;</span></p>
<p id="t2008" class="pln">        <span class="key">not</span> <span class="nam">b_is_sparse</span><span class="op">)</span> <span class="key">and</span> <span class="op">(</span><span class="op">(</span><span class="nam">a_shape</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">len</span><span class="op">(</span><span class="nam">a_shape</span><span class="op">)</span> <span class="op">></span> <span class="num">2</span><span class="op">)</span> <span class="key">and</span><span class="strut">&nbsp;</span></p>
<p id="t2009" class="pln">                              <span class="op">(</span><span class="nam">b_shape</span> <span class="key">is</span> <span class="key">None</span> <span class="key">or</span> <span class="nam">len</span><span class="op">(</span><span class="nam">b_shape</span><span class="op">)</span> <span class="op">></span> <span class="num">2</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2010" class="pln">      <span class="com"># BatchMatmul does not support transpose, so we conjugate the matrix and</span><span class="strut">&nbsp;</span></p>
<p id="t2011" class="pln">      <span class="com"># use adjoint instead. Conj() is a noop for real matrices.</span><span class="strut">&nbsp;</span></p>
<p id="t2012" class="stm mis">      <span class="key">if</span> <span class="nam">transpose_a</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2013" class="stm mis">        <span class="nam">a</span> <span class="op">=</span> <span class="nam">conj</span><span class="op">(</span><span class="nam">a</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2014" class="stm mis">        <span class="nam">adjoint_a</span> <span class="op">=</span> <span class="key">True</span><span class="strut">&nbsp;</span></p>
<p id="t2015" class="stm mis">      <span class="key">if</span> <span class="nam">transpose_b</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2016" class="stm mis">        <span class="nam">b</span> <span class="op">=</span> <span class="nam">conj</span><span class="op">(</span><span class="nam">b</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2017" class="stm mis">        <span class="nam">adjoint_b</span> <span class="op">=</span> <span class="key">True</span><span class="strut">&nbsp;</span></p>
<p id="t2018" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">batch_mat_mul</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2019" class="pln">          <span class="nam">a</span><span class="op">,</span> <span class="nam">b</span><span class="op">,</span> <span class="nam">adj_x</span><span class="op">=</span><span class="nam">adjoint_a</span><span class="op">,</span> <span class="nam">adj_y</span><span class="op">=</span><span class="nam">adjoint_b</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2020" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2021" class="pln">    <span class="com"># Neither matmul nor sparse_matmul support adjoint, so we conjugate</span><span class="strut">&nbsp;</span></p>
<p id="t2022" class="pln">    <span class="com"># the matrix and use transpose instead. Conj() is a noop for real</span><span class="strut">&nbsp;</span></p>
<p id="t2023" class="pln">    <span class="com"># matrices.</span><span class="strut">&nbsp;</span></p>
<p id="t2024" class="stm mis">    <span class="key">if</span> <span class="nam">adjoint_a</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2025" class="stm mis">      <span class="nam">a</span> <span class="op">=</span> <span class="nam">conj</span><span class="op">(</span><span class="nam">a</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2026" class="stm mis">      <span class="nam">transpose_a</span> <span class="op">=</span> <span class="key">True</span><span class="strut">&nbsp;</span></p>
<p id="t2027" class="stm mis">    <span class="key">if</span> <span class="nam">adjoint_b</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2028" class="stm mis">      <span class="nam">b</span> <span class="op">=</span> <span class="nam">conj</span><span class="op">(</span><span class="nam">b</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2029" class="stm mis">      <span class="nam">transpose_b</span> <span class="op">=</span> <span class="key">True</span><span class="strut">&nbsp;</span></p>
<p id="t2030" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2031" class="stm mis">    <span class="nam">use_sparse_matmul</span> <span class="op">=</span> <span class="key">False</span><span class="strut">&nbsp;</span></p>
<p id="t2032" class="stm mis">    <span class="key">if</span> <span class="nam">a_is_sparse</span> <span class="key">or</span> <span class="nam">b_is_sparse</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2033" class="stm mis">      <span class="nam">sparse_matmul_types</span> <span class="op">=</span> <span class="op">[</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">bfloat16</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">float32</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2034" class="stm mis">      <span class="nam">use_sparse_matmul</span> <span class="op">=</span> <span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2035" class="pln">          <span class="nam">a</span><span class="op">.</span><span class="nam">dtype</span> <span class="key">in</span> <span class="nam">sparse_matmul_types</span> <span class="key">and</span> <span class="nam">b</span><span class="op">.</span><span class="nam">dtype</span> <span class="key">in</span> <span class="nam">sparse_matmul_types</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2036" class="stm mis">    <span class="key">if</span> <span class="op">(</span><span class="op">(</span><span class="nam">a</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">==</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">bfloat16</span> <span class="key">or</span> <span class="nam">b</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">==</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">bfloat16</span><span class="op">)</span> <span class="key">and</span><span class="strut">&nbsp;</span></p>
<p id="t2037" class="pln">        <span class="nam">a</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">!=</span> <span class="nam">b</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2038" class="pln">      <span class="com"># matmul currently doesn't handle mixed-precision inputs.</span><span class="strut">&nbsp;</span></p>
<p id="t2039" class="stm mis">      <span class="nam">use_sparse_matmul</span> <span class="op">=</span> <span class="key">True</span><span class="strut">&nbsp;</span></p>
<p id="t2040" class="stm mis">    <span class="key">if</span> <span class="nam">use_sparse_matmul</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2041" class="stm mis">      <span class="nam">ret</span> <span class="op">=</span> <span class="nam">sparse_matmul</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2042" class="pln">          <span class="nam">a</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2043" class="pln">          <span class="nam">b</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2044" class="pln">          <span class="nam">transpose_a</span><span class="op">=</span><span class="nam">transpose_a</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2045" class="pln">          <span class="nam">transpose_b</span><span class="op">=</span><span class="nam">transpose_b</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2046" class="pln">          <span class="nam">a_is_sparse</span><span class="op">=</span><span class="nam">a_is_sparse</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2047" class="pln">          <span class="nam">b_is_sparse</span><span class="op">=</span><span class="nam">b_is_sparse</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2048" class="pln">          <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2049" class="pln">      <span class="com"># sparse_matmul always returns float32, even with</span><span class="strut">&nbsp;</span></p>
<p id="t2050" class="pln">      <span class="com"># bfloat16 inputs. This prevents us from configuring bfloat16 training.</span><span class="strut">&nbsp;</span></p>
<p id="t2051" class="pln">      <span class="com"># casting to bfloat16 also matches non-sparse matmul behavior better.</span><span class="strut">&nbsp;</span></p>
<p id="t2052" class="stm mis">      <span class="key">if</span> <span class="nam">a</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">==</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">bfloat16</span> <span class="key">and</span> <span class="nam">b</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">==</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">bfloat16</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2053" class="stm mis">        <span class="nam">ret</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">ret</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">bfloat16</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2054" class="stm mis">      <span class="key">return</span> <span class="nam">ret</span><span class="strut">&nbsp;</span></p>
<p id="t2055" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2056" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">mat_mul</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2057" class="pln">          <span class="nam">a</span><span class="op">,</span> <span class="nam">b</span><span class="op">,</span> <span class="nam">transpose_a</span><span class="op">=</span><span class="nam">transpose_a</span><span class="op">,</span> <span class="nam">transpose_b</span><span class="op">=</span><span class="nam">transpose_b</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2058" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2059" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2060" class="stm run hide_run"><span class="nam">_OverrideBinaryOperatorHelper</span><span class="op">(</span><span class="nam">matmul</span><span class="op">,</span> <span class="str">"matmul"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2061" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2062" class="stm run hide_run"><span class="nam">sparse_matmul</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sparse_mat_mul</span><span class="strut">&nbsp;</span></p>
<p id="t2063" class="stm run hide_run"><span class="nam">tf_export</span><span class="op">(</span><span class="str">"sparse_matmul"</span><span class="op">)</span><span class="op">(</span><span class="nam">sparse_matmul</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2064" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2065" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2066" class="stm run hide_run"><span class="op">@</span><span class="nam">ops</span><span class="op">.</span><span class="nam">RegisterStatistics</span><span class="op">(</span><span class="str">"MatMul"</span><span class="op">,</span> <span class="str">"flops"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2067" class="pln"><span class="key">def</span> <span class="nam">_calc_mat_mul_flops</span><span class="op">(</span><span class="nam">graph</span><span class="op">,</span> <span class="nam">node</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2068" class="pln">  <span class="str">"""Calculates the compute resources needed for MatMul."""</span><span class="strut">&nbsp;</span></p>
<p id="t2069" class="stm mis">  <span class="nam">transpose_a</span> <span class="op">=</span> <span class="nam">node</span><span class="op">.</span><span class="nam">attr</span><span class="op">[</span><span class="str">"transpose_a"</span><span class="op">]</span><span class="op">.</span><span class="nam">b</span><span class="strut">&nbsp;</span></p>
<p id="t2070" class="stm mis">  <span class="nam">a_shape</span> <span class="op">=</span> <span class="nam">graph_util</span><span class="op">.</span><span class="nam">tensor_shape_from_node_def_name</span><span class="op">(</span><span class="nam">graph</span><span class="op">,</span> <span class="nam">node</span><span class="op">.</span><span class="nam">input</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2071" class="stm mis">  <span class="nam">a_shape</span><span class="op">.</span><span class="nam">assert_is_fully_defined</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2072" class="stm mis">  <span class="key">if</span> <span class="nam">transpose_a</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2073" class="stm mis">    <span class="nam">k</span> <span class="op">=</span> <span class="nam">int</span><span class="op">(</span><span class="nam">a_shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2074" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2075" class="stm mis">    <span class="nam">k</span> <span class="op">=</span> <span class="nam">int</span><span class="op">(</span><span class="nam">a_shape</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2076" class="stm mis">  <span class="nam">output_shape</span> <span class="op">=</span> <span class="nam">graph_util</span><span class="op">.</span><span class="nam">tensor_shape_from_node_def_name</span><span class="op">(</span><span class="nam">graph</span><span class="op">,</span> <span class="nam">node</span><span class="op">.</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2077" class="stm mis">  <span class="nam">output_shape</span><span class="op">.</span><span class="nam">assert_is_fully_defined</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2078" class="stm mis">  <span class="nam">output_count</span> <span class="op">=</span> <span class="nam">np</span><span class="op">.</span><span class="nam">prod</span><span class="op">(</span><span class="nam">output_shape</span><span class="op">.</span><span class="nam">as_list</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2079" class="stm mis">  <span class="key">return</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">OpStats</span><span class="op">(</span><span class="str">"flops"</span><span class="op">,</span> <span class="op">(</span><span class="nam">k</span> <span class="op">*</span> <span class="nam">output_count</span> <span class="op">*</span> <span class="num">2</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2080" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2081" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2082" class="stm run hide_run"><span class="key">def</span> <span class="nam">_as_indexed_slices</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">optimize</span><span class="op">=</span><span class="key">True</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2083" class="pln">  <span class="str">"""Convert 'x' to IndexedSlices.</span><span class="strut">&nbsp;</span></p>
<p id="t2084" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2085" class="pln"><span class="str">  Convert a dense Tensor to a block-sparse IndexedSlices.</span><span class="strut">&nbsp;</span></p>
<p id="t2086" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2087" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2088" class="pln"><span class="str">    x: Either a Tensor object, or an IndexedSlices object.</span><span class="strut">&nbsp;</span></p>
<p id="t2089" class="pln"><span class="str">    optimize: if true, attempt to optimize the conversion of 'x'.</span><span class="strut">&nbsp;</span></p>
<p id="t2090" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2091" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2092" class="pln"><span class="str">    An IndexedSlices object.</span><span class="strut">&nbsp;</span></p>
<p id="t2093" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2094" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t2095" class="pln"><span class="str">    TypeError: If 'x' is not a Tensor or an IndexedSlices object.</span><span class="strut">&nbsp;</span></p>
<p id="t2096" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2097" class="pln">  <span class="com"># TODO(touts): op_scope</span><span class="strut">&nbsp;</span></p>
<p id="t2098" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="op">(</span><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2099" class="stm mis">    <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"Not a Tensor or IndexedSlices: %s"</span> <span class="op">%</span> <span class="nam">type</span><span class="op">(</span><span class="nam">x</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2100" class="stm mis">  <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2101" class="stm mis">    <span class="key">return</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t2102" class="stm mis">  <span class="nam">x_shape</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">shape_internal</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">optimize</span><span class="op">=</span><span class="nam">optimize</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2103" class="stm mis">  <span class="key">return</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">range</span><span class="op">(</span><span class="num">0</span><span class="op">,</span> <span class="nam">x_shape</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">)</span><span class="op">,</span> <span class="nam">x_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2104" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2105" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2106" class="stm run hide_run"><span class="key">def</span> <span class="nam">_as_indexed_slices_list</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="nam">optimize</span><span class="op">=</span><span class="key">True</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2107" class="pln">  <span class="str">"""Convert all elements of 'inputs' to IndexedSlices.</span><span class="strut">&nbsp;</span></p>
<p id="t2108" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2109" class="pln"><span class="str">  Additionally, homogenize the types of all the indices to</span><span class="strut">&nbsp;</span></p>
<p id="t2110" class="pln"><span class="str">  either int32 or int64.</span><span class="strut">&nbsp;</span></p>
<p id="t2111" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2112" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2113" class="pln"><span class="str">    inputs: List containing either Tensor or IndexedSlices objects.</span><span class="strut">&nbsp;</span></p>
<p id="t2114" class="pln"><span class="str">    optimize: if true, attempt to optimize the conversion of each input.</span><span class="strut">&nbsp;</span></p>
<p id="t2115" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2116" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2117" class="pln"><span class="str">    A list of IndexedSlices objects.</span><span class="strut">&nbsp;</span></p>
<p id="t2118" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2119" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t2120" class="pln"><span class="str">    TypeError: If 'inputs' is not a list or a tuple.</span><span class="strut">&nbsp;</span></p>
<p id="t2121" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2122" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="op">(</span><span class="nam">list</span><span class="op">,</span> <span class="nam">tuple</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2123" class="stm mis">    <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"Expected a list or tuple, not a %s"</span> <span class="op">%</span> <span class="nam">type</span><span class="op">(</span><span class="nam">inputs</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2124" class="stm mis">  <span class="nam">outputs</span> <span class="op">=</span> <span class="op">[</span><span class="nam">_as_indexed_slices</span><span class="op">(</span><span class="nam">i</span><span class="op">,</span> <span class="nam">optimize</span><span class="op">=</span><span class="nam">optimize</span><span class="op">)</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">inputs</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2125" class="stm mis">  <span class="nam">with_int32_index</span> <span class="op">=</span> <span class="op">[</span><span class="strut">&nbsp;</span></p>
<p id="t2126" class="pln">      <span class="nam">o</span><span class="op">.</span><span class="nam">indices</span> <span class="key">for</span> <span class="nam">o</span> <span class="key">in</span> <span class="nam">outputs</span> <span class="key">if</span> <span class="nam">o</span><span class="op">.</span><span class="nam">indices</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">==</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="strut">&nbsp;</span></p>
<p id="t2127" class="pln">  <span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2128" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">with_int32_index</span> <span class="key">or</span> <span class="nam">len</span><span class="op">(</span><span class="nam">with_int32_index</span><span class="op">)</span> <span class="op">==</span> <span class="nam">len</span><span class="op">(</span><span class="nam">outputs</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2129" class="stm mis">    <span class="key">return</span> <span class="nam">outputs</span><span class="strut">&nbsp;</span></p>
<p id="t2130" class="stm mis">  <span class="nam">casted_outputs</span> <span class="op">=</span> <span class="op">[</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2131" class="stm mis">  <span class="key">for</span> <span class="nam">o</span> <span class="key">in</span> <span class="nam">outputs</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2132" class="stm mis">    <span class="key">if</span> <span class="nam">o</span><span class="op">.</span><span class="nam">indices</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">==</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2133" class="stm mis">      <span class="nam">casted_outputs</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2134" class="pln">          <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">(</span><span class="nam">o</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">o</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int64</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2135" class="pln">                            <span class="nam">o</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2136" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2137" class="stm mis">      <span class="nam">casted_outputs</span><span class="op">.</span><span class="nam">append</span><span class="op">(</span><span class="nam">o</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2138" class="stm mis">  <span class="key">return</span> <span class="nam">casted_outputs</span><span class="strut">&nbsp;</span></p>
<p id="t2139" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2140" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2141" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.add_n"</span><span class="op">,</span> <span class="str">"add_n"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2142" class="stm run hide_run"><span class="key">def</span> <span class="nam">add_n</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2143" class="pln">  <span class="str">"""Adds all input tensors element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t2144" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2145" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2146" class="pln"><span class="str">    inputs: A list of `Tensor` or `IndexedSlices` objects, each with same shape</span><span class="strut">&nbsp;</span></p>
<p id="t2147" class="pln"><span class="str">      and type.</span><span class="strut">&nbsp;</span></p>
<p id="t2148" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2149" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2150" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2151" class="pln"><span class="str">    A `Tensor` of same shape and type as the elements of `inputs`.</span><span class="strut">&nbsp;</span></p>
<p id="t2152" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2153" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t2154" class="pln"><span class="str">    ValueError: If `inputs` don't all have same shape and dtype or the shape</span><span class="strut">&nbsp;</span></p>
<p id="t2155" class="pln"><span class="str">    cannot be inferred.</span><span class="strut">&nbsp;</span></p>
<p id="t2156" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2157" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">inputs</span> <span class="key">or</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="op">(</span><span class="nam">list</span><span class="op">,</span> <span class="nam">tuple</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2158" class="stm mis">    <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"inputs must be a list of at least one"</span><span class="strut">&nbsp;</span></p>
<p id="t2159" class="pln">                     <span class="str">"Tensor/IndexedSlices with the same dtype and shape"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2160" class="stm mis">  <span class="nam">inputs</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_n_to_tensor_or_indexed_slices</span><span class="op">(</span><span class="nam">inputs</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2161" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">all</span><span class="op">(</span><span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="op">(</span><span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">)</span><span class="op">)</span> <span class="key">for</span> <span class="nam">x</span> <span class="key">in</span> <span class="nam">inputs</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2162" class="stm mis">    <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"inputs must be a list of at least one"</span><span class="strut">&nbsp;</span></p>
<p id="t2163" class="pln">                     <span class="str">"Tensor/IndexedSlices with the same dtype and shape"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2164" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2165" class="stm mis">  <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">inputs</span><span class="op">)</span> <span class="op">==</span> <span class="num">1</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2166" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">IndexedSlices</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2167" class="stm mis">      <span class="nam">values</span> <span class="op">=</span> <span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">values</span><span class="strut">&nbsp;</span></p>
<p id="t2168" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2169" class="stm mis">      <span class="nam">values</span> <span class="op">=</span> <span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2170" class="stm mis">    <span class="key">if</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2171" class="stm mis">      <span class="key">return</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">identity</span><span class="op">(</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2172" class="stm mis">    <span class="key">return</span> <span class="nam">values</span><span class="strut">&nbsp;</span></p>
<p id="t2173" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">add_n</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2174" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2175" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2176" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.accumulate_n"</span><span class="op">,</span> <span class="str">"accumulate_n"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2177" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"accumulate_n"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2178" class="stm run hide_run"><span class="key">def</span> <span class="nam">accumulate_n</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="nam">shape</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">tensor_dtype</span><span class="op">=</span><span class="key">None</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2179" class="pln">  <span class="str">"""Returns the element-wise sum of a list of tensors.</span><span class="strut">&nbsp;</span></p>
<p id="t2180" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2181" class="pln"><span class="str">  Optionally, pass `shape` and `tensor_dtype` for shape and type checking,</span><span class="strut">&nbsp;</span></p>
<p id="t2182" class="pln"><span class="str">  otherwise, these are inferred.</span><span class="strut">&nbsp;</span></p>
<p id="t2183" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2184" class="pln"><span class="str">  `tf.math.accumulate_n` performs the same operation as `tf.add_n`, but does not</span><span class="strut">&nbsp;</span></p>
<p id="t2185" class="pln"><span class="str">  wait for all of its inputs to be ready before beginning to sum. This can</span><span class="strut">&nbsp;</span></p>
<p id="t2186" class="pln"><span class="str">  save memory if inputs are ready at different times, since minimum temporary</span><span class="strut">&nbsp;</span></p>
<p id="t2187" class="pln"><span class="str">  storage is proportional to the output size rather than the inputs size.</span><span class="strut">&nbsp;</span></p>
<p id="t2188" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2189" class="pln"><span class="str">  `accumulate_n` is differentiable (but wasn't previous to TensorFlow 1.7).</span><span class="strut">&nbsp;</span></p>
<p id="t2190" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2191" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t2192" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2193" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2194" class="pln"><span class="str">  a = tf.constant([[1, 2], [3, 4]])</span><span class="strut">&nbsp;</span></p>
<p id="t2195" class="pln"><span class="str">  b = tf.constant([[5, 0], [0, 6]])</span><span class="strut">&nbsp;</span></p>
<p id="t2196" class="pln"><span class="str">  tf.math.accumulate_n([a, b, a])  # [[7, 4], [6, 14]]</span><span class="strut">&nbsp;</span></p>
<p id="t2197" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2198" class="pln"><span class="str">  # Explicitly pass shape and type</span><span class="strut">&nbsp;</span></p>
<p id="t2199" class="pln"><span class="str">  tf.math.accumulate_n([a, b, a], shape=[2, 2], tensor_dtype=tf.int32)</span><span class="strut">&nbsp;</span></p>
<p id="t2200" class="pln"><span class="str">                                                                 # [[7,  4],</span><span class="strut">&nbsp;</span></p>
<p id="t2201" class="pln"><span class="str">                                                                 #  [6, 14]]</span><span class="strut">&nbsp;</span></p>
<p id="t2202" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2203" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2204" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2205" class="pln"><span class="str">    inputs: A list of `Tensor` objects, each with same shape and type.</span><span class="strut">&nbsp;</span></p>
<p id="t2206" class="pln"><span class="str">    shape: Shape of elements of `inputs`.</span><span class="strut">&nbsp;</span></p>
<p id="t2207" class="pln"><span class="str">    tensor_dtype: The type of `inputs`.</span><span class="strut">&nbsp;</span></p>
<p id="t2208" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2209" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2210" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2211" class="pln"><span class="str">    A `Tensor` of same shape and type as the elements of `inputs`.</span><span class="strut">&nbsp;</span></p>
<p id="t2212" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2213" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t2214" class="pln"><span class="str">    ValueError: If `inputs` don't all have same shape and dtype or the shape</span><span class="strut">&nbsp;</span></p>
<p id="t2215" class="pln"><span class="str">    cannot be inferred.</span><span class="strut">&nbsp;</span></p>
<p id="t2216" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2217" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2218" class="stm mis">  <span class="key">def</span> <span class="nam">_input_error</span><span class="op">(</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2219" class="stm mis">    <span class="key">return</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"inputs must be a list of at least one Tensor with the "</span><span class="strut">&nbsp;</span></p>
<p id="t2220" class="pln">                      <span class="str">"same dtype and shape"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2221" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2222" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">inputs</span> <span class="key">or</span> <span class="key">not</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="op">(</span><span class="nam">list</span><span class="op">,</span> <span class="nam">tuple</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2223" class="stm mis">    <span class="key">raise</span> <span class="nam">_input_error</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2224" class="stm mis">  <span class="nam">inputs</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_n_to_tensor_or_indexed_slices</span><span class="op">(</span><span class="nam">inputs</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2225" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">all</span><span class="op">(</span><span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span> <span class="key">for</span> <span class="nam">x</span> <span class="key">in</span> <span class="nam">inputs</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2226" class="stm mis">    <span class="key">raise</span> <span class="nam">_input_error</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2227" class="stm mis">  <span class="key">if</span> <span class="key">not</span> <span class="nam">all</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">==</span> <span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">dtype</span> <span class="key">for</span> <span class="nam">x</span> <span class="key">in</span> <span class="nam">inputs</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2228" class="stm mis">    <span class="key">raise</span> <span class="nam">_input_error</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2229" class="stm mis">  <span class="key">if</span> <span class="nam">shape</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2230" class="stm mis">    <span class="nam">shape</span> <span class="op">=</span> <span class="nam">tensor_shape</span><span class="op">.</span><span class="nam">as_shape</span><span class="op">(</span><span class="nam">shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2231" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2232" class="stm mis">    <span class="nam">shape</span> <span class="op">=</span> <span class="nam">tensor_shape</span><span class="op">.</span><span class="nam">unknown_shape</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2233" class="stm mis">  <span class="key">for</span> <span class="nam">input_tensor</span> <span class="key">in</span> <span class="nam">inputs</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2234" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2235" class="stm mis">      <span class="nam">shape</span> <span class="op">=</span> <span class="nam">shape</span><span class="op">.</span><span class="nam">merge_with</span><span class="op">(</span><span class="nam">input_tensor</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2236" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2237" class="pln">  <span class="com"># tensor_dtype is for safety only; operator's output type computed in C++</span><span class="strut">&nbsp;</span></p>
<p id="t2238" class="stm mis">  <span class="key">if</span> <span class="nam">tensor_dtype</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span> <span class="key">and</span> <span class="nam">tensor_dtype</span> <span class="op">!=</span> <span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">dtype</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2239" class="stm mis">    <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="str">"tensor_dtype is {}, but input is of type {}"</span><span class="op">.</span><span class="nam">format</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2240" class="pln">        <span class="nam">tensor_dtype</span><span class="op">,</span> <span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2241" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2242" class="stm mis">  <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">inputs</span><span class="op">)</span> <span class="op">==</span> <span class="num">1</span> <span class="key">and</span> <span class="nam">name</span> <span class="key">is</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2243" class="stm mis">    <span class="key">return</span> <span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2244" class="stm mis">  <span class="key">elif</span> <span class="nam">len</span><span class="op">(</span><span class="nam">inputs</span><span class="op">)</span> <span class="op">==</span> <span class="num">1</span> <span class="key">and</span> <span class="nam">name</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2245" class="stm mis">    <span class="key">return</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">identity</span><span class="op">(</span><span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2246" class="stm mis">  <span class="key">elif</span> <span class="nam">context</span><span class="op">.</span><span class="nam">executing_eagerly</span><span class="op">(</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2247" class="pln">    <span class="com"># TemporaryVariable not currently supported in eager mode; fall back</span><span class="strut">&nbsp;</span></p>
<p id="t2248" class="pln">    <span class="com"># onto AddN for now.</span><span class="strut">&nbsp;</span></p>
<p id="t2249" class="pln">    <span class="com"># TODO(frreiss) remove this once the lifetime of eager variables gets</span><span class="strut">&nbsp;</span></p>
<p id="t2250" class="pln">    <span class="com"># addressed</span><span class="strut">&nbsp;</span></p>
<p id="t2251" class="stm mis">    <span class="key">return</span> <span class="nam">add_n</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2252" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2253" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">accumulate_nv2</span><span class="op">(</span><span class="nam">inputs</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">,</span> <span class="nam">shape</span><span class="op">=</span><span class="nam">shape</span><span class="op">)</span>  <span class="com"># pylint: disable=protected-access</span><span class="strut">&nbsp;</span></p>
<p id="t2254" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2255" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2256" class="stm run hide_run"><span class="op">@</span><span class="nam">ops</span><span class="op">.</span><span class="nam">RegisterGradient</span><span class="op">(</span><span class="str">"AccumulateNV2"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2257" class="pln"><span class="key">def</span> <span class="nam">_accumulate_n_grad</span><span class="op">(</span><span class="nam">op</span><span class="op">,</span> <span class="nam">grad</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2258" class="pln">  <span class="str">"""Same as gradient for AddN. Copies the gradient to all inputs."""</span><span class="strut">&nbsp;</span></p>
<p id="t2259" class="pln">  <span class="com"># Not broadcasting.</span><span class="strut">&nbsp;</span></p>
<p id="t2260" class="stm mis">  <span class="key">return</span> <span class="op">[</span><span class="nam">grad</span><span class="op">]</span> <span class="op">*</span> <span class="nam">len</span><span class="op">(</span><span class="nam">op</span><span class="op">.</span><span class="nam">inputs</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2261" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2262" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2263" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.sigmoid"</span><span class="op">,</span> <span class="str">"nn.sigmoid"</span><span class="op">,</span> <span class="str">"sigmoid"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2264" class="stm run hide_run"><span class="key">def</span> <span class="nam">sigmoid</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2265" class="pln">  <span class="str">"""Computes sigmoid of `x` element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t2266" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2267" class="pln"><span class="str">  Specifically, `y = 1 / (1 + exp(-x))`.</span><span class="strut">&nbsp;</span></p>
<p id="t2268" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2269" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2270" class="pln"><span class="str">    x: A Tensor with type `float16`, `float32`, `float64`, `complex64`,</span><span class="strut">&nbsp;</span></p>
<p id="t2271" class="pln"><span class="str">      or `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t2272" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2273" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2274" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2275" class="pln"><span class="str">    A Tensor with the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t2276" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2277" class="pln"><span class="str">  @compatibility(scipy)</span><span class="strut">&nbsp;</span></p>
<p id="t2278" class="pln"><span class="str">  Equivalent to scipy.special.expit</span><span class="strut">&nbsp;</span></p>
<p id="t2279" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t2280" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2281" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Sigmoid"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2282" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2283" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sigmoid</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2284" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2285" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2286" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.log_sigmoid"</span><span class="op">,</span> <span class="str">"log_sigmoid"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2287" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"log_sigmoid"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2288" class="stm run hide_run"><span class="key">def</span> <span class="nam">log_sigmoid</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2289" class="pln">  <span class="str">"""Computes log sigmoid of `x` element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t2290" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2291" class="pln"><span class="str">  Specifically, `y = log(1 / (1 + exp(-x)))`.  For numerical stability,</span><span class="strut">&nbsp;</span></p>
<p id="t2292" class="pln"><span class="str">  we use `y = -tf.nn.softplus(-x)`.</span><span class="strut">&nbsp;</span></p>
<p id="t2293" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2294" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2295" class="pln"><span class="str">    x: A Tensor with type `float32` or `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t2296" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2297" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2298" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2299" class="pln"><span class="str">    A Tensor with the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t2300" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2301" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"LogSigmoid"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2302" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2303" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">neg</span><span class="op">(</span><span class="nam">gen_nn_ops</span><span class="op">.</span><span class="nam">softplus</span><span class="op">(</span><span class="op">-</span><span class="nam">x</span><span class="op">)</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2304" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2305" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2306" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.tanh"</span><span class="op">,</span> <span class="str">"nn.tanh"</span><span class="op">,</span> <span class="str">"tanh"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2307" class="stm run hide_run"><span class="key">def</span> <span class="nam">tanh</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2308" class="pln">  <span class="str">"""Computes hyperbolic tangent of `x` element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t2309" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2310" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2311" class="pln"><span class="str">    x: A Tensor or SparseTensor with type `float16`, `float32`, `double`,</span><span class="strut">&nbsp;</span></p>
<p id="t2312" class="pln"><span class="str">      `complex64`, or `complex128`.</span><span class="strut">&nbsp;</span></p>
<p id="t2313" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2314" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2315" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2316" class="pln"><span class="str">    A Tensor or SparseTensor respectively with the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t2317" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2318" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Tanh"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2319" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2320" class="stm mis">      <span class="nam">x_tanh</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">tanh</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2321" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2322" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_tanh</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2323" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2324" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">tanh</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2325" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2326" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2327" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.bincount"</span><span class="op">,</span> <span class="str">"bincount"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2328" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"bincount"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2329" class="stm run hide_run"><span class="key">def</span> <span class="nam">bincount</span><span class="op">(</span><span class="nam">arr</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2330" class="pln">             <span class="nam">weights</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2331" class="pln">             <span class="nam">minlength</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2332" class="pln">             <span class="nam">maxlength</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2333" class="pln">             <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2334" class="pln">  <span class="str">"""Counts the number of occurrences of each value in an integer array.</span><span class="strut">&nbsp;</span></p>
<p id="t2335" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2336" class="pln"><span class="str">  If `minlength` and `maxlength` are not given, returns a vector with length</span><span class="strut">&nbsp;</span></p>
<p id="t2337" class="pln"><span class="str">  `tf.reduce_max(arr) + 1` if `arr` is non-empty, and length 0 otherwise.</span><span class="strut">&nbsp;</span></p>
<p id="t2338" class="pln"><span class="str">  If `weights` are non-None, then index `i` of the output stores the sum of the</span><span class="strut">&nbsp;</span></p>
<p id="t2339" class="pln"><span class="str">  value in `weights` at each index where the corresponding value in `arr` is</span><span class="strut">&nbsp;</span></p>
<p id="t2340" class="pln"><span class="str">  `i`.</span><span class="strut">&nbsp;</span></p>
<p id="t2341" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2342" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2343" class="pln"><span class="str">    arr: An int32 tensor of non-negative values.</span><span class="strut">&nbsp;</span></p>
<p id="t2344" class="pln"><span class="str">    weights: If non-None, must be the same shape as arr. For each value in</span><span class="strut">&nbsp;</span></p>
<p id="t2345" class="pln"><span class="str">        `arr`, the bin will be incremented by the corresponding weight instead</span><span class="strut">&nbsp;</span></p>
<p id="t2346" class="pln"><span class="str">        of 1.</span><span class="strut">&nbsp;</span></p>
<p id="t2347" class="pln"><span class="str">    minlength: If given, ensures the output has length at least `minlength`,</span><span class="strut">&nbsp;</span></p>
<p id="t2348" class="pln"><span class="str">        padding with zeros at the end if necessary.</span><span class="strut">&nbsp;</span></p>
<p id="t2349" class="pln"><span class="str">    maxlength: If given, skips values in `arr` that are equal or greater than</span><span class="strut">&nbsp;</span></p>
<p id="t2350" class="pln"><span class="str">        `maxlength`, ensuring that the output has length at most `maxlength`.</span><span class="strut">&nbsp;</span></p>
<p id="t2351" class="pln"><span class="str">    dtype: If `weights` is None, determines the type of the output bins.</span><span class="strut">&nbsp;</span></p>
<p id="t2352" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2353" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2354" class="pln"><span class="str">    A vector with the same dtype as `weights` or the given `dtype`. The bin</span><span class="strut">&nbsp;</span></p>
<p id="t2355" class="pln"><span class="str">    values.</span><span class="strut">&nbsp;</span></p>
<p id="t2356" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2357" class="stm mis">  <span class="nam">arr</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">arr</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"arr"</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2358" class="stm mis">  <span class="nam">array_is_nonempty</span> <span class="op">=</span> <span class="nam">reduce_prod</span><span class="op">(</span><span class="nam">array_ops</span><span class="op">.</span><span class="nam">shape</span><span class="op">(</span><span class="nam">arr</span><span class="op">)</span><span class="op">)</span> <span class="op">></span> <span class="num">0</span><span class="strut">&nbsp;</span></p>
<p id="t2359" class="stm mis">  <span class="nam">output_size</span> <span class="op">=</span> <span class="nam">cast</span><span class="op">(</span><span class="nam">array_is_nonempty</span><span class="op">,</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span> <span class="op">*</span> <span class="op">(</span><span class="nam">reduce_max</span><span class="op">(</span><span class="nam">arr</span><span class="op">)</span> <span class="op">+</span> <span class="num">1</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2360" class="stm mis">  <span class="key">if</span> <span class="nam">minlength</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2361" class="stm mis">    <span class="nam">minlength</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2362" class="pln">        <span class="nam">minlength</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"minlength"</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2363" class="stm mis">    <span class="nam">output_size</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">maximum</span><span class="op">(</span><span class="nam">minlength</span><span class="op">,</span> <span class="nam">output_size</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2364" class="stm mis">  <span class="key">if</span> <span class="nam">maxlength</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2365" class="stm mis">    <span class="nam">maxlength</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2366" class="pln">        <span class="nam">maxlength</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"maxlength"</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2367" class="stm mis">    <span class="nam">output_size</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">minimum</span><span class="op">(</span><span class="nam">maxlength</span><span class="op">,</span> <span class="nam">output_size</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2368" class="stm mis">  <span class="key">if</span> <span class="nam">weights</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2369" class="stm mis">    <span class="nam">weights</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">weights</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"weights"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2370" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">unsorted_segment_sum</span><span class="op">(</span><span class="nam">weights</span><span class="op">,</span> <span class="nam">arr</span><span class="op">,</span> <span class="nam">output_size</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2371" class="stm mis">  <span class="nam">weights</span> <span class="op">=</span> <span class="nam">constant_op</span><span class="op">.</span><span class="nam">constant</span><span class="op">(</span><span class="op">[</span><span class="op">]</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2372" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">bincount</span><span class="op">(</span><span class="nam">arr</span><span class="op">,</span> <span class="nam">output_size</span><span class="op">,</span> <span class="nam">weights</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2373" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2374" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2375" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.cumsum"</span><span class="op">,</span> <span class="str">"cumsum"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2376" class="stm run hide_run"><span class="key">def</span> <span class="nam">cumsum</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">axis</span><span class="op">=</span><span class="num">0</span><span class="op">,</span> <span class="nam">exclusive</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">reverse</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2377" class="pln">  <span class="str">"""Compute the cumulative sum of the tensor `x` along `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t2378" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2379" class="pln"><span class="str">  By default, this op performs an inclusive cumsum, which means that the first</span><span class="strut">&nbsp;</span></p>
<p id="t2380" class="pln"><span class="str">  element of the input is identical to the first element of the output:</span><span class="strut">&nbsp;</span></p>
<p id="t2381" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2382" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2383" class="pln"><span class="str">  tf.cumsum([a, b, c])  # [a, a + b, a + b + c]</span><span class="strut">&nbsp;</span></p>
<p id="t2384" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2385" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2386" class="pln"><span class="str">  By setting the `exclusive` kwarg to `True`, an exclusive cumsum is performed</span><span class="strut">&nbsp;</span></p>
<p id="t2387" class="pln"><span class="str">  instead:</span><span class="strut">&nbsp;</span></p>
<p id="t2388" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2389" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2390" class="pln"><span class="str">  tf.cumsum([a, b, c], exclusive=True)  # [0, a, a + b]</span><span class="strut">&nbsp;</span></p>
<p id="t2391" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2392" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2393" class="pln"><span class="str">  By setting the `reverse` kwarg to `True`, the cumsum is performed in the</span><span class="strut">&nbsp;</span></p>
<p id="t2394" class="pln"><span class="str">  opposite direction:</span><span class="strut">&nbsp;</span></p>
<p id="t2395" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2396" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2397" class="pln"><span class="str">  tf.cumsum([a, b, c], reverse=True)  # [a + b + c, b + c, c]</span><span class="strut">&nbsp;</span></p>
<p id="t2398" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2399" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2400" class="pln"><span class="str">  This is more efficient than using separate `tf.reverse` ops.</span><span class="strut">&nbsp;</span></p>
<p id="t2401" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2402" class="pln"><span class="str">  The `reverse` and `exclusive` kwargs can also be combined:</span><span class="strut">&nbsp;</span></p>
<p id="t2403" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2404" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2405" class="pln"><span class="str">  tf.cumsum([a, b, c], exclusive=True, reverse=True)  # [b + c, c, 0]</span><span class="strut">&nbsp;</span></p>
<p id="t2406" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2407" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2408" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2409" class="pln"><span class="str">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span><span class="strut">&nbsp;</span></p>
<p id="t2410" class="pln"><span class="str">       `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,</span><span class="strut">&nbsp;</span></p>
<p id="t2411" class="pln"><span class="str">       `complex128`, `qint8`, `quint8`, `qint32`, `half`.</span><span class="strut">&nbsp;</span></p>
<p id="t2412" class="pln"><span class="str">    axis: A `Tensor` of type `int32` (default: 0). Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t2413" class="pln"><span class="str">      `[-rank(x), rank(x))`.</span><span class="strut">&nbsp;</span></p>
<p id="t2414" class="pln"><span class="str">    exclusive: If `True`, perform exclusive cumsum.</span><span class="strut">&nbsp;</span></p>
<p id="t2415" class="pln"><span class="str">    reverse: A `bool` (default: False).</span><span class="strut">&nbsp;</span></p>
<p id="t2416" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2417" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2418" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2419" class="pln"><span class="str">    A `Tensor`. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t2420" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2421" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Cumsum"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2422" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2423" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">cumsum</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2424" class="pln">        <span class="nam">x</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">exclusive</span><span class="op">=</span><span class="nam">exclusive</span><span class="op">,</span> <span class="nam">reverse</span><span class="op">=</span><span class="nam">reverse</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2425" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2426" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2427" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.cumprod"</span><span class="op">,</span> <span class="str">"cumprod"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2428" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"cumprod"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2429" class="stm run hide_run"><span class="key">def</span> <span class="nam">cumprod</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">axis</span><span class="op">=</span><span class="num">0</span><span class="op">,</span> <span class="nam">exclusive</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">reverse</span><span class="op">=</span><span class="key">False</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2430" class="pln">  <span class="str">"""Compute the cumulative product of the tensor `x` along `axis`.</span><span class="strut">&nbsp;</span></p>
<p id="t2431" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2432" class="pln"><span class="str">  By default, this op performs an inclusive cumprod, which means that the</span><span class="strut">&nbsp;</span></p>
<p id="t2433" class="pln"><span class="str">  first element of the input is identical to the first element of the output:</span><span class="strut">&nbsp;</span></p>
<p id="t2434" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2435" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2436" class="pln"><span class="str">  tf.math.cumprod([a, b, c])  # [a, a * b, a * b * c]</span><span class="strut">&nbsp;</span></p>
<p id="t2437" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2438" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2439" class="pln"><span class="str">  By setting the `exclusive` kwarg to `True`, an exclusive cumprod is</span><span class="strut">&nbsp;</span></p>
<p id="t2440" class="pln"><span class="str">  performed</span><span class="strut">&nbsp;</span></p>
<p id="t2441" class="pln"><span class="str">  instead:</span><span class="strut">&nbsp;</span></p>
<p id="t2442" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2443" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2444" class="pln"><span class="str">  tf.math.cumprod([a, b, c], exclusive=True)  # [1, a, a * b]</span><span class="strut">&nbsp;</span></p>
<p id="t2445" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2446" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2447" class="pln"><span class="str">  By setting the `reverse` kwarg to `True`, the cumprod is performed in the</span><span class="strut">&nbsp;</span></p>
<p id="t2448" class="pln"><span class="str">  opposite direction:</span><span class="strut">&nbsp;</span></p>
<p id="t2449" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2450" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2451" class="pln"><span class="str">  tf.math.cumprod([a, b, c], reverse=True)  # [a * b * c, b * c, c]</span><span class="strut">&nbsp;</span></p>
<p id="t2452" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2453" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2454" class="pln"><span class="str">  This is more efficient than using separate `tf.reverse` ops.</span><span class="strut">&nbsp;</span></p>
<p id="t2455" class="pln"><span class="str">  The `reverse` and `exclusive` kwargs can also be combined:</span><span class="strut">&nbsp;</span></p>
<p id="t2456" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2457" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2458" class="pln"><span class="str">  tf.math.cumprod([a, b, c], exclusive=True, reverse=True)  # [b * c, c, 1]</span><span class="strut">&nbsp;</span></p>
<p id="t2459" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2460" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2461" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2462" class="pln"><span class="str">    x: A `Tensor`. Must be one of the following types: `float32`, `float64`,</span><span class="strut">&nbsp;</span></p>
<p id="t2463" class="pln"><span class="str">       `int64`, `int32`, `uint8`, `uint16`, `int16`, `int8`, `complex64`,</span><span class="strut">&nbsp;</span></p>
<p id="t2464" class="pln"><span class="str">       `complex128`, `qint8`, `quint8`, `qint32`, `half`.</span><span class="strut">&nbsp;</span></p>
<p id="t2465" class="pln"><span class="str">    axis: A `Tensor` of type `int32` (default: 0). Must be in the range</span><span class="strut">&nbsp;</span></p>
<p id="t2466" class="pln"><span class="str">      `[-rank(x), rank(x))`.</span><span class="strut">&nbsp;</span></p>
<p id="t2467" class="pln"><span class="str">    exclusive: If `True`, perform exclusive cumprod.</span><span class="strut">&nbsp;</span></p>
<p id="t2468" class="pln"><span class="str">    reverse: A `bool` (default: False).</span><span class="strut">&nbsp;</span></p>
<p id="t2469" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2470" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2471" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2472" class="pln"><span class="str">    A `Tensor`. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t2473" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2474" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Cumprod"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2475" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2476" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">cumprod</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2477" class="pln">        <span class="nam">x</span><span class="op">,</span> <span class="nam">axis</span><span class="op">,</span> <span class="nam">exclusive</span><span class="op">=</span><span class="nam">exclusive</span><span class="op">,</span> <span class="nam">reverse</span><span class="op">=</span><span class="nam">reverse</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2478" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2479" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2480" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.conj"</span><span class="op">,</span> <span class="str">"conj"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2481" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"conj"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2482" class="stm run hide_run"><span class="key">def</span> <span class="nam">conj</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2483" class="pln">  <span class="str">r"""Returns the complex conjugate of a complex number.</span><span class="strut">&nbsp;</span></p>
<p id="t2484" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2485" class="pln"><span class="str">  Given a tensor `input` of complex numbers, this operation returns a tensor of</span><span class="strut">&nbsp;</span></p>
<p id="t2486" class="pln"><span class="str">  complex numbers that are the complex conjugate of each element in `input`. The</span><span class="strut">&nbsp;</span></p>
<p id="t2487" class="pln"><span class="str">  complex numbers in `input` must be of the form \\(a + bj\\), where *a* is the</span><span class="strut">&nbsp;</span></p>
<p id="t2488" class="pln"><span class="str">  real part and *b* is the imaginary part.</span><span class="strut">&nbsp;</span></p>
<p id="t2489" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2490" class="pln"><span class="str">  The complex conjugate returned by this operation is of the form \\(a - bj\\).</span><span class="strut">&nbsp;</span></p>
<p id="t2491" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2492" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t2493" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2494" class="pln"><span class="str">      # tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]</span><span class="strut">&nbsp;</span></p>
<p id="t2495" class="pln"><span class="str">      tf.math.conj(input) ==> [-2.25 - 4.75j, 3.25 - 5.75j]</span><span class="strut">&nbsp;</span></p>
<p id="t2496" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2497" class="pln"><span class="str">  If `x` is real, it is returned unchanged.</span><span class="strut">&nbsp;</span></p>
<p id="t2498" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2499" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2500" class="pln"><span class="str">    x: `Tensor` to conjugate.  Must have numeric or variant type.</span><span class="strut">&nbsp;</span></p>
<p id="t2501" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2502" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2503" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2504" class="pln"><span class="str">    A `Tensor` that is the conjugate of `x` (with the same type).</span><span class="strut">&nbsp;</span></p>
<p id="t2505" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2506" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t2507" class="pln"><span class="str">    TypeError: If `x` is not a numeric tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t2508" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2509" class="stm mis">  <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">Tensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2510" class="stm mis">    <span class="nam">dt</span> <span class="op">=</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="strut">&nbsp;</span></p>
<p id="t2511" class="stm mis">    <span class="key">if</span> <span class="nam">dt</span><span class="op">.</span><span class="nam">is_floating</span> <span class="key">or</span> <span class="nam">dt</span><span class="op">.</span><span class="nam">is_integer</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2512" class="stm mis">      <span class="key">return</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t2513" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Conj"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2514" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2515" class="stm mis">    <span class="key">if</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_complex</span> <span class="key">or</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span> <span class="op">==</span> <span class="nam">dtypes</span><span class="op">.</span><span class="nam">variant</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2516" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">conj</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2517" class="stm mis">    <span class="key">elif</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_floating</span> <span class="key">or</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">.</span><span class="nam">is_integer</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2518" class="stm mis">      <span class="key">return</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t2519" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2520" class="stm mis">      <span class="key">raise</span> <span class="nam">TypeError</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2521" class="pln">          <span class="str">"Expected numeric or variant tensor, got dtype %r"</span> <span class="op">%</span> <span class="nam">x</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2522" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2523" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2524" class="stm run hide_run"><span class="key">def</span> <span class="nam">_BroadcastShape</span><span class="op">(</span><span class="nam">op</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2525" class="pln">  <span class="str">"""Common shape function for binary operators that broadcast their inputs."""</span><span class="strut">&nbsp;</span></p>
<p id="t2526" class="stm mis">  <span class="key">return</span> <span class="op">[</span><span class="strut">&nbsp;</span></p>
<p id="t2527" class="pln">      <span class="nam">common_shapes</span><span class="op">.</span><span class="nam">broadcast_shape</span><span class="op">(</span><span class="nam">op</span><span class="op">.</span><span class="nam">inputs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2528" class="pln">                                    <span class="nam">op</span><span class="op">.</span><span class="nam">inputs</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2529" class="pln">  <span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2530" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2531" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2532" class="stm run hide_run"><span class="key">def</span> <span class="nam">reduced_shape</span><span class="op">(</span><span class="nam">input_shape</span><span class="op">,</span> <span class="nam">axes</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2533" class="pln">  <span class="str">"""Helper function for reduction ops.</span><span class="strut">&nbsp;</span></p>
<p id="t2534" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2535" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2536" class="pln"><span class="str">    input_shape: 1-D Tensor, the shape of the Tensor being reduced.</span><span class="strut">&nbsp;</span></p>
<p id="t2537" class="pln"><span class="str">    axes: 1-D Tensor, the reduction axes.</span><span class="strut">&nbsp;</span></p>
<p id="t2538" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2539" class="pln"><span class="str">    A 1-D Tensor, the output shape as if keepdims were set to True.</span><span class="strut">&nbsp;</span></p>
<p id="t2540" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2541" class="pln">  <span class="com"># Example:</span><span class="strut">&nbsp;</span></p>
<p id="t2542" class="pln">  <span class="com"># cast needed for SparseTensor reductions</span><span class="strut">&nbsp;</span></p>
<p id="t2543" class="stm mis">  <span class="key">if</span> <span class="nam">context</span><span class="op">.</span><span class="nam">executing_eagerly</span><span class="op">(</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2544" class="stm mis">    <span class="nam">input_shape</span> <span class="op">=</span> <span class="nam">input_shape</span><span class="op">.</span><span class="nam">numpy</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2545" class="stm mis">    <span class="nam">axes</span> <span class="op">=</span> <span class="nam">axes</span><span class="op">.</span><span class="nam">numpy</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2546" class="stm mis">    <span class="nam">input_shape</span><span class="op">[</span><span class="nam">axes</span><span class="op">]</span> <span class="op">=</span> <span class="num">1</span><span class="strut">&nbsp;</span></p>
<p id="t2547" class="stm mis">    <span class="key">return</span> <span class="nam">input_shape</span><span class="strut">&nbsp;</span></p>
<p id="t2548" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2549" class="stm mis">  <span class="nam">input_shape</span> <span class="op">=</span> <span class="nam">to_int32</span><span class="op">(</span><span class="nam">input_shape</span><span class="op">)</span>  <span class="com"># [2, 3, 5, 7]</span><span class="strut">&nbsp;</span></p>
<p id="t2550" class="stm mis">  <span class="nam">axes</span> <span class="op">=</span> <span class="nam">to_int32</span><span class="op">(</span><span class="nam">axes</span><span class="op">)</span>  <span class="com"># [1, 2]</span><span class="strut">&nbsp;</span></p>
<p id="t2551" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2552" class="stm mis">  <span class="nam">input_rank</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">size</span><span class="op">(</span><span class="nam">input_shape</span><span class="op">)</span>  <span class="com"># 4</span><span class="strut">&nbsp;</span></p>
<p id="t2553" class="stm mis">  <span class="nam">axes</span> <span class="op">=</span> <span class="op">(</span><span class="nam">axes</span> <span class="op">+</span> <span class="nam">input_rank</span><span class="op">)</span> <span class="op">%</span> <span class="nam">input_rank</span><span class="strut">&nbsp;</span></p>
<p id="t2554" class="stm mis">  <span class="nam">axes_shape</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">shape</span><span class="op">(</span><span class="nam">axes</span><span class="op">)</span>  <span class="com"># [2]</span><span class="strut">&nbsp;</span></p>
<p id="t2555" class="stm mis">  <span class="key">return</span> <span class="nam">gen_data_flow_ops</span><span class="op">.</span><span class="nam">dynamic_stitch</span><span class="op">(</span>  <span class="com"># [2, 1, 1, 7]</span><span class="strut">&nbsp;</span></p>
<p id="t2556" class="pln">      <span class="op">[</span><span class="strut">&nbsp;</span></p>
<p id="t2557" class="pln">          <span class="nam">range</span><span class="op">(</span><span class="nam">input_rank</span><span class="op">)</span><span class="op">,</span>  <span class="com"># [0, 1, 2, 3]</span><span class="strut">&nbsp;</span></p>
<p id="t2558" class="pln">          <span class="nam">axes</span><span class="strut">&nbsp;</span></p>
<p id="t2559" class="pln">      <span class="op">]</span><span class="op">,</span>  <span class="com"># [1, 2]</span><span class="strut">&nbsp;</span></p>
<p id="t2560" class="pln">      <span class="op">[</span><span class="strut">&nbsp;</span></p>
<p id="t2561" class="pln">          <span class="nam">input_shape</span><span class="op">,</span>  <span class="com"># [2, 3, 5, 7]</span><span class="strut">&nbsp;</span></p>
<p id="t2562" class="pln">          <span class="nam">array_ops</span><span class="op">.</span><span class="nam">fill</span><span class="op">(</span><span class="nam">axes_shape</span><span class="op">,</span> <span class="num">1</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2563" class="pln">      <span class="op">]</span><span class="op">)</span>  <span class="com"># [1, 1]</span><span class="strut">&nbsp;</span></p>
<p id="t2564" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2565" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2566" class="stm run hide_run"><span class="key">def</span> <span class="nam">_unsorted_segment_N</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">num_segments</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2567" class="pln">  <span class="str">""" Helper function for unsorted_segment_mean/_sqrtN. Computes the number</span><span class="strut">&nbsp;</span></p>
<p id="t2568" class="pln"><span class="str">      of segment entries with 0-entries set to 1 to allow division by N.</span><span class="strut">&nbsp;</span></p>
<p id="t2569" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2570" class="pln">  <span class="com"># bincount doesn't support negative indices so we use unsorted_segment_sum</span><span class="strut">&nbsp;</span></p>
<p id="t2571" class="stm mis">  <span class="nam">segment_ids_shape</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">shape_internal</span><span class="op">(</span><span class="nam">segment_ids</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2572" class="stm mis">  <span class="nam">ones_tensor</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">ones</span><span class="op">(</span><span class="nam">segment_ids_shape</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">data</span><span class="op">.</span><span class="nam">dtype</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2573" class="stm mis">  <span class="nam">N</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">unsorted_segment_sum</span><span class="op">(</span><span class="nam">ones_tensor</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">num_segments</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2574" class="pln">  <span class="com"># add dimensions for all non-reduced axes</span><span class="strut">&nbsp;</span></p>
<p id="t2575" class="stm mis">  <span class="nam">ndims_output</span> <span class="op">=</span> <span class="nam">data</span><span class="op">.</span><span class="nam">shape</span><span class="op">.</span><span class="nam">ndims</span> <span class="op">-</span> <span class="nam">segment_ids</span><span class="op">.</span><span class="nam">shape</span><span class="op">.</span><span class="nam">ndims</span><span class="strut">&nbsp;</span></p>
<p id="t2576" class="stm mis">  <span class="nam">broadcast_shape</span> <span class="op">=</span> <span class="op">[</span><span class="nam">num_segments</span><span class="op">]</span> <span class="op">+</span> <span class="op">[</span><span class="num">1</span><span class="op">]</span> <span class="op">*</span> <span class="nam">ndims_output</span><span class="strut">&nbsp;</span></p>
<p id="t2577" class="stm mis">  <span class="nam">N</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="nam">N</span><span class="op">,</span> <span class="nam">broadcast_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2578" class="stm mis">  <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">maximum</span><span class="op">(</span><span class="nam">N</span><span class="op">,</span> <span class="num">1</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2579" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2580" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2581" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.unsorted_segment_mean"</span><span class="op">,</span> <span class="str">"unsorted_segment_mean"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2582" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"unsorted_segment_mean"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2583" class="stm run hide_run"><span class="key">def</span> <span class="nam">unsorted_segment_mean</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">num_segments</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2584" class="pln">  <span class="str">r"""Computes the mean along segments of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t2585" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2586" class="pln"><span class="str">  Read [the section on</span><span class="strut">&nbsp;</span></p>
<p id="t2587" class="pln"><span class="str">  segmentation](https://tensorflow.org/api_guides/python/math_ops#segmentation)</span><span class="strut">&nbsp;</span></p>
<p id="t2588" class="pln"><span class="str">  for an explanation of segments.</span><span class="strut">&nbsp;</span></p>
<p id="t2589" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2590" class="pln"><span class="str">  This operator is similar to the unsorted segment sum operator found</span><span class="strut">&nbsp;</span></p>
<p id="t2591" class="pln"><span class="str">  [here](../../../api_docs/python/math_ops.md#UnsortedSegmentSum).</span><span class="strut">&nbsp;</span></p>
<p id="t2592" class="pln"><span class="str">  Instead of computing the sum over segments, it computes the mean of all</span><span class="strut">&nbsp;</span></p>
<p id="t2593" class="pln"><span class="str">  entries belonging to a segment such that:</span><span class="strut">&nbsp;</span></p>
<p id="t2594" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2595" class="pln"><span class="str">  \\(output_i = 1/N_i \sum_{j...} data[j...]\\) where the sum is over tuples</span><span class="strut">&nbsp;</span></p>
<p id="t2596" class="pln"><span class="str">  `j...` such that `segment_ids[j...] == i` with \\N_i\\ being the number of</span><span class="strut">&nbsp;</span></p>
<p id="t2597" class="pln"><span class="str">  occurrences of id \\i\\.</span><span class="strut">&nbsp;</span></p>
<p id="t2598" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2599" class="pln"><span class="str">  If there is no entry for a given segment ID `i`, it outputs 0.</span><span class="strut">&nbsp;</span></p>
<p id="t2600" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2601" class="pln"><span class="str">  If the given segment ID `i` is negative, the value is dropped and will not</span><span class="strut">&nbsp;</span></p>
<p id="t2602" class="pln"><span class="str">  be added to the sum of the segment.</span><span class="strut">&nbsp;</span></p>
<p id="t2603" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2604" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2605" class="pln"><span class="str">    data: A `Tensor` with floating point or complex dtype.</span><span class="strut">&nbsp;</span></p>
<p id="t2606" class="pln"><span class="str">    segment_ids: An integer tensor whose shape is a prefix of `data.shape`.</span><span class="strut">&nbsp;</span></p>
<p id="t2607" class="pln"><span class="str">    num_segments: An integer scalar `Tensor`.  The number of distinct</span><span class="strut">&nbsp;</span></p>
<p id="t2608" class="pln"><span class="str">      segment IDs.</span><span class="strut">&nbsp;</span></p>
<p id="t2609" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2610" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2611" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2612" class="pln"><span class="str">    A `Tensor`.  Has same shape as data, except for the first `segment_ids.rank`</span><span class="strut">&nbsp;</span></p>
<p id="t2613" class="pln"><span class="str">    dimensions, which are replaced with a single dimension which has size</span><span class="strut">&nbsp;</span></p>
<p id="t2614" class="pln"><span class="str">   `num_segments`.</span><span class="strut">&nbsp;</span></p>
<p id="t2615" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2616" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"UnsortedSegmentMean"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2617" class="stm mis">    <span class="nam">data</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">data</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2618" class="stm mis">    <span class="nam">segment_ids</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">segment_ids</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2619" class="stm mis">    <span class="nam">N</span> <span class="op">=</span> <span class="nam">_unsorted_segment_N</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">num_segments</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2620" class="stm mis">    <span class="nam">summed</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">unsorted_segment_sum</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">num_segments</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2621" class="stm mis">    <span class="key">return</span> <span class="nam">summed</span> <span class="op">/</span> <span class="nam">N</span><span class="strut">&nbsp;</span></p>
<p id="t2622" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2623" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2624" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.unsorted_segment_sqrt_n"</span><span class="op">,</span> <span class="str">"unsorted_segment_sqrt_n"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2625" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"unsorted_segment_sqrt_n"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2626" class="stm run hide_run"><span class="key">def</span> <span class="nam">unsorted_segment_sqrt_n</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">num_segments</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2627" class="pln">  <span class="str">r"""Computes the sum along segments of a tensor divided by the sqrt(N).</span><span class="strut">&nbsp;</span></p>
<p id="t2628" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2629" class="pln"><span class="str">  Read [the section on</span><span class="strut">&nbsp;</span></p>
<p id="t2630" class="pln"><span class="str">  segmentation](https://tensorflow.org/api_guides/python/math_ops#segmentation)</span><span class="strut">&nbsp;</span></p>
<p id="t2631" class="pln"><span class="str">  for an explanation of segments.</span><span class="strut">&nbsp;</span></p>
<p id="t2632" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2633" class="pln"><span class="str">  This operator is similar to the unsorted segment sum operator found</span><span class="strut">&nbsp;</span></p>
<p id="t2634" class="pln"><span class="str">  [here](../../../api_docs/python/math_ops.md#UnsortedSegmentSum).</span><span class="strut">&nbsp;</span></p>
<p id="t2635" class="pln"><span class="str">  Additionally to computing the sum over segments, it divides the results by</span><span class="strut">&nbsp;</span></p>
<p id="t2636" class="pln"><span class="str">  sqrt(N).</span><span class="strut">&nbsp;</span></p>
<p id="t2637" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2638" class="pln"><span class="str">  \\(output_i = 1/sqrt(N_i) \sum_{j...} data[j...]\\) where the sum is over</span><span class="strut">&nbsp;</span></p>
<p id="t2639" class="pln"><span class="str">  tuples `j...` such that `segment_ids[j...] == i` with \\N_i\\ being the</span><span class="strut">&nbsp;</span></p>
<p id="t2640" class="pln"><span class="str">  number of occurrences of id \\i\\.</span><span class="strut">&nbsp;</span></p>
<p id="t2641" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2642" class="pln"><span class="str">  If there is no entry for a given segment ID `i`, it outputs 0.</span><span class="strut">&nbsp;</span></p>
<p id="t2643" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2644" class="pln"><span class="str">  Note that this op only supports floating point and complex dtypes,</span><span class="strut">&nbsp;</span></p>
<p id="t2645" class="pln"><span class="str">  due to tf.sqrt only supporting these types.</span><span class="strut">&nbsp;</span></p>
<p id="t2646" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2647" class="pln"><span class="str">  If the given segment ID `i` is negative, the value is dropped and will not</span><span class="strut">&nbsp;</span></p>
<p id="t2648" class="pln"><span class="str">  be added to the sum of the segment.</span><span class="strut">&nbsp;</span></p>
<p id="t2649" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2650" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2651" class="pln"><span class="str">    data: A `Tensor` with floating point or complex dtype.</span><span class="strut">&nbsp;</span></p>
<p id="t2652" class="pln"><span class="str">    segment_ids: An integer tensor whose shape is a prefix of `data.shape`.</span><span class="strut">&nbsp;</span></p>
<p id="t2653" class="pln"><span class="str">    num_segments: An integer scalar `Tensor`.  The number of distinct</span><span class="strut">&nbsp;</span></p>
<p id="t2654" class="pln"><span class="str">      segment IDs.</span><span class="strut">&nbsp;</span></p>
<p id="t2655" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2656" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2657" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2658" class="pln"><span class="str">    A `Tensor`.  Has same shape as data, except for the first `segment_ids.rank`</span><span class="strut">&nbsp;</span></p>
<p id="t2659" class="pln"><span class="str">    dimensions, which are replaced with a single dimension which has size</span><span class="strut">&nbsp;</span></p>
<p id="t2660" class="pln"><span class="str">   `num_segments`.</span><span class="strut">&nbsp;</span></p>
<p id="t2661" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2662" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"UnsortedSegmentSqrtN"</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2663" class="stm mis">    <span class="nam">data</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">data</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2664" class="stm mis">    <span class="nam">segment_ids</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">segment_ids</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2665" class="stm mis">    <span class="nam">N</span> <span class="op">=</span> <span class="nam">_unsorted_segment_N</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">num_segments</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2666" class="stm mis">    <span class="nam">summed</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">unsorted_segment_sum</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">num_segments</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2667" class="stm mis">    <span class="key">return</span> <span class="nam">summed</span> <span class="op">/</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sqrt</span><span class="op">(</span><span class="nam">N</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2668" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2669" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2670" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"sparse.segment_sum"</span><span class="op">,</span> <span class="str">"sparse_segment_sum"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2671" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"sparse_segment_sum"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2672" class="stm run hide_run"><span class="key">def</span> <span class="nam">sparse_segment_sum</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span> <span class="nam">indices</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2673" class="pln">                       <span class="nam">num_segments</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2674" class="pln">  <span class="str">r"""Computes the sum along sparse segments of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t2675" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2676" class="pln"><span class="str">  Read [the section on</span><span class="strut">&nbsp;</span></p>
<p id="t2677" class="pln"><span class="str">  segmentation](https://tensorflow.org/api_guides/python/math_ops#Segmentation)</span><span class="strut">&nbsp;</span></p>
<p id="t2678" class="pln"><span class="str">  for an explanation of segments.</span><span class="strut">&nbsp;</span></p>
<p id="t2679" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2680" class="pln"><span class="str">  Like `SegmentSum`, but `segment_ids` can have rank less than `data`'s first</span><span class="strut">&nbsp;</span></p>
<p id="t2681" class="pln"><span class="str">  dimension, selecting a subset of dimension 0, specified by `indices`.</span><span class="strut">&nbsp;</span></p>
<p id="t2682" class="pln"><span class="str">  `segment_ids` is allowed to have missing ids, in which case the output will</span><span class="strut">&nbsp;</span></p>
<p id="t2683" class="pln"><span class="str">  be zeros at those indices. In those cases `num_segments` is used to determine</span><span class="strut">&nbsp;</span></p>
<p id="t2684" class="pln"><span class="str">  the size of the output.</span><span class="strut">&nbsp;</span></p>
<p id="t2685" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2686" class="pln"><span class="str">  For example:</span><span class="strut">&nbsp;</span></p>
<p id="t2687" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2688" class="pln"><span class="str">  ```python</span><span class="strut">&nbsp;</span></p>
<p id="t2689" class="pln"><span class="str">  c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])</span><span class="strut">&nbsp;</span></p>
<p id="t2690" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2691" class="pln"><span class="str">  # Select two rows, one segment.</span><span class="strut">&nbsp;</span></p>
<p id="t2692" class="pln"><span class="str">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))</span><span class="strut">&nbsp;</span></p>
<p id="t2693" class="pln"><span class="str">  # => [[0 0 0 0]]</span><span class="strut">&nbsp;</span></p>
<p id="t2694" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2695" class="pln"><span class="str">  # Select two rows, two segment.</span><span class="strut">&nbsp;</span></p>
<p id="t2696" class="pln"><span class="str">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))</span><span class="strut">&nbsp;</span></p>
<p id="t2697" class="pln"><span class="str">  # => [[ 1  2  3  4]</span><span class="strut">&nbsp;</span></p>
<p id="t2698" class="pln"><span class="str">  #     [-1 -2 -3 -4]]</span><span class="strut">&nbsp;</span></p>
<p id="t2699" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2700" class="pln"><span class="str">  # With missing segment ids.</span><span class="strut">&nbsp;</span></p>
<p id="t2701" class="pln"><span class="str">  tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),</span><span class="strut">&nbsp;</span></p>
<p id="t2702" class="pln"><span class="str">                        num_segments=4)</span><span class="strut">&nbsp;</span></p>
<p id="t2703" class="pln"><span class="str">  # => [[ 1  2  3  4]</span><span class="strut">&nbsp;</span></p>
<p id="t2704" class="pln"><span class="str">  #     [ 0  0  0  0]</span><span class="strut">&nbsp;</span></p>
<p id="t2705" class="pln"><span class="str">  #     [-1 -2 -3 -4]</span><span class="strut">&nbsp;</span></p>
<p id="t2706" class="pln"><span class="str">  #     [ 0  0  0  0]]</span><span class="strut">&nbsp;</span></p>
<p id="t2707" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2708" class="pln"><span class="str">  # Select all rows, two segments.</span><span class="strut">&nbsp;</span></p>
<p id="t2709" class="pln"><span class="str">  tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))</span><span class="strut">&nbsp;</span></p>
<p id="t2710" class="pln"><span class="str">  # => [[0 0 0 0]</span><span class="strut">&nbsp;</span></p>
<p id="t2711" class="pln"><span class="str">  #     [5 6 7 8]]</span><span class="strut">&nbsp;</span></p>
<p id="t2712" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2713" class="pln"><span class="str">  # Which is equivalent to:</span><span class="strut">&nbsp;</span></p>
<p id="t2714" class="pln"><span class="str">  tf.segment_sum(c, tf.constant([0, 0, 1]))</span><span class="strut">&nbsp;</span></p>
<p id="t2715" class="pln"><span class="str">  ```</span><span class="strut">&nbsp;</span></p>
<p id="t2716" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2717" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2718" class="pln"><span class="str">    data: A `Tensor` with data that will be assembled in the output.</span><span class="strut">&nbsp;</span></p>
<p id="t2719" class="pln"><span class="str">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span><span class="strut">&nbsp;</span></p>
<p id="t2720" class="pln"><span class="str">      `segment_ids`.</span><span class="strut">&nbsp;</span></p>
<p id="t2721" class="pln"><span class="str">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t2722" class="pln"><span class="str">      Values should be sorted and can be repeated.</span><span class="strut">&nbsp;</span></p>
<p id="t2723" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2724" class="pln"><span class="str">    num_segments: An optional int32 scalar. Indicates the size of the output</span><span class="strut">&nbsp;</span></p>
<p id="t2725" class="pln"><span class="str">      `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t2726" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2727" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2728" class="pln"><span class="str">    A `tensor` of the shape as data, except for dimension 0 which</span><span class="strut">&nbsp;</span></p>
<p id="t2729" class="pln"><span class="str">    has size `k`, the number of segments specified via `num_segments` or</span><span class="strut">&nbsp;</span></p>
<p id="t2730" class="pln"><span class="str">    inferred for the last element in `segments_ids`.</span><span class="strut">&nbsp;</span></p>
<p id="t2731" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2732" class="stm mis">  <span class="key">if</span> <span class="nam">num_segments</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2733" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sparse_segment_sum_with_num_segments</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2734" class="pln">        <span class="nam">data</span><span class="op">=</span><span class="nam">data</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2735" class="pln">        <span class="nam">indices</span><span class="op">=</span><span class="nam">indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2736" class="pln">        <span class="nam">segment_ids</span><span class="op">=</span><span class="nam">segment_ids</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2737" class="pln">        <span class="nam">num_segments</span><span class="op">=</span><span class="nam">num_segments</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2738" class="pln">        <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2739" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2740" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sparse_segment_sum</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2741" class="pln">        <span class="nam">data</span><span class="op">=</span><span class="nam">data</span><span class="op">,</span> <span class="nam">indices</span><span class="op">=</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">=</span><span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2742" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2743" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2744" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"sparse.segment_mean"</span><span class="op">,</span> <span class="str">"sparse_segment_mean"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2745" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"sparse_segment_mean"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2746" class="stm run hide_run"><span class="key">def</span> <span class="nam">sparse_segment_mean</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2747" class="pln">                        <span class="nam">indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2748" class="pln">                        <span class="nam">segment_ids</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2749" class="pln">                        <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2750" class="pln">                        <span class="nam">num_segments</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2751" class="pln">  <span class="str">r"""Computes the mean along sparse segments of a tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t2752" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2753" class="pln"><span class="str">  Read [the section on</span><span class="strut">&nbsp;</span></p>
<p id="t2754" class="pln"><span class="str">  segmentation](https://tensorflow.org/api_guides/python/math_ops#Segmentation)</span><span class="strut">&nbsp;</span></p>
<p id="t2755" class="pln"><span class="str">  for an explanation of segments.</span><span class="strut">&nbsp;</span></p>
<p id="t2756" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2757" class="pln"><span class="str">  Like `SegmentMean`, but `segment_ids` can have rank less than `data`'s first</span><span class="strut">&nbsp;</span></p>
<p id="t2758" class="pln"><span class="str">  dimension, selecting a subset of dimension 0, specified by `indices`.</span><span class="strut">&nbsp;</span></p>
<p id="t2759" class="pln"><span class="str">  `segment_ids` is allowed to have missing ids, in which case the output will</span><span class="strut">&nbsp;</span></p>
<p id="t2760" class="pln"><span class="str">  be zeros at those indices. In those cases `num_segments` is used to determine</span><span class="strut">&nbsp;</span></p>
<p id="t2761" class="pln"><span class="str">  the size of the output.</span><span class="strut">&nbsp;</span></p>
<p id="t2762" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2763" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2764" class="pln"><span class="str">    data: A `Tensor` with data that will be assembled in the output.</span><span class="strut">&nbsp;</span></p>
<p id="t2765" class="pln"><span class="str">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span><span class="strut">&nbsp;</span></p>
<p id="t2766" class="pln"><span class="str">      `segment_ids`.</span><span class="strut">&nbsp;</span></p>
<p id="t2767" class="pln"><span class="str">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t2768" class="pln"><span class="str">      Values should be sorted and can be repeated.</span><span class="strut">&nbsp;</span></p>
<p id="t2769" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2770" class="pln"><span class="str">    num_segments: An optional int32 scalar. Indicates the size of the output</span><span class="strut">&nbsp;</span></p>
<p id="t2771" class="pln"><span class="str">      `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t2772" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2773" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2774" class="pln"><span class="str">    A `tensor` of the shape as data, except for dimension 0 which</span><span class="strut">&nbsp;</span></p>
<p id="t2775" class="pln"><span class="str">    has size `k`, the number of segments specified via `num_segments` or</span><span class="strut">&nbsp;</span></p>
<p id="t2776" class="pln"><span class="str">    inferred for the last element in `segments_ids`.</span><span class="strut">&nbsp;</span></p>
<p id="t2777" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2778" class="stm mis">  <span class="key">if</span> <span class="nam">num_segments</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2779" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sparse_segment_mean_with_num_segments</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2780" class="pln">        <span class="nam">data</span><span class="op">=</span><span class="nam">data</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2781" class="pln">        <span class="nam">indices</span><span class="op">=</span><span class="nam">indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2782" class="pln">        <span class="nam">segment_ids</span><span class="op">=</span><span class="nam">segment_ids</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2783" class="pln">        <span class="nam">num_segments</span><span class="op">=</span><span class="nam">num_segments</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2784" class="pln">        <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2785" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2786" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sparse_segment_mean</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2787" class="pln">        <span class="nam">data</span><span class="op">=</span><span class="nam">data</span><span class="op">,</span> <span class="nam">indices</span><span class="op">=</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">=</span><span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2788" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2789" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2790" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"sparse.segment_sqrt_n"</span><span class="op">,</span> <span class="str">"sparse_segment_sqrt_n"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2791" class="stm run hide_run"><span class="op">@</span><span class="nam">deprecation</span><span class="op">.</span><span class="nam">deprecated_endpoints</span><span class="op">(</span><span class="str">"sparse_segment_sqrt_n"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2792" class="stm run hide_run"><span class="key">def</span> <span class="nam">sparse_segment_sqrt_n</span><span class="op">(</span><span class="nam">data</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2793" class="pln">                          <span class="nam">indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2794" class="pln">                          <span class="nam">segment_ids</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2795" class="pln">                          <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2796" class="pln">                          <span class="nam">num_segments</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2797" class="pln">  <span class="str">r"""Computes the sum along sparse segments of a tensor divided by the sqrt(N).</span><span class="strut">&nbsp;</span></p>
<p id="t2798" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2799" class="pln"><span class="str">  `N` is the size of the segment being reduced.</span><span class="strut">&nbsp;</span></p>
<p id="t2800" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2801" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2802" class="pln"><span class="str">    data: A `Tensor` with data that will be assembled in the output.</span><span class="strut">&nbsp;</span></p>
<p id="t2803" class="pln"><span class="str">    indices: A 1-D `Tensor` with indices into `data`. Has same rank as</span><span class="strut">&nbsp;</span></p>
<p id="t2804" class="pln"><span class="str">      `segment_ids`.</span><span class="strut">&nbsp;</span></p>
<p id="t2805" class="pln"><span class="str">    segment_ids: A 1-D `Tensor` with indices into the output `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t2806" class="pln"><span class="str">      Values should be sorted and can be repeated.</span><span class="strut">&nbsp;</span></p>
<p id="t2807" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2808" class="pln"><span class="str">    num_segments: An optional int32 scalar. Indicates the size of the output</span><span class="strut">&nbsp;</span></p>
<p id="t2809" class="pln"><span class="str">      `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t2810" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2811" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2812" class="pln"><span class="str">    A `tensor` of the shape as data, except for dimension 0 which</span><span class="strut">&nbsp;</span></p>
<p id="t2813" class="pln"><span class="str">    has size `k`, the number of segments specified via `num_segments` or</span><span class="strut">&nbsp;</span></p>
<p id="t2814" class="pln"><span class="str">    inferred for the last element in `segments_ids`.</span><span class="strut">&nbsp;</span></p>
<p id="t2815" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2816" class="stm mis">  <span class="key">if</span> <span class="nam">num_segments</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2817" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sparse_segment_sqrt_n_with_num_segments</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2818" class="pln">        <span class="nam">data</span><span class="op">=</span><span class="nam">data</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2819" class="pln">        <span class="nam">indices</span><span class="op">=</span><span class="nam">indices</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2820" class="pln">        <span class="nam">segment_ids</span><span class="op">=</span><span class="nam">segment_ids</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2821" class="pln">        <span class="nam">num_segments</span><span class="op">=</span><span class="nam">num_segments</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2822" class="pln">        <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2823" class="pln">  <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2824" class="stm mis">    <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">sparse_segment_sqrt_n</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2825" class="pln">        <span class="nam">data</span><span class="op">=</span><span class="nam">data</span><span class="op">,</span> <span class="nam">indices</span><span class="op">=</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">segment_ids</span><span class="op">=</span><span class="nam">segment_ids</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2826" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2827" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2828" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"tensordot"</span><span class="op">,</span> <span class="str">"linalg.tensordot"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2829" class="stm run hide_run"><span class="key">def</span> <span class="nam">tensordot</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">b</span><span class="op">,</span> <span class="nam">axes</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2830" class="pln">  <span class="str">r"""Tensor contraction of a and b along specified axes.</span><span class="strut">&nbsp;</span></p>
<p id="t2831" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2832" class="pln"><span class="str">  Tensordot (also known as tensor contraction) sums the product of elements</span><span class="strut">&nbsp;</span></p>
<p id="t2833" class="pln"><span class="str">  from `a` and `b` over the indices specified by `a_axes` and `b_axes`.</span><span class="strut">&nbsp;</span></p>
<p id="t2834" class="pln"><span class="str">  The lists `a_axes` and `b_axes` specify those pairs of axes along which to</span><span class="strut">&nbsp;</span></p>
<p id="t2835" class="pln"><span class="str">  contract the tensors. The axis `a_axes[i]` of `a` must have the same dimension</span><span class="strut">&nbsp;</span></p>
<p id="t2836" class="pln"><span class="str">  as axis `b_axes[i]` of `b` for all `i` in `range(0, len(a_axes))`. The lists</span><span class="strut">&nbsp;</span></p>
<p id="t2837" class="pln"><span class="str">  `a_axes` and `b_axes` must have identical length and consist of unique</span><span class="strut">&nbsp;</span></p>
<p id="t2838" class="pln"><span class="str">  integers that specify valid axes for each of the tensors.</span><span class="strut">&nbsp;</span></p>
<p id="t2839" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2840" class="pln"><span class="str">  This operation corresponds to `numpy.tensordot(a, b, axes)`.</span><span class="strut">&nbsp;</span></p>
<p id="t2841" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2842" class="pln"><span class="str">  Example 1: When `a` and `b` are matrices (order 2), the case `axes = 1`</span><span class="strut">&nbsp;</span></p>
<p id="t2843" class="pln"><span class="str">  is equivalent to matrix multiplication.</span><span class="strut">&nbsp;</span></p>
<p id="t2844" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2845" class="pln"><span class="str">  Example 2: When `a` and `b` are matrices (order 2), the case</span><span class="strut">&nbsp;</span></p>
<p id="t2846" class="pln"><span class="str">  `axes = [[1], [0]]` is equivalent to matrix multiplication.</span><span class="strut">&nbsp;</span></p>
<p id="t2847" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2848" class="pln"><span class="str">  Example 3: Suppose that \\(a_{ijk}\\) and \\(b_{lmn}\\) represent two</span><span class="strut">&nbsp;</span></p>
<p id="t2849" class="pln"><span class="str">  tensors of order 3. Then, `contract(a, b, [[0], [2]])` is the order 4 tensor</span><span class="strut">&nbsp;</span></p>
<p id="t2850" class="pln"><span class="str">  \\(c_{jklm}\\) whose entry</span><span class="strut">&nbsp;</span></p>
<p id="t2851" class="pln"><span class="str">  corresponding to the indices \\((j,k,l,m)\\) is given by:</span><span class="strut">&nbsp;</span></p>
<p id="t2852" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2853" class="pln"><span class="str">  \\( c_{jklm} = \sum_i a_{ijk} b_{lmi} \\).</span><span class="strut">&nbsp;</span></p>
<p id="t2854" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2855" class="pln"><span class="str">  In general, `order(c) = order(a) + order(b) - 2*len(axes[0])`.</span><span class="strut">&nbsp;</span></p>
<p id="t2856" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2857" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2858" class="pln"><span class="str">    a: `Tensor` of type `float32` or `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t2859" class="pln"><span class="str">    b: `Tensor` with the same type as `a`.</span><span class="strut">&nbsp;</span></p>
<p id="t2860" class="pln"><span class="str">    axes: Either a scalar `N`, or a list or an `int32` `Tensor` of shape [2, k].</span><span class="strut">&nbsp;</span></p>
<p id="t2861" class="pln"><span class="str">     If axes is a scalar, sum over the last N axes of a and the first N axes</span><span class="strut">&nbsp;</span></p>
<p id="t2862" class="pln"><span class="str">     of b in order.</span><span class="strut">&nbsp;</span></p>
<p id="t2863" class="pln"><span class="str">     If axes is a list or `Tensor` the first and second row contain the set of</span><span class="strut">&nbsp;</span></p>
<p id="t2864" class="pln"><span class="str">     unique integers specifying axes along which the contraction is computed,</span><span class="strut">&nbsp;</span></p>
<p id="t2865" class="pln"><span class="str">     for `a` and `b`, respectively. The number of axes for `a` and `b` must</span><span class="strut">&nbsp;</span></p>
<p id="t2866" class="pln"><span class="str">     be equal.</span><span class="strut">&nbsp;</span></p>
<p id="t2867" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t2868" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2869" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2870" class="pln"><span class="str">    A `Tensor` with the same type as `a`.</span><span class="strut">&nbsp;</span></p>
<p id="t2871" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2872" class="pln"><span class="str">  Raises:</span><span class="strut">&nbsp;</span></p>
<p id="t2873" class="pln"><span class="str">    ValueError: If the shapes of `a`, `b`, and `axes` are incompatible.</span><span class="strut">&nbsp;</span></p>
<p id="t2874" class="pln"><span class="str">    IndexError: If the values in axes exceed the rank of the corresponding</span><span class="strut">&nbsp;</span></p>
<p id="t2875" class="pln"><span class="str">      tensor.</span><span class="strut">&nbsp;</span></p>
<p id="t2876" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t2877" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2878" class="stm mis">  <span class="key">def</span> <span class="nam">_tensordot_reshape</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">axes</span><span class="op">,</span> <span class="nam">flipped</span><span class="op">=</span><span class="key">False</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2879" class="pln">    <span class="str">"""Helper method to perform transpose and reshape for contraction op.</span><span class="strut">&nbsp;</span></p>
<p id="t2880" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2881" class="pln"><span class="str">    This method is helpful in reducing `math_ops.tensordot` to `math_ops.matmul`</span><span class="strut">&nbsp;</span></p>
<p id="t2882" class="pln"><span class="str">    using `array_ops.transpose` and `array_ops.reshape`. The method takes a</span><span class="strut">&nbsp;</span></p>
<p id="t2883" class="pln"><span class="str">    tensor and performs the correct transpose and reshape operation for a given</span><span class="strut">&nbsp;</span></p>
<p id="t2884" class="pln"><span class="str">    set of indices. It returns the reshaped tensor as well as a list of indices</span><span class="strut">&nbsp;</span></p>
<p id="t2885" class="pln"><span class="str">    necessary to reshape the tensor again after matrix multiplication.</span><span class="strut">&nbsp;</span></p>
<p id="t2886" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2887" class="pln"><span class="str">    Args:</span><span class="strut">&nbsp;</span></p>
<p id="t2888" class="pln"><span class="str">      a: `Tensor`.</span><span class="strut">&nbsp;</span></p>
<p id="t2889" class="pln"><span class="str">      axes: List or `int32` `Tensor` of unique indices specifying valid axes of</span><span class="strut">&nbsp;</span></p>
<p id="t2890" class="pln"><span class="str">       `a`.</span><span class="strut">&nbsp;</span></p>
<p id="t2891" class="pln"><span class="str">      flipped: An optional `bool`. Defaults to `False`. If `True`, the method</span><span class="strut">&nbsp;</span></p>
<p id="t2892" class="pln"><span class="str">        assumes that `a` is the second argument in the contraction operation.</span><span class="strut">&nbsp;</span></p>
<p id="t2893" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2894" class="pln"><span class="str">    Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t2895" class="pln"><span class="str">      A tuple `(reshaped_a, free_dims, free_dims_static)` where `reshaped_a` is</span><span class="strut">&nbsp;</span></p>
<p id="t2896" class="pln"><span class="str">      the tensor `a` reshaped to allow contraction via `matmul`, `free_dims` is</span><span class="strut">&nbsp;</span></p>
<p id="t2897" class="pln"><span class="str">      either a list of integers or an `int32` `Tensor`, depending on whether</span><span class="strut">&nbsp;</span></p>
<p id="t2898" class="pln"><span class="str">      the shape of a is fully specified, and free_dims_static is either a list</span><span class="strut">&nbsp;</span></p>
<p id="t2899" class="pln"><span class="str">      of integers and None values, or None, representing the inferred</span><span class="strut">&nbsp;</span></p>
<p id="t2900" class="pln"><span class="str">      static shape of the free dimensions</span><span class="strut">&nbsp;</span></p>
<p id="t2901" class="pln"><span class="str">    """</span><span class="strut">&nbsp;</span></p>
<p id="t2902" class="stm mis">    <span class="key">if</span> <span class="nam">a</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">is_fully_defined</span><span class="op">(</span><span class="op">)</span> <span class="key">and</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">axes</span><span class="op">,</span> <span class="op">(</span><span class="nam">list</span><span class="op">,</span> <span class="nam">tuple</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2903" class="stm mis">      <span class="nam">shape_a</span> <span class="op">=</span> <span class="nam">a</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">as_list</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2904" class="stm mis">      <span class="nam">axes</span> <span class="op">=</span> <span class="op">[</span><span class="nam">i</span> <span class="key">if</span> <span class="nam">i</span> <span class="op">>=</span> <span class="num">0</span> <span class="key">else</span> <span class="nam">i</span> <span class="op">+</span> <span class="nam">len</span><span class="op">(</span><span class="nam">shape_a</span><span class="op">)</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">axes</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2905" class="stm mis">      <span class="nam">free</span> <span class="op">=</span> <span class="op">[</span><span class="nam">i</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">xrange</span><span class="op">(</span><span class="nam">len</span><span class="op">(</span><span class="nam">shape_a</span><span class="op">)</span><span class="op">)</span> <span class="key">if</span> <span class="nam">i</span> <span class="key">not</span> <span class="key">in</span> <span class="nam">axes</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2906" class="stm mis">      <span class="nam">free_dims</span> <span class="op">=</span> <span class="op">[</span><span class="nam">shape_a</span><span class="op">[</span><span class="nam">i</span><span class="op">]</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">free</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2907" class="stm mis">      <span class="nam">prod_free</span> <span class="op">=</span> <span class="nam">int</span><span class="op">(</span><span class="nam">np</span><span class="op">.</span><span class="nam">prod</span><span class="op">(</span><span class="op">[</span><span class="nam">shape_a</span><span class="op">[</span><span class="nam">i</span><span class="op">]</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">free</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2908" class="stm mis">      <span class="nam">prod_axes</span> <span class="op">=</span> <span class="nam">int</span><span class="op">(</span><span class="nam">np</span><span class="op">.</span><span class="nam">prod</span><span class="op">(</span><span class="op">[</span><span class="nam">shape_a</span><span class="op">[</span><span class="nam">i</span><span class="op">]</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">axes</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2909" class="stm mis">      <span class="nam">perm</span> <span class="op">=</span> <span class="nam">list</span><span class="op">(</span><span class="nam">axes</span><span class="op">)</span> <span class="op">+</span> <span class="nam">free</span> <span class="key">if</span> <span class="nam">flipped</span> <span class="key">else</span> <span class="nam">free</span> <span class="op">+</span> <span class="nam">list</span><span class="op">(</span><span class="nam">axes</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2910" class="stm mis">      <span class="nam">new_shape</span> <span class="op">=</span> <span class="op">[</span><span class="nam">prod_axes</span><span class="op">,</span> <span class="nam">prod_free</span><span class="op">]</span> <span class="key">if</span> <span class="nam">flipped</span> <span class="key">else</span> <span class="op">[</span><span class="nam">prod_free</span><span class="op">,</span> <span class="nam">prod_axes</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2911" class="stm mis">      <span class="nam">reshaped_a</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="nam">array_ops</span><span class="op">.</span><span class="nam">transpose</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">perm</span><span class="op">)</span><span class="op">,</span> <span class="nam">new_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2912" class="stm mis">      <span class="key">return</span> <span class="nam">reshaped_a</span><span class="op">,</span> <span class="nam">free_dims</span><span class="op">,</span> <span class="nam">free_dims</span><span class="strut">&nbsp;</span></p>
<p id="t2913" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2914" class="stm mis">      <span class="key">if</span> <span class="nam">a</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">ndims</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span> <span class="key">and</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">axes</span><span class="op">,</span> <span class="op">(</span><span class="nam">list</span><span class="op">,</span> <span class="nam">tuple</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2915" class="stm mis">        <span class="nam">shape_a</span> <span class="op">=</span> <span class="nam">a</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="op">.</span><span class="nam">as_list</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2916" class="stm mis">        <span class="nam">axes</span> <span class="op">=</span> <span class="op">[</span><span class="nam">i</span> <span class="key">if</span> <span class="nam">i</span> <span class="op">>=</span> <span class="num">0</span> <span class="key">else</span> <span class="nam">i</span> <span class="op">+</span> <span class="nam">len</span><span class="op">(</span><span class="nam">shape_a</span><span class="op">)</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">axes</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2917" class="stm mis">        <span class="nam">free</span> <span class="op">=</span> <span class="op">[</span><span class="nam">i</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">xrange</span><span class="op">(</span><span class="nam">len</span><span class="op">(</span><span class="nam">shape_a</span><span class="op">)</span><span class="op">)</span> <span class="key">if</span> <span class="nam">i</span> <span class="key">not</span> <span class="key">in</span> <span class="nam">axes</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2918" class="stm mis">        <span class="nam">axes_dims</span> <span class="op">=</span> <span class="op">[</span><span class="nam">shape_a</span><span class="op">[</span><span class="nam">i</span><span class="op">]</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">axes</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2919" class="stm mis">        <span class="nam">free_dims</span> <span class="op">=</span> <span class="op">[</span><span class="nam">shape_a</span><span class="op">[</span><span class="nam">i</span><span class="op">]</span> <span class="key">for</span> <span class="nam">i</span> <span class="key">in</span> <span class="nam">free</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2920" class="stm mis">        <span class="nam">free_dims_static</span> <span class="op">=</span> <span class="nam">free_dims</span><span class="strut">&nbsp;</span></p>
<p id="t2921" class="stm mis">        <span class="nam">axes</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">axes</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"axes"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2922" class="stm mis">        <span class="nam">free</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">free</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"free"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2923" class="stm mis">        <span class="nam">shape_a</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">shape</span><span class="op">(</span><span class="nam">a</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2924" class="pln">      <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2925" class="stm mis">        <span class="nam">free_dims_static</span> <span class="op">=</span> <span class="key">None</span><span class="strut">&nbsp;</span></p>
<p id="t2926" class="stm mis">        <span class="nam">shape_a</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">shape</span><span class="op">(</span><span class="nam">a</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2927" class="stm mis">        <span class="nam">rank_a</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">rank</span><span class="op">(</span><span class="nam">a</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2928" class="stm mis">        <span class="nam">axes</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">axes</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"axes"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2929" class="stm mis">        <span class="nam">axes</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">where</span><span class="op">(</span><span class="nam">axes</span> <span class="op">>=</span> <span class="num">0</span><span class="op">,</span> <span class="nam">axes</span><span class="op">,</span> <span class="nam">axes</span> <span class="op">+</span> <span class="nam">rank_a</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2930" class="stm mis">        <span class="nam">free</span><span class="op">,</span> <span class="nam">_</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">setdiff1d</span><span class="op">(</span><span class="nam">range</span><span class="op">(</span><span class="nam">rank_a</span><span class="op">)</span><span class="op">,</span> <span class="nam">axes</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2931" class="stm mis">      <span class="nam">free_dims</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">gather</span><span class="op">(</span><span class="nam">shape_a</span><span class="op">,</span> <span class="nam">free</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2932" class="stm mis">      <span class="nam">axes_dims</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">gather</span><span class="op">(</span><span class="nam">shape_a</span><span class="op">,</span> <span class="nam">axes</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2933" class="stm mis">      <span class="nam">prod_free_dims</span> <span class="op">=</span> <span class="nam">reduce_prod</span><span class="op">(</span><span class="nam">free_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2934" class="stm mis">      <span class="nam">prod_axes_dims</span> <span class="op">=</span> <span class="nam">reduce_prod</span><span class="op">(</span><span class="nam">axes_dims</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2935" class="stm mis">      <span class="key">if</span> <span class="nam">flipped</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2936" class="stm mis">        <span class="nam">perm</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">concat</span><span class="op">(</span><span class="op">[</span><span class="nam">axes</span><span class="op">,</span> <span class="nam">free</span><span class="op">]</span><span class="op">,</span> <span class="num">0</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2937" class="stm mis">        <span class="nam">new_shape</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">prod_axes_dims</span><span class="op">,</span> <span class="nam">prod_free_dims</span><span class="op">]</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2938" class="pln">      <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2939" class="stm mis">        <span class="nam">perm</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">concat</span><span class="op">(</span><span class="op">[</span><span class="nam">free</span><span class="op">,</span> <span class="nam">axes</span><span class="op">]</span><span class="op">,</span> <span class="num">0</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2940" class="stm mis">        <span class="nam">new_shape</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">stack</span><span class="op">(</span><span class="op">[</span><span class="nam">prod_free_dims</span><span class="op">,</span> <span class="nam">prod_axes_dims</span><span class="op">]</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2941" class="stm mis">      <span class="nam">reshaped_a</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="nam">array_ops</span><span class="op">.</span><span class="nam">transpose</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">perm</span><span class="op">)</span><span class="op">,</span> <span class="nam">new_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2942" class="stm mis">      <span class="key">return</span> <span class="nam">reshaped_a</span><span class="op">,</span> <span class="nam">free_dims</span><span class="op">,</span> <span class="nam">free_dims_static</span><span class="strut">&nbsp;</span></p>
<p id="t2943" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2944" class="stm mis">  <span class="key">def</span> <span class="nam">_tensordot_axes</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">axes</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2945" class="pln">    <span class="str">"""Generates two sets of contraction axes for the two tensor arguments."""</span><span class="strut">&nbsp;</span></p>
<p id="t2946" class="stm mis">    <span class="nam">a_shape</span> <span class="op">=</span> <span class="nam">a</span><span class="op">.</span><span class="nam">get_shape</span><span class="op">(</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2947" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">axes</span><span class="op">,</span> <span class="nam">compat</span><span class="op">.</span><span class="nam">integral_types</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2948" class="stm mis">      <span class="key">if</span> <span class="nam">axes</span> <span class="op">&lt;</span> <span class="num">0</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2949" class="stm mis">        <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"'axes' must be at least 0."</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2950" class="stm mis">      <span class="key">if</span> <span class="nam">a_shape</span><span class="op">.</span><span class="nam">ndims</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2951" class="stm mis">        <span class="key">if</span> <span class="nam">axes</span> <span class="op">></span> <span class="nam">a_shape</span><span class="op">.</span><span class="nam">ndims</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2952" class="stm mis">          <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"'axes' must not be larger than the number of "</span><span class="strut">&nbsp;</span></p>
<p id="t2953" class="pln">                           <span class="str">"dimensions of tensor %s."</span> <span class="op">%</span> <span class="nam">a</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2954" class="stm mis">        <span class="key">return</span> <span class="op">(</span><span class="nam">list</span><span class="op">(</span><span class="nam">xrange</span><span class="op">(</span><span class="nam">a_shape</span><span class="op">.</span><span class="nam">ndims</span> <span class="op">-</span> <span class="nam">axes</span><span class="op">,</span> <span class="nam">a_shape</span><span class="op">.</span><span class="nam">ndims</span><span class="op">)</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2955" class="pln">                <span class="nam">list</span><span class="op">(</span><span class="nam">xrange</span><span class="op">(</span><span class="nam">axes</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2956" class="pln">      <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2957" class="stm mis">        <span class="nam">rank</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">rank</span><span class="op">(</span><span class="nam">a</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2958" class="stm mis">        <span class="key">return</span> <span class="op">(</span><span class="nam">range</span><span class="op">(</span><span class="nam">rank</span> <span class="op">-</span> <span class="nam">axes</span><span class="op">,</span> <span class="nam">rank</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="op">,</span><span class="strut">&nbsp;</span></p>
<p id="t2959" class="pln">                <span class="nam">range</span><span class="op">(</span><span class="nam">axes</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2960" class="stm mis">    <span class="key">elif</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">axes</span><span class="op">,</span> <span class="op">(</span><span class="nam">list</span><span class="op">,</span> <span class="nam">tuple</span><span class="op">)</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2961" class="stm mis">      <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">axes</span><span class="op">)</span> <span class="op">!=</span> <span class="num">2</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2962" class="stm mis">        <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="str">"'axes' must be an integer or have length 2."</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2963" class="stm mis">      <span class="nam">a_axes</span> <span class="op">=</span> <span class="nam">axes</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2964" class="stm mis">      <span class="nam">b_axes</span> <span class="op">=</span> <span class="nam">axes</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2965" class="stm mis">      <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">a_axes</span><span class="op">,</span> <span class="nam">compat</span><span class="op">.</span><span class="nam">integral_types</span><span class="op">)</span> <span class="key">and</span> <span class="xx">\</span><span class="strut">&nbsp;</span></p>
<p id="t2966" class="pln">          <span class="nam">isinstance</span><span class="op">(</span><span class="nam">b_axes</span><span class="op">,</span> <span class="nam">compat</span><span class="op">.</span><span class="nam">integral_types</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2967" class="stm mis">        <span class="nam">a_axes</span> <span class="op">=</span> <span class="op">[</span><span class="nam">a_axes</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2968" class="stm mis">        <span class="nam">b_axes</span> <span class="op">=</span> <span class="op">[</span><span class="nam">b_axes</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2969" class="stm mis">      <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">a_axes</span><span class="op">)</span> <span class="op">!=</span> <span class="nam">len</span><span class="op">(</span><span class="nam">b_axes</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2970" class="stm mis">        <span class="key">raise</span> <span class="nam">ValueError</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2971" class="pln">            <span class="str">"Different number of contraction axes 'a' and 'b', %s != %s."</span> <span class="op">%</span><span class="strut">&nbsp;</span></p>
<p id="t2972" class="pln">            <span class="op">(</span><span class="nam">len</span><span class="op">(</span><span class="nam">a_axes</span><span class="op">)</span><span class="op">,</span> <span class="nam">len</span><span class="op">(</span><span class="nam">b_axes</span><span class="op">)</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2973" class="stm mis">      <span class="key">return</span> <span class="nam">a_axes</span><span class="op">,</span> <span class="nam">b_axes</span><span class="strut">&nbsp;</span></p>
<p id="t2974" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2975" class="stm mis">      <span class="nam">axes</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">axes</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"axes"</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2976" class="stm mis">      <span class="key">return</span> <span class="nam">axes</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="op">,</span> <span class="nam">axes</span><span class="op">[</span><span class="num">1</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t2977" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2978" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"Tensordot"</span><span class="op">,</span> <span class="op">[</span><span class="nam">a</span><span class="op">,</span> <span class="nam">b</span><span class="op">,</span> <span class="nam">axes</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2979" class="stm mis">    <span class="nam">a</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"a"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2980" class="stm mis">    <span class="nam">b</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">b</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"b"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2981" class="stm mis">    <span class="nam">a_axes</span><span class="op">,</span> <span class="nam">b_axes</span> <span class="op">=</span> <span class="nam">_tensordot_axes</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">axes</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2982" class="stm mis">    <span class="nam">a_reshape</span><span class="op">,</span> <span class="nam">a_free_dims</span><span class="op">,</span> <span class="nam">a_free_dims_static</span> <span class="op">=</span> <span class="nam">_tensordot_reshape</span><span class="op">(</span><span class="nam">a</span><span class="op">,</span> <span class="nam">a_axes</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2983" class="stm mis">    <span class="nam">b_reshape</span><span class="op">,</span> <span class="nam">b_free_dims</span><span class="op">,</span> <span class="nam">b_free_dims_static</span> <span class="op">=</span> <span class="nam">_tensordot_reshape</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2984" class="pln">        <span class="nam">b</span><span class="op">,</span> <span class="nam">b_axes</span><span class="op">,</span> <span class="key">True</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2985" class="stm mis">    <span class="nam">ab_matmul</span> <span class="op">=</span> <span class="nam">matmul</span><span class="op">(</span><span class="nam">a_reshape</span><span class="op">,</span> <span class="nam">b_reshape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2986" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">a_free_dims</span><span class="op">,</span> <span class="nam">list</span><span class="op">)</span> <span class="key">and</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">b_free_dims</span><span class="op">,</span> <span class="nam">list</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2987" class="stm mis">      <span class="key">return</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="nam">ab_matmul</span><span class="op">,</span> <span class="nam">a_free_dims</span> <span class="op">+</span> <span class="nam">b_free_dims</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2988" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2989" class="stm mis">      <span class="nam">a_free_dims</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">a_free_dims</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2990" class="stm mis">      <span class="nam">b_free_dims</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">b_free_dims</span><span class="op">,</span> <span class="nam">dtype</span><span class="op">=</span><span class="nam">dtypes</span><span class="op">.</span><span class="nam">int32</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2991" class="stm mis">      <span class="nam">product</span> <span class="op">=</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">reshape</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t2992" class="pln">          <span class="nam">ab_matmul</span><span class="op">,</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">concat</span><span class="op">(</span><span class="op">[</span><span class="nam">a_free_dims</span><span class="op">,</span> <span class="nam">b_free_dims</span><span class="op">]</span><span class="op">,</span> <span class="num">0</span><span class="op">)</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2993" class="stm mis">      <span class="key">if</span> <span class="nam">a_free_dims_static</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span> <span class="key">and</span> <span class="nam">b_free_dims_static</span> <span class="key">is</span> <span class="key">not</span> <span class="key">None</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t2994" class="stm mis">        <span class="nam">product</span><span class="op">.</span><span class="nam">set_shape</span><span class="op">(</span><span class="nam">a_free_dims_static</span> <span class="op">+</span> <span class="nam">b_free_dims_static</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2995" class="stm mis">      <span class="key">return</span> <span class="nam">product</span><span class="strut">&nbsp;</span></p>
<p id="t2996" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2997" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t2998" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.polyval"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t2999" class="stm run hide_run"><span class="key">def</span> <span class="nam">polyval</span><span class="op">(</span><span class="nam">coeffs</span><span class="op">,</span> <span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3000" class="pln">  <span class="str">r"""Computes the elementwise value of a polynomial.</span><span class="strut">&nbsp;</span></p>
<p id="t3001" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3002" class="pln"><span class="str">  If `x` is a tensor and `coeffs` is a list n + 1 tensors, this function returns</span><span class="strut">&nbsp;</span></p>
<p id="t3003" class="pln"><span class="str">  the value of the n-th order polynomial</span><span class="strut">&nbsp;</span></p>
<p id="t3004" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3005" class="pln"><span class="str">     p(x) = coeffs[n-1] + coeffs[n-2] * x + ...  + coeffs[0] * x**(n-1)</span><span class="strut">&nbsp;</span></p>
<p id="t3006" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3007" class="pln"><span class="str">  evaluated using Horner's method, i.e.</span><span class="strut">&nbsp;</span></p>
<p id="t3008" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3009" class="pln"><span class="str">     p(x) = coeffs[n-1] + x * (coeffs[n-2] + ... + x * (coeffs[1] +</span><span class="strut">&nbsp;</span></p>
<p id="t3010" class="pln"><span class="str">            x * coeffs[0]))</span><span class="strut">&nbsp;</span></p>
<p id="t3011" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3012" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t3013" class="pln"><span class="str">    coeffs: A list of `Tensor` representing the coefficients of the polynomial.</span><span class="strut">&nbsp;</span></p>
<p id="t3014" class="pln"><span class="str">    x: A `Tensor` representing the variable of the polynomial.</span><span class="strut">&nbsp;</span></p>
<p id="t3015" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t3016" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3017" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t3018" class="pln"><span class="str">    A `tensor` of the shape as the expression p(x) with usual broadcasting rules</span><span class="strut">&nbsp;</span></p>
<p id="t3019" class="pln"><span class="str">    for element-wise addition and multiplication applied.</span><span class="strut">&nbsp;</span></p>
<p id="t3020" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3021" class="pln"><span class="str">  @compatibility(numpy)</span><span class="strut">&nbsp;</span></p>
<p id="t3022" class="pln"><span class="str">  Equivalent to numpy.polyval.</span><span class="strut">&nbsp;</span></p>
<p id="t3023" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t3024" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t3025" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3026" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"polyval"</span><span class="op">,</span> <span class="nam">nest</span><span class="op">.</span><span class="nam">flatten</span><span class="op">(</span><span class="nam">coeffs</span><span class="op">)</span> <span class="op">+</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3027" class="stm mis">    <span class="nam">x</span> <span class="op">=</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="str">"x"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3028" class="stm mis">    <span class="key">if</span> <span class="nam">len</span><span class="op">(</span><span class="nam">coeffs</span><span class="op">)</span> <span class="op">&lt;</span> <span class="num">1</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3029" class="stm mis">      <span class="key">return</span> <span class="nam">array_ops</span><span class="op">.</span><span class="nam">zeros_like</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3030" class="stm mis">    <span class="nam">coeffs</span> <span class="op">=</span> <span class="op">[</span><span class="strut">&nbsp;</span></p>
<p id="t3031" class="pln">        <span class="nam">ops</span><span class="op">.</span><span class="nam">convert_to_tensor</span><span class="op">(</span><span class="nam">coeff</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="op">(</span><span class="str">"coeff_%d"</span> <span class="op">%</span> <span class="nam">index</span><span class="op">)</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3032" class="pln">        <span class="key">for</span> <span class="nam">index</span><span class="op">,</span> <span class="nam">coeff</span> <span class="key">in</span> <span class="nam">enumerate</span><span class="op">(</span><span class="nam">coeffs</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3033" class="pln">    <span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t3034" class="stm mis">    <span class="nam">p</span> <span class="op">=</span> <span class="nam">coeffs</span><span class="op">[</span><span class="num">0</span><span class="op">]</span><span class="strut">&nbsp;</span></p>
<p id="t3035" class="stm mis">    <span class="key">for</span> <span class="nam">c</span> <span class="key">in</span> <span class="nam">coeffs</span><span class="op">[</span><span class="num">1</span><span class="op">:</span><span class="op">]</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3036" class="stm mis">      <span class="nam">p</span> <span class="op">=</span> <span class="nam">c</span> <span class="op">+</span> <span class="nam">p</span> <span class="op">*</span> <span class="nam">x</span><span class="strut">&nbsp;</span></p>
<p id="t3037" class="stm mis">    <span class="key">return</span> <span class="nam">p</span><span class="strut">&nbsp;</span></p>
<p id="t3038" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3039" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3040" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.bessel_i0e"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3041" class="stm run hide_run"><span class="key">def</span> <span class="nam">bessel_i0e</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3042" class="pln">  <span class="str">"""Computes the Bessel i0e function of `x` element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t3043" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3044" class="pln"><span class="str">  Exponentially scaled modified Bessel function of order 0 defined as</span><span class="strut">&nbsp;</span></p>
<p id="t3045" class="pln"><span class="str">  `bessel_i0e(x) = exp(-abs(x)) bessel_i0(x)`.</span><span class="strut">&nbsp;</span></p>
<p id="t3046" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3047" class="pln"><span class="str">  This function is faster and numerically stabler than `bessel_i0(x)`.</span><span class="strut">&nbsp;</span></p>
<p id="t3048" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3049" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t3050" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span><span class="strut">&nbsp;</span></p>
<p id="t3051" class="pln"><span class="str">      `float32`, `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t3052" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t3053" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3054" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t3055" class="pln"><span class="str">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t3056" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3057" class="pln"><span class="str">  @compatibility(scipy)</span><span class="strut">&nbsp;</span></p>
<p id="t3058" class="pln"><span class="str">  Equivalent to scipy.special.i0e</span><span class="strut">&nbsp;</span></p>
<p id="t3059" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t3060" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t3061" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"bessel_i0e"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3062" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3063" class="stm mis">      <span class="nam">x_i0e</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">bessel_i0e</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3064" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t3065" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_i0e</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3066" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3067" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">bessel_i0e</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3068" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3069" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3070" class="stm run hide_run"><span class="op">@</span><span class="nam">tf_export</span><span class="op">(</span><span class="str">"math.bessel_i1e"</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3071" class="stm run hide_run"><span class="key">def</span> <span class="nam">bessel_i1e</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="key">None</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3072" class="pln">  <span class="str">"""Computes the Bessel i1e function of `x` element-wise.</span><span class="strut">&nbsp;</span></p>
<p id="t3073" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3074" class="pln"><span class="str">  Exponentially scaled modified Bessel function of order 1 defined as</span><span class="strut">&nbsp;</span></p>
<p id="t3075" class="pln"><span class="str">  `bessel_i1e(x) = exp(-abs(x)) bessel_i1(x)`.</span><span class="strut">&nbsp;</span></p>
<p id="t3076" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3077" class="pln"><span class="str">  This function is faster and numerically stabler than `bessel_i1(x)`.</span><span class="strut">&nbsp;</span></p>
<p id="t3078" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3079" class="pln"><span class="str">  Args:</span><span class="strut">&nbsp;</span></p>
<p id="t3080" class="pln"><span class="str">    x: A `Tensor` or `SparseTensor`. Must be one of the following types: `half`,</span><span class="strut">&nbsp;</span></p>
<p id="t3081" class="pln"><span class="str">      `float32`, `float64`.</span><span class="strut">&nbsp;</span></p>
<p id="t3082" class="pln"><span class="str">    name: A name for the operation (optional).</span><span class="strut">&nbsp;</span></p>
<p id="t3083" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3084" class="pln"><span class="str">  Returns:</span><span class="strut">&nbsp;</span></p>
<p id="t3085" class="pln"><span class="str">    A `Tensor` or `SparseTensor`, respectively. Has the same type as `x`.</span><span class="strut">&nbsp;</span></p>
<p id="t3086" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3087" class="pln"><span class="str">  @compatibility(scipy)</span><span class="strut">&nbsp;</span></p>
<p id="t3088" class="pln"><span class="str">  Equivalent to scipy.special.i1e</span><span class="strut">&nbsp;</span></p>
<p id="t3089" class="pln"><span class="str">  @end_compatibility</span><span class="strut">&nbsp;</span></p>
<p id="t3090" class="pln"><span class="str">  """</span><span class="strut">&nbsp;</span></p>
<p id="t3091" class="stm mis">  <span class="key">with</span> <span class="nam">ops</span><span class="op">.</span><span class="nam">name_scope</span><span class="op">(</span><span class="nam">name</span><span class="op">,</span> <span class="str">"bessel_i1e"</span><span class="op">,</span> <span class="op">[</span><span class="nam">x</span><span class="op">]</span><span class="op">)</span> <span class="key">as</span> <span class="nam">name</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3092" class="stm mis">    <span class="key">if</span> <span class="nam">isinstance</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">)</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3093" class="stm mis">      <span class="nam">x_i1e</span> <span class="op">=</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">bessel_i1e</span><span class="op">(</span><span class="nam">x</span><span class="op">.</span><span class="nam">values</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3094" class="stm mis">      <span class="key">return</span> <span class="nam">sparse_tensor</span><span class="op">.</span><span class="nam">SparseTensor</span><span class="op">(</span><span class="strut">&nbsp;</span></p>
<p id="t3095" class="pln">          <span class="nam">indices</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">indices</span><span class="op">,</span> <span class="nam">values</span><span class="op">=</span><span class="nam">x_i1e</span><span class="op">,</span> <span class="nam">dense_shape</span><span class="op">=</span><span class="nam">x</span><span class="op">.</span><span class="nam">dense_shape</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3096" class="pln">    <span class="key">else</span><span class="op">:</span><span class="strut">&nbsp;</span></p>
<p id="t3097" class="stm mis">      <span class="key">return</span> <span class="nam">gen_math_ops</span><span class="op">.</span><span class="nam">bessel_i1e</span><span class="op">(</span><span class="nam">x</span><span class="op">,</span> <span class="nam">name</span><span class="op">=</span><span class="nam">name</span><span class="op">)</span><span class="strut">&nbsp;</span></p>
<p id="t3098" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3099" class="pln"><span class="strut">&nbsp;</span></p>
<p id="t3100" class="pln"><span class="com"># FFT ops were moved to tf.spectral. tf.fft symbols were part of the TensorFlow</span><span class="strut">&nbsp;</span></p>
<p id="t3101" class="pln"><span class="com"># 1.0 API so we leave these here for backwards compatibility.</span><span class="strut">&nbsp;</span></p>
<p id="t3102" class="stm run hide_run"><span class="nam">fft</span> <span class="op">=</span> <span class="nam">gen_spectral_ops</span><span class="op">.</span><span class="nam">fft</span><span class="strut">&nbsp;</span></p>
<p id="t3103" class="stm run hide_run"><span class="nam">ifft</span> <span class="op">=</span> <span class="nam">gen_spectral_ops</span><span class="op">.</span><span class="nam">ifft</span><span class="strut">&nbsp;</span></p>
<p id="t3104" class="stm run hide_run"><span class="nam">fft2d</span> <span class="op">=</span> <span class="nam">gen_spectral_ops</span><span class="op">.</span><span class="nam">fft2d</span><span class="strut">&nbsp;</span></p>
<p id="t3105" class="stm run hide_run"><span class="nam">ifft2d</span> <span class="op">=</span> <span class="nam">gen_spectral_ops</span><span class="op">.</span><span class="nam">ifft2d</span><span class="strut">&nbsp;</span></p>
<p id="t3106" class="stm run hide_run"><span class="nam">fft3d</span> <span class="op">=</span> <span class="nam">gen_spectral_ops</span><span class="op">.</span><span class="nam">fft3d</span><span class="strut">&nbsp;</span></p>
<p id="t3107" class="stm run hide_run"><span class="nam">ifft3d</span> <span class="op">=</span> <span class="nam">gen_spectral_ops</span><span class="op">.</span><span class="nam">ifft3d</span><span class="strut">&nbsp;</span></p>

            </td>
        </tr>
    </table>
</div>

<div id="footer">
    <div class="content">
        <p>
            <a class="nav" href="index.html">&#xab; index</a> &nbsp; &nbsp; <a class="nav" href="https://coverage.readthedocs.io">coverage.py v4.5.2</a>,
            created at 2018-12-27 22:27
        </p>
    </div>
</div>

</body>
</html>
